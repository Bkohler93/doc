{
  "doc/large-objects.html": {
    "href": "doc/large-objects.html",
    "title": "Large Objects | Npgsql Documentation",
    "keywords": "Large Objects The Large Objects feature is a way of storing large files in a PostgreSQL database. Files can normally be stored in bytea columns but there are two downsides; a file can only be 1 GB and the backend buffers the whole file when reading or writing a column, which may use significant amounts of RAM on the backend. With the Large Objects feature, objects are instead stored in a separate system table in smaller chunks and provides a streaming API for the user. Each object is given an integral identifier that is used for accessing the object, that can, for example, be stored in a user's table containing information about this object. Example // Retrieve a Large Object Manager for this connection var manager = new NpgsqlLargeObjectManager(Conn); // Create a new empty file, returning the identifier to later access it uint oid = manager.Create(); // Reading and writing Large Objects requires the use of a transaction using (var transaction = Conn.BeginTransaction()) { // Open the file for reading and writing using (var stream = manager.OpenReadWrite(oid)) { var buf = new byte[] { 1, 2, 3 }; stream.Write(buf, 0, buf.Length); stream.Seek(0, System.IO.SeekOrigin.Begin); var buf2 = new byte[buf.Length]; stream.Read(buf2, 0, buf2.Length); // buf2 now contains 1, 2, 3 } // Save the changes to the object transaction.Commit(); } See also See the PostgreSQL documentation for more information. All functionality are implemented and wrapped in the classes NpgsqlLargeObjectManager and NpgsqlLargeObjectStream using standard .NET Stream as base class."
  },
  "doc/migration/3.0.html": {
    "href": "doc/migration/3.0.html",
    "title": "Migration Notes | Npgsql Documentation",
    "keywords": "Migrating from 2.2 to 3.0 Version 3.0 represents a near-total rewrite of Npgsql. In addition to changing how Npgsql works internally and communicates with PostgreSQL, a conscious effort was made to better align Npgsql with the ADO.NET specs/standard and with SqlClient where that made sense. This means that you cannot expect to drop 3.0 as a replacement to 2.2 and expect things to work - upgrade cautiously and test extensively before deploying anything to production. The following is a non-exhaustive list of things that changed. If you run against a breaking change not documented here, please let us know and we'll add it. Major Support for .NET 2.0, .NET 3.5 and .NET 4.0 has been dropped - you will have to upgrade to .NET 4.5 to use Npgsql 3.0. We'll continue to do bugfixes on the 2.2 branch for a while on a best-effort basis. The Entity Framework provider packages have been renamed to align with Microsoft's new naming. The new packages are EntityFramework5.Npgsql and EntityFramework6.Npgsql . EntityFramework7.Npgsql is in alpha. A brand-new bulk copy API has been written, using binary encoding for much better performance. See the docs . Composite (custom) types aren't supported yet, but this is a high-priority feature for us. See #441 . SSL Npgsql 2.2 didn't perform validation on the server's certificate by default, so self-signed certificate were accepted. The new default is to perform validation. Specify the Trust Server Certificate connection string parameter to get back previous behavior. The \"SSL\" connection string parameter has been removed, use \"SSL Mode\" instead. The \"SSL Mode\" parameter's Allow option has been removed, as it wasn't doing anything. Type Handling Previously, Npgsql allowed writing a NULL by setting NpgsqlParameter.Value to null . This is not allowed in ADO.NET and is no longer supported, set to DBNull.Value instead. In some cases, you will now be required to explicitly set a parameter's type although you didn't have to before (you'll get an error 42804 explaining this). This can happen especially in Dapper custom custom type handlers ( #694 ). Simply set the NpgsqlDbType property on the parameter. Removed support for writing a parameter with an IEnumerable<T> value, since that would require Npgsql to enumerate it multiple times internally. IList<T> and IList are permitted. It is no longer possible to write a .NET enum to an integral PostgreSQL column (e.g. int4). Proper enum support has been added which allows writing to PostgreSQL enum columns (see the docs . To continue writing enums to integral columns as before, simply add an explicit cast to the integral type in your code. NpgsqlMacAddress has been removed and replaced by the standard .NET PhysicalAddress. Npgsql's BitString has been removed and replaced by the standard .NET BitArray. NpgsqlTime has been removed and replaced by the standard .NET TimeSpan. NpgsqlTimeZone has been removed. NpgsqlTimeTZ now holds 2 TimeSpans, rather than an NpgsqlTime and an NpgsqlTimeZone. NpgsqlTimeStamp no longer maps DateTime.{Max,Min}Value to {positive,negative} infinity. Use NpgsqlTimeStamp.Infinity and NpgsqlTimeStamp.MinusInfinity explicitly for that. You can also specify the \"Convert Infinity DateTime\" connection string parameter to retain the old behavior. Renamed NpgsqlInet's addr and mask to Address and Mask. NpgsqlPoint now holds Doubles instead of Singles ( #437 ). NpgsqlDataReader.GetFieldType() and GetProviderSpecificFieldType() now return Array for arrays. Previously they returned int[], even for multidimensional arrays. NpgsqlDataReader.GetDataTypeName() now returns the name of the PostgreSQL type rather than its OID. Retired features Removed the \"Preload Reader\" feature, which loaded the entire resultset into memory. If you require this (inefficient) behavior, read the result into memory outside Npgsql. We plan on working on MARS support, see #462 . The \"Use Extended Types\" parameter is no longer needed and isn't supported. To access PostgreSQL values that can't be represented by the standard CLR types, use the standard ADO.NET NpgsqlDataReader.GetProviderSpecificValue or even better, the generic NpgsqlDataReader.GetFieldValue<T> . Removed the feature where Npgsql automatically \"dereferenced\" a resultset of refcursors into multiple resultsets (this was used to emulate returning multiple resultsets from stored procedures). Note that if your function needs to return a single resultset, it should be simply returning a table rather than a cursor (see RETURNS TABLE ). See #438 . Removed the AlwaysPrepare connection string parameter Removed the Encoding connection string parameter, which was obsolete and unused anyway (UTF8 was always used regardless of what was specified) Removed the Protocol connection string parameter, which was obsolete and unused anyway (protocol 3 was always used) Removed NpgsqlDataReader.LastInsertedOID, it did not allow accessing individual OIDs in multi-statement commands. Replaced with NpgsqlDataReader.Statements, which provides OID and affected row information on a statement-by-statement basis. Removed NpgsqlDataReader.HasOrdinal , was a badly-named non-standard API without a serious use case. GetName() can be used as a workaround. Other It is no longer possible to create database entities (tables, functions) and then use them in the same multi-query command - you must first send a command creating the entity, and only then send commands using it. See #641 for more details. Previously, Npgsql set DateStyle=ISO, lc_monetary=C and extra_float_digits=3 on all connections it created. This is no longer case, if you rely on these parameters you must send them yourself. NpgsqlConnection.Clone() will now only return a new connection with the same connection string as the original. Previous versions returned an open connection if the original was open, and copied the Notice event listeners as well. Note: NpgsqlConnection.Clone() was accidentally missing from 3.0.0 and 3.0.1. Removed the obsolete NpgsqlParameterCollection.Add(name, value) method. Use AddWithValue() instead, which also exists in SqlClient. The savepoint manipulation methods on NpgsqlTransaction have been renamed from Save , and Rollback to CreateSavepoint and RollbackToSavepoint . This broke the naming conventions for these methods across other providers (SqlClient, Oracle...) and so in 3.0.2 the previous names were returned and the new names marked as obsolete. 3.1 will remove the the new names and leaves only Save and Rollback . See #738 . The default CommandTimeout has changed from 20 seconds to 30 seconds, as in ADO.NET . CommandType.TableDirect now requires CommandText to contain the name of a table, as per the MSDN docs . Multiple tables (join) aren't supported. CommandType.StoredProcedure now requires CommandText contain only the name of a function, without parentheses or parameter information, as per the MSDN docs . Moved the LastInsertedOID property from NpgsqlCommand to NpgsqlReader, like the standard ADO.NET RecordsAffected ."
  },
  "efcore/value-generation.html": {
    "href": "efcore/value-generation.html",
    "title": "Value Generation | Npgsql Documentation",
    "keywords": "Value Generation See the general EF Docs on value generation to better understand the concepts described here. Serial (Autoincrement) Columns In PostgreSQL, the standard autoincrement column type is called serial . This isn't really a special type like in some other databases (e.g. SQL Server's IDENTITY), but rather a shorthand for specifying that the column's default value should come from a sequence. See the PostgreSQL docs for more info. When ValueGeneratedOnAdd is specified on a short, int or long property, the Npgsql EF Core provider will automatically map it to a serial column. Note that EF Core will automatically recognize key properties by convention (e.g. a property called Id in your entity) and will implicitly set them to ValueGeneratedOnAdd . Note that there was a significant and breaking change in 1.1. If you have existing migrations generated with 1.0, please read the migration notes . Standard Sequence-Driven Columns While serial sets up a sequence for you, you may want to manage sequence creation yourself. This can be useful for cases where you need to control the sequence's increment value (i.e. increment by 2), populate two columns from the same sequence, etc. Adding a sequence to your model is described in the general EF Core documentation ; once the sequence is specified, you can simply set a column's default value to extract the next value from that sequence. Note that the SQL used to fetch the next value from a sequence differs across databases (see the PostgreSQL docs ). Your models' OnModelCreating should look like this: modelBuilder.HasSequence<int>(\"OrderNumbers\") .StartsAt(1000) .IncrementsBy(5); modelBuilder.Entity<Order>() .Property(o => o.OrderNo) .HasDefaultValueSql(\"nextval('\\\"OrderNumbers\\\"')\"); HiLo Autoincrement Generation One disadvantage of database-generated values is that these values must be read back from the database after a row is inserted. If you're saving multiple related entities, this means you must perform multiple roundtrips as the first entity's generated key must be read before writing the second one. One solution to this problem is HiLo value generation: rather than relying on the database to generate each and every value, the application \"allocates\" a range of values, which it can then populate directly on new entities without any additional roundtrips. When the range is exhausted, a new range is allocated. In practical terms, this uses a sequence that increments by some large value (100 by default), allowing the application to insert 100 rows autonomously. To use HiLo, specify ForNpgsqlUseSequenceHiLo on a property in your model's OnModelCreating : modelBuilder.Entity<Blog>().Property(b => b.Id).ForNpgsqlUseSequenceHiLo(); You can also make your model use HiLo everywhere: modelBuilder.ForNpgsqlUseSequenceHiLo(); Guid/UUID Generation By default, if you specify ValueGeneratedOnAdd on a Guid property, a random Guid value will be generated client-side and sent to the database. If you prefer to generate values in the database instead, you can do so by specifying HasDefaultValueSql on your property. Note that PostgreSQL doesn't include any Guid/UUID generation functions, you must add an extension such as uuid-ossp or pgcrypto . This can be done by placing the following code in your model's OnModelCreating : modelBuilder.HasPostgresExtension(\"uuid-ossp\"); modelBuilder .Entity<Blog>() .Property(e => e.SomeGuidProperty) .HasDefaultValueSql(\"uuid_generate_v4()\"); See the PostgreSQL docs on UUID for more details . Computed Columns (On Add or Update) PostgreSQL does not support computed columns."
  },
  "doc/copy.html": {
    "href": "doc/copy.html",
    "title": "COPY | Npgsql Documentation",
    "keywords": "COPY PostgreSQL has a feature allowing efficient bulk import or export of data to and from a table. This is usually a much faster way of getting data in and out of a table than using INSERT and SELECT. See documentation for the COPY command for more details. Npgsql supports three COPY operation modes: binary, text and raw binary. Binary COPY This mode uses the efficient PostgreSQL binary format to transfer data in and out of the database. The user uses an API to read and write rows and fields, which Npgsql decodes and encodes. IMPORTANT : Note that it is the your responsibility to read and write the correct type! If you use COPY to write an int32 into a string field you may get an exception, or worse, silent data corruption. It is also highly recommended to use the overload of Write() which accepts an NpgsqlDbType , allowing you to unambiguously specify exactly what type you want to write. Test your code throroughly. // Import two columns to table data using (var writer = conn.BeginBinaryImport(\"COPY data (field_text, field_int2) FROM STDIN (FORMAT BINARY)\")) { writer.StartRow(); writer.Write(\"Hello\"); writer.Write(8, NpgsqlDbType.Smallint); writer.StartRow(); writer.Write(\"Goodbye\"); writer.WriteNull(); } // Export two columns to table data using (var reader = Conn.BeginBinaryExport(\"COPY data (field_text, field_int2) TO STDOUT (FORMAT BINARY)\")) { reader.StartRow(); Console.WriteLine(reader.Read<string>()); Console.WriteLine(reader.Read<int>(NpgsqlDbType.Smallint)); reader.StartRow(); reader.Skip(); Console.WriteLine(reader.IsNull); // Null check doesn't consume the column Console.WriteLine(reader.Read<int>()); reader.StartRow(); // Last StartRow() returns -1 to indicate end of data } Text COPY This mode uses the PostgreSQL text or csv format to transfer data in and out of the database. It is the user's responsibility to format the text or CSV appropriately, Npgsql simply provides a TextReader or Writer. This mode is less efficient than binary copy, and is suitable mainly if you already have the data in a CSV or compatible text format and don't care about performance. using (var writer = conn.BeginTextImport(\"COPY data (field_text, field_int4) FROM STDIN\")) { writer.Write(\"HELLO\\t1\\n\"); writer.Write(\"GOODBYE\\t2\\n\"); } using (var reader = conn.BeginTextExport(\"COPY data (field_text, field_int4) TO STDOUT\")) { Console.WriteLine(reader.ReadLine()); Console.WriteLine(reader.ReadLine()); } Raw Binary COPY In this mode, data transfer is binary, but Npgsql does no encoding or decoding whatsoever - data is exposed as a raw .NET Stream. This mode makes sense only for bulk data and restore a table: the table is saved as a blob, which can later be restored. If you need to actually make sense of the data, you should be using regular binary mode instead (not raw). Example: int len; var data = new byte[10000]; // Export table1 to data array using (var inStream = conn.BeginRawBinaryCopy(\"COPY table1 TO STDOUT (FORMAT BINARY)\")) { // We assume the data will fit in 10000 bytes, in real usage you would read repeatedly, writine to a file. len = inStream.Read(data, 0, data.Length); } // Import data array into table2 using (var outStream = conn.BeginRawBinaryCopy(\"COPY table2 FROM STDIN (FORMAT BINARY)\")) { outStream.Write(data, 0, len); } Cancel Import operations can be cancelled at any time by calling the Cancel() method on the importer object. No data is committed to the database before the importer is closed or disposed. Export operations can be cancelled as well, also by calling Cancel() . Other See the CopyTests.cs test fixture for more usage samples."
  },
  "doc/performance.html": {
    "href": "doc/performance.html",
    "title": "Performance | Npgsql Documentation",
    "keywords": "Performance Prepared Statements One of the most important (and easy) ways to improve your application's performance is to prepare your commands. Even if you're not coding against ADO.NET directly (e.g. using Dapper or an O/RM), Npgsql has an automatic preparation feature which allows you to benefit from the performance gains associated with prepared statements. See this blog post and/or the documentation for more details. Batching/Pipelining When you execute a command, Npgsql executes a roundtrip to the database. If you execute multiple commands (say, inserting 3 rows or performing multiple selects), you're executing multiple roundtrips; each command has to complete before the next command can start execution. Depending on your network latency, this can considerably degrade your application's performance. You can batch multiple SQL statements in a single command, executing them a single roundtrip: using (var cmd = new NpgsqlCommand(\"SELECT ...; SELECT ...\")) using (var reader = cmd.ExecuteReader()) { while (reader.Read()) { // Read first resultset } reader.NextResult(); while (reader.Read()) { // Read second resultset } } Performance Counters Npgsql 3.2 includes support for performance counters , which provide visibility into connections and the connection pool - this helps you understand what your application is doing in real-time, whether there's a connection leak, etc. Npgsql counter support is very similar to that of other ADO.NET providers, such as SqlClient , it's recommended that your read that page first. Using performance counters first involves setting them up on your Windows system. To do this you will need to install Npgsql's MSI, which is available on the github releases page . Note that GAC installation isn't necessary (or recommended). Once the counters are installed, fire up the Windows Performance Monitor and look for the category \".NET Data Provider for PostgreSQL (Npgsql)\". In addition, you will need to pass Use Perf Counters=true on your connection string. Once you start your Npgsql application with this addition, you should start seeing real-time data in the Performance Monitor. Performance counters are currently only available on Windows with .NET Framework (.NET Core doesn't include performance counters yet). Pooled Connection Reset When a pooled connection is closed, Npgsql will arrange for its state to be reset the next time it's used. This prevents leakage of state from one usage cycle of a physical connection to another one. For example, you may change certain PostgreSQL parameters (e.g. statement_timeout ), and it's undesirable for this change to persist when the connection is closed. Connection reset happens via the PostgreSQL DISCARD ALL command , or, if there are any prepared statements at the time of closing, by a combination of the equivalent statements described in the docs (to prevent closing those statements). Note that these statements aren't actually sent when closing the connection - they're written into Npgsql's internal write buffer, and will be sent with the first user statement after the connection is reopened. This prevents a costly database roundtrip. If you really want to squeeze every last bit of performance from PostgreSQL, you may disable connect reset by specifying No Reset On Close on your connection string - this will slightly improve performance in scenarios where connection are very short-lived, and especially if prepared statements are in use. Reading Large Values When reading results from PostgreSQL, Npgsql first reads raw binary data from the network into an internal read buffer, and then parses that data as you call methods such as NpgsqlDataReader.GetString() . While this allows for efficient network reads, it's worth thinking about the size of this buffer, which is 8K by default. Under normal usage,, Npgsql attempts to read each row into the buffer; if that entire row fits in 8K, you'll have optimal performance. However, if a row is bigger than 8K, Npgsql will allocate an \"oversize buffer\", which will be used until the connection is closed or returned to the pool. If you're not careful, this can create significant memory churn that will slow down your application. To avoid this, if you know you're going to be reading 16k rows, you can specify Read Buffer Size=18000 in your connection string (leaving some margin for protocol overhead), this will ensure that the read buffer is reused and no extra allocation occur. Another option is to pass CommandBehavior.SequentialAccess to NpgsqlCommand.ExecuteReader() . Sequential mode means that Npgsql will no longer read entire rows into its buffer, but will rather fill up the buffer as needed, reading more data only when it's empty. The same 8K read buffer will be used regardless of the row's total size, and Npgsql will take care of the details. In sequential mode, however, you must read the row's fields in the order in which you specified them; you cannot read the 2nd field and then go back to the 1st field, and trying to do so will generate an exception. Similarly, you cannot read the same field twice - once you've read a field, it has been consumed. For more information on CommandBehavior.SequentialAccess , see this page . If you decide to use this feature, be aware that it isn't used as often and may therefore contain bugs. You can also control the socket's receive buffer size (not to be confused with Npgsql's internal buffer) by setting the Socket Receive Buffer Size connection string parameter. Writing Large Values Writing is somewhat similar - Npgsql has an internally write buffer (also 8K by default). When writing your query's SQL and parameters to PostgreSQL, Npgsql always writes \"sequentially\", that is, filling up the 8K buffer and flushing it when full. You can use Write Buffer Size to control the buffer's size. You can also control the socket's receive buffer size (not to be confused with Npgsql's internal buffer) by setting the Socket Receive Buffer Size connection string parameter. Unix Domain Socket If you're on Linux or macOS and are connecting to a PostgreSQL server on the same machine, you can boost performance a little by connecting via Unix domain socket rather than via a regular TCP/IP socket. To do this, simply specify the directory of your PostgreSQL sockets in the Host connection string parameter - if this parameter starts with a slash, it will be taken to mean a filesystem path."
  },
  "doc/types/basic.html": {
    "href": "doc/types/basic.html",
    "title": "Supported Types and their Mappings | Npgsql Documentation",
    "keywords": "Supported Types and their Mappings Type mappings when reading values sent from the backend PostgreSQL type Default .NET type Provider-specific type Other .NET types bool bool int2 short byte, sbyte, int, long, float, double, decimal, string int4 int byte, short, long, float, double, decimal, string int8 long long, byte, short, int, float, double, decimal, string float4 float double float8 double numeric decimal byte, short, int, long, float, double, string money decimal text string char[] varchar string char[] bpchar string char[] citext string char[] json string char[] jsonb string char[] xml string char[] point NpgsqlPoint string lseg NpgsqlLSeg string path NpgsqlPath polygon NpgsqlPolygon line NpgsqlLine string circle NpgsqlCircle string box NpgsqlBox string bit(1) bool BitArray bit(n) BitArray varbit BitArray hstore IDictionary<string, string> string uuid Guid string cidr NpgsqlInet string inet IPAddress NpgsqlInet string macaddr PhysicalAddress string tsquery NpgsqlTsQuery tsvector NpgsqlTsVector date DateTime NpgsqlDate interval TimeSpan NpgsqlTimeSpan timestamp DateTime NpgsqlDateTime timestamptz DateTime NpgsqlDateTime DateTimeOffset time TimeSpan timetz DateTimeOffset DateTimeOffset, DateTime, TimeSpan bytea byte[] oid uint xid uint cid uint oidvector uint[] name string char[] (internal) char char byte, short, int, long geometry (PostGIS) PostgisGeometry record object[] composite types T range subtypes NpgsqlRange enum types TEnum array types Array (of child element type) The Default .NET type column specifies the data type NpgsqlDataReader.GetValue() will return. NpgsqlDataReader.GetProviderSpecificValue will return a value of a data type specified in the Provider-specific type column, or the Default .NET type if there is no specialization. Finally, the third column specifies other CLR types which Npgsql supports for the PostgreSQL data type. These can be retrieved by calling NpgsqlDataReader.GetBoolean() , GetByte() , GetDouble() etc. or via GetFieldValue<T>() . Type mappings when sending parameters to the backend There are three rules that determine the PostgreSQL type sent for a parameter: If the parameter's NpgsqlDbType is set, it is used. If the parameter's DbType is set, it is used. If neither of the above is set, the backend type will be inferred from the CLR value type. Note that for DateTime and NpgsqlDateTime, the Kind attribute determines whether to use timestamp or timestamptz . Note that when NpgsqlDbType or DbType is set to a primitive type (bool, numbers and string), most other primitive types are accepted since they all implement the IConvertible interface, which is what Npgsql uses to convert the value to the target type. NpgsqlDbType DbType PostgreSQL type Accepted .NET types Boolean Boolean bool bool, IConvertible Smallint Int16 int2 short, IConvertible Integer Int32 int4 int, IConvertible Bigint Int64 int8 long, IConvertible Real Single float4 float, IConvertible Double Double float8 double, IConvertible Numeric Decimal, VarNumeric numeric decimal, IConvertible Money Currency money decimal, IConvertible Text String, StringFixedLength, AnsiString, AnsiStringFixedLength text string, char[], char, IConvertible Varchar varchar string, char[], char, IConvertible Char char string, char[], char, IConvertible Citext citext string, char[], char, IConvertible Json json string, char[], char, IConvertible Jsonb jsonb string, char[], char, IConvertible Xml xml string, char[], char, IConvertible Point point NpgsqlPoint LSeg lseg NpgsqlLSeg Path path NpgsqlPath Polygon polygon NpgsqlPolygon Line line NpgsqlLine Circle circle NpgsqlCircle Box box NpgsqlBox Bit bit BitArray, bool, string Varbit varbit BitArray, bool, string Hstore hstore IDictionary<string, string> Uuid uuid Guid, string Cidr cidr IPAddress, NpgsqlInet Inet inet IPAddress, NpgsqlInet MacAddr macaddr PhysicalAddress TsQuery tsquery NpgsqlTsQuery TsVector tsvector NpgsqlTsVector Date Date date DateTime, NpgsqlDate, IConvertible Interval interval TimeSpan, NpgsqlTimeSpan, string Timestamp DateTime, DateTime2 timestamp DateTime, DateTimeOffset, NpgsqlDateTime, IConvertible TimestampTZ DateTimeOffset timestamptz DateTime, DateTimeOffset, NpgsqlDateTime, IConvertible Time Time time TimeSpan, string TimeTZ timetz DateTimeOffset, DateTime, TimeSpan Bytea Binary bytea byte[], ArraySegment Oid oid uint, IConvertible Xid xid uint, IConvertible Cid cid uint, IConvertible Oidvector oidvector uint[] Name name string, char[], char, IConvertible InternalChar (internal) char byte, IConvertible Geometry geometry PostgisGeometry Composite composite types T Range | (other NpgsqlDbType) range types NpgsqlRange Enum enum types TEnum Array | (other NpgsqlDbType) array types Array, IList , IList Notes when using Range and Array, bitwise-or NpgsqlDbType.Range or NpgsqlDbType.Array with the child type. For example, to construct the NpgsqlDbType for a int4range , write NpgsqlDbType.Range | NpgsqlDbType.Integer . To construct the NpgsqlDbType for an int[] , write NpgsqlDbType.Array | NpgsqlDbType.Integer . For information about enums, see the Enums and Composites page . .NET type Auto-inferred PostgreSQL type bool bool byte int2 sbyte int2 short int2 int int4 long int8 float float4 double float8 decimal numeric string text char[] text char text NpgsqlPoint point NpgsqlLSeg lseg NpgsqlPath path NpgsqlPolygon polygon NpgsqlLine line NpgsqlCircle circle NpgsqlBox box BitArray varbit Guid uuid IPAddress inet NpgsqlInet inet PhysicalAddress macaddr NpgsqlTsQuery tsquery NpgsqlTsVector tsvector NpgsqlDate date NpgsqlDateTime(Kind=Local,Unspecified) timestamp NpgsqlDateTime(Kind=Utc) timestamptz DateTime(Kind=Local,Unspecified) timestamp DateTime(Kind=Utc) timestamptz DateTimeOffset timestamptz TimeSpan time byte[] bytea PostgisGeometry geometry Custom composite type composite types NpgsqlRange range types Enum types enum types Array types array types"
  },
  "dev/tests.html": {
    "href": "dev/tests.html",
    "title": "Tests | Npgsql Documentation",
    "keywords": "Overview Npgsql comes with an extensive test suite to make sure no regressions occur. All tests are run on our build server on all supported .NET versions (including a recent version of mono) and all supported PostgreSQL backends. There is also a growing suite of speed tests to be able to measure performance. These tests are currently marked [Explicit] and aren't executed automatically. Simple setup The Npgsql test suite requires a PostgreSQL backend to test against. Simply use the latest version of PostgreSQL on your dev machine on the default port (5432). By default, all tests will be run using user npgsql_tests , and password npgsql_tests . Npgsql will automatically create a database called npgsql_tests and run its tests against this. To set this up, connect to PostgreSQL as the admin user as follows: psql -h localhost -U postgres <enter the admin password> create user npgsql_tests password 'npgsql_tests' superuser; And you're done. Superuser access is needed for some tests, e.g. loading the hstore extension, creating and dropping test databases in the Entity Framework tests..."
  },
  "doc/compatibility.html": {
    "href": "doc/compatibility.html",
    "title": "Compatibility Notes | Npgsql Documentation",
    "keywords": "Compatibility Notes This page centralizes Npgsql's compatibility status with PostgreSQL and other components, and documents some important gotchas. We aim to be compatible with all currently supported PostgreSQL versions , which means 5 years back. Earlier versions may still work but we don't perform continuous testing on them or commit to resolving issues on them. PostgreSQL We aim to be compatible with all currently supported PostgreSQL versions , which means 5 years back. Earlier versions may still work but we don't perform continuous testing on them or commit to resolving issues on them. ADO.NET Npgsql is an ADO.NET-compatible provider, so it has the same APIs as other .NET database drivers and should behave the same. Please let us know if you notice any non-standard behavior. .NET Framework/.NET Core/mono Npgsql 3.1 targets .NET Framework 4.5 and 4.5.1, as well as the .NET Standard 1.3 which allows it to run on .NET Core. It is also tested and runs well on mono. Here is a sample project.json to get you started with .NET Core: { \"buildOptions\": { \"emitEntryPoint\": \"true\" }, \"dependencies\": { \"Npgsql\" : \"3.1.8\" }, \"frameworks\": { \"net451\": { \"frameworkAssemblies\": { \"System.Data\": { \"version\": \"4.0.0.0\", \"type\": \"build\" } } }, \"netcoreapp1.0\": { \"dependencies\": { \"Microsoft.NETCore.App\": { \"version\": \"1.0.1\", \"type\": \"platform\" } } } } } Note that netcoreapp1.0 can be replaced with netstandard13 (or up) to create a library. Amazon Redshift Amazon Redshift is a cloud-based data warehouse originally based on PostgreSQL 8.0.2. In addition, due to its nature some features have been removed and others changed in ways that make them incompatible with PostgreSQL. We try to support Redshift as much as we can, please let us know about issues you run across. First, check out Amazon's page about Redshift and PostgreSQL which contains lots of useful compatibility information. Additional known issues: If you want to connect over SSL, your connection string must contain Server Compatibility Mode=Redshift , otherwise you'll get a connection error about ssl_renegotiation_limit . Entity Framework with database-computed identity values don't work with Redshift, since it doesn't support sequences (see issue #544 ). pgbouncer Npgsql works well with pgbouncer, but there are some quirks to be aware of. Don't forget to turn off Npgsql's internal connection pool by specifying Pooling=false on the connection string. Prior to version 3.1, Npgsql sends the statement_timeout startup parameter when it connects, but this parameter isn't supported by pgbouncer. You can get around this by specifying CommandTimeout=0 on the connection string, and then manually setting the CommandTimeout property on your NpgsqlCommand objects. Version 3.1 no longer sends statement_timeout ."
  },
  "doc/faq.html": {
    "href": "doc/faq.html",
    "title": "FAQ | Npgsql Documentation",
    "keywords": "FAQ I get an exception \"The field field1 has a type currently unknown to Npgsql (OID XXXXX). You can retrieve it as a string by marking it as unknown\". Npgsql has to implement support for each PostgreSQL type, and it seems you've stumbled upon an unsupported type. First, head over to our issues page and check if an issue already exists on your type, otherwise please open one to let us know. Then, as a workaround, you can have your type treated as text - it will be up to you to parse it in your program. One simple way to do this is to append ::TEXT in your query (e.g. SELECT 3::TEXT ). If you don't want to modify your query, Npgsql also includes an API for requesting types as text. The fetch all the columns in the resultset as text, using (var cmd = new NpgsqlCommand(...)) { cmd.AllResultTypesAreUnknown = true; var reader = cmd.ExecuteReader(); // Read everything as strings } You can also specify text only for some columns in your resultset: using (var cmd = new NpgsqlCommand(...)) { // Only the second field will be fetched as text cmd.UnknownResultTypeList = new[] { false, true }; var reader = cmd.ExecuteReader(); // Read everything as strings } I'm trying to write a JSONB type and am getting 'column \"XXX\" is of type jsonb but expression is of type text' When sending a JSONB parameter, you must explicitly specify its type to be JSONB with NpgsqlDbType: using (var cmd = new NpgsqlCommand(\"INSERT INTO foo (col) VALUES (@p)\", conn)) { cmd.Parameters.AddWithValue(\"p\", NpgsqlDbType.Jsonb, jsonText); } I'm trying to apply an Entity Framework 6 migration and I get Type is not resolved for member 'Npgsql.NpgsqlException,Npgsql' ! Unfortunately, a shortcoming of EF6 requires you to have Npgsql.dll in the Global Assembly Cache (GAC), otherwise you can't see migration-triggered exceptions. You can add Npgsql.dll to the GAC by opening a VS Developer Command Prompt as administator and running the command gacutil /i Npgsql.dll . You can remove it from the GAC with gacutil /u Npgsql ."
  },
  "doc/logging.html": {
    "href": "doc/logging.html",
    "title": "Logging | Npgsql Documentation",
    "keywords": "Logging Note: Npgsql 3.2.0 and 3.2.1 significantly changed logging to use Microsoft.Extensions.Logging. After several complaints and issues (see #1504 ), this feature was rolled back. Starting with Npgsql 3.2.2, logging support is identical to Npgsql 3.1. Npgsql includes a built-in feature for outputting logging events which can help debug issues. Npgsql logging is disabled by default and must be turned on. Logging can be turned on by setting NpgsqlLogManager.Provider to a class implementing the INpgsqlLoggingProvider interface. Npgsql comes with a console implementation which can be set up as follows: NpgsqlLogManager.Provider = new ??? Note: you must set the logging provider before invoking any other Npgsql method, at the very start of your program. It's trivial to create a logging provider that passes log messages to whatever logging framework you use. You can find such an adaptor for NLog here . Note: the logging API is a first implementation and will probably improve/change - don't treat it as a stable part of the Npgsql API. Let us know if you think there are any missing messages or features! ConsoleLoggingProvider Npgsql comes with one built-in logging provider: ConsoleLoggingProvider. It will simply dump all log messages with a given level or above to stdanrd output. You can set it up by including the following line at the beginning of your application: NpgsqlLogManager.Provider = new ConsoleLoggingProvider(<min level>, <print level?>, <print connector id?>); Level defaults to NpgsqlLogLevel.Info (which will only print warnings and errors). You can also have log levels and connector IDs logged. Statement and Parameter Logging Npgsql will log all SQL statements at level Debug, this can help you debug exactly what's being sent to PostgreSQL. By default, Npgsql will not log parameter values as these may contain sensitive information. You can turn on parameter logging by setting NpgsqlLogManager.IsParameterLoggingEnabled to true. NLogLoggingProvider (or implementing your own) The following provider is used in the Npgsql unit tests to pass log messages to NLog . You're welcome to copy-paste it into your project, or to use it as a starting point for implementing your own custom provider. class NLogLoggingProvider : INpgsqlLoggingProvider { public NpgsqlLogger CreateLogger(string name) { return new NLogLogger(name); } } class NLogLogger : NpgsqlLogger { readonly Logger _log; internal NLogLogger(string name) { _log = LogManager.GetLogger(name); } public override bool IsEnabled(NpgsqlLogLevel level) { return _log.IsEnabled(ToNLogLogLevel(level)); } public override void Log(NpgsqlLogLevel level, int connectorId, string msg, Exception exception = null) { var ev = new LogEventInfo(ToNLogLogLevel(level), \"\", msg); if (exception != null) ev.Exception = exception; if (connectorId != 0) ev.Properties[\"ConnectorId\"] = connectorId; _log.Log(ev); } static LogLevel ToNLogLogLevel(NpgsqlLogLevel level) { switch (level) { case NpgsqlLogLevel.Trace: return LogLevel.Trace; case NpgsqlLogLevel.Debug: return LogLevel.Debug; case NpgsqlLogLevel.Info: return LogLevel.Info; case NpgsqlLogLevel.Warn: return LogLevel.Warn; case NpgsqlLogLevel.Error: return LogLevel.Error; case NpgsqlLogLevel.Fatal: return LogLevel.Fatal; default: throw new ArgumentOutOfRangeException(\"level\"); } } }"
  },
  "doc/prepare.html": {
    "href": "doc/prepare.html",
    "title": "Prepared Statements | Npgsql Documentation",
    "keywords": "Prepared Statements Introduction It's recommended that you start by reading this blog post . Most applications repeat the same SQL statements many times, passing different parameters. In such cases, it's very beneficial to prepare commands - this will send the command's statement(s) to PostgreSQL, which will parse and plan for them. The prepared statements can then be used on execution, saving valuable planning time. The more complex your queries, the more you'll notice the performance gain; but even very simple queries tend to benefit from preparation. Following is a benchmark Npgsql.Benchmarks.Prepare, which measures the execution time of the same query, executed prepared and unprepared. TablesToJoin is a parameter which increases the query complexity - it determines how many tables the query joins from. Method TablesToJoin Mean StdErr StdDev Op/s Scaled Scaled-StdDev Allocated Unprepared 0 67.1964 us 0.1586 us 0.6142 us 14881.75 1.00 0.00 1.9 kB Prepared 0 43.5007 us 0.2466 us 0.9227 us 22988.13 0.65 0.01 305 B Unprepared 1 98.8502 us 0.1278 us 0.4949 us 10116.32 1.00 0.00 1.93 kB Prepared 1 53.7518 us 0.0486 us 0.1818 us 18604.04 0.54 0.00 306 B Unprepared 2 180.0599 us 0.2990 us 1.1579 us 5553.71 1.00 0.00 2.06 kB Prepared 2 70.3609 us 0.1715 us 0.6417 us 14212.44 0.39 0.00 306 B Unprepared 5 1,084.6065 us 1.1822 us 4.2626 us 921.99 1.00 0.00 2.37 kB Prepared 5 110.0652 us 0.1098 us 0.3805 us 9085.52 0.10 0.00 308 B Unprepared 10 23,086.5956 us 37.2072 us 139.2167 us 43.32 1.00 0.00 3.11 kB Prepared 10 197.1392 us 0.3044 us 1.1790 us 5072.56 0.01 0.00 308 B As is immediately apparent, even an extremely simple scenario (TablesToJoin=0, SQL=SELECT 1), preparing the query with PostgreSQL provides a 36% speedup. As query complexity increases by adding join tables, the gap widens dramatically. The only potential disadvantage of prepared statements is that they hold server-side resources (e.g. cached plans). If you're dynamically generating SQL queries, make sure you don't overwhelm the server by preparing too much. Most reasonable applications shouldn't have to worry about this. Simple Preparation To prepare your commands, simply use the following standard ADO.NET code: var cmd = new NpgsqlCommand(...); cmd.Parameters.Add(\"param\", NpgsqlDbType.Integer); cmd.Prepare(); // Set parameters cmd.ExecuteNonQuery(); // And so on Note that all parameters must be set before calling Prepare() - they are part of the information transmitted to PostgreSQL and used to effectively plan the statement. You must also set the DbType or NpgsqlDbType on your parameters to unambiguously specify the data type (setting the value isn't support). Note that preparation happens on individual statements, and not on commands, which can contain multiple statements, batching them together. This can be important in cases such as the following: var cmd = new NpgsqlCommand(\"UPDATE foo SET bar=@bar WHERE baz=@baz; UPDATE foo SET bar=@bar WHERE baz=@baz\"); // set parameters. cmd.Prepare(); Although there are two statements in this command, the same prepared statement is used to execute since the SQL is identical. Persistency Prior to 3.2, prepared statements were closed when their owning command was disposed. This significantly reduced their usefulness, especially since closing a pooled connection automatically closed all prepared statements. For applications where connections are short-lived, such as most web applications, this effectively made prepared statements useless. Starting from 3.2, all prepared statements are persistent - they no longer get closed when a command or connection is closed. Npgsql keeps track of statements prepared on each physical connection; if you prepare the same SQL a second time on the same connection, Npgsql will simply reuse the prepared statement from the first preparation. This means that in an application with short-lived, pooled connections, prepared statements will gradually be created as the application warms up and the connections are first used. Then, opening a new pooled connection will return a physical connection that already has a prepared statement for your SQL, providing a very substantial performance boost. For example: using (var conn = new NpgsqlConnection(...) using (var cmd = new NpgsqlCommand(\"<some_sql>\", conn) { conn.Open(); cmd.Prepare(); // First time on this physical connection, Npgsql prepares with PostgreSQL cmd.ExecuteNonQuery(); } using (var conn = new NpgsqlConnection(...) using (var cmd = new NpgsqlCommand(\"<some_sql>\", conn) { conn.Open(); // We assume the pool returned the same physical connection used above cmd.Prepare(); // The connection already has a prepared statement for <some_sql>, this doesn't need to do anything cmd.ExecuteNonQuery(); } You can still choose to close a prepared statement by calling NpgsqlCommand.Unprepare() . You can also unprepare all statements on a given connection by calling NpgsqlConnection.UnprepareAll() . Automatic Preparation While the preparation examples shown above provide a very significant performance boost, they depend on you calling the Prepare() command. Unfortunately, if you're using some data layer above ADO.NET, such as Dapper or Entity Framework , chances are these layers don't prepare for you. While issues exist for both Dapper and Entity Framework Core , they don't take advantage of prepared statement at the moment. Npgsql 3.2 introduces automatic preparation. When turned on, this will make Npgsql track the statements you execute and automatically prepare them when you reach a certain threshold. When you reach that threshold, the statement is automatically prepared, and from that point on will be executed as prepared, yielding all the performance benefits discussed above. To turn on this feature, you simply need to set the Max Auto Prepare connection string parameter, which determines how many statements can be automatically prepared on the connection at any given time (this parameter defaults to 0, disabling the feature). A second parameter, Auto Prepare Min Usages , determines how many times a statement needs to be executed before it is auto-prepared (defaults to 5). Since no code changes are required, you can simply try setting Max Auto Prepare and running your application to see an immediate speed increase. Note also that, like explicitly-prepared statements, auto-prepared statements are persistent, allowing you to reap the performance benefits in short-lived connection applications. Note that if you're coding directly against Npgsql or ADO.NET, explicitly preparing your commands with Prepare() is still recommended over letting Npgsql prepare automatically. Automatic preparation does incur a slight performance cost compared to explicit preparation, because of the internal LRU cache and various book-keeping data structures. Explicitly preparing also allows you to better control exactly which statements are prepared and which aren't, and ensures your statements will always stay prepared, and never get ejected because of the LRU mechanism. Note that automatic preparation is a complex new feature which should be considered somewhat experimental; test carefully, and if you see any strange behavior or problem try turning it off."
  },
  "dev/build-server.html": {
    "href": "dev/build-server.html",
    "title": "Build Server Notes | Npgsql Documentation",
    "keywords": "This page describes the steps used to set up the Npgsql build server. If you're upgrading the TeamCity version, see \"Give agent service start/stop permissions\" below. Install all supported versions of the Postgresql backend At the time of writing, this means 9.1, 9.2, 9.3, 9.4, 9.5. They are configured on ports 5491, 5492, 5493, 5494, 5495. For SSPI/GSS tests, you need to set up a user with the same name as the user that will be running the tests (i.e. teamcity_agent). You must also add the following lines at the top of each PG's pg_hba.conf to set up SSPI/GSS for that user: host all teamcity_agent 127.0.0.1/32 sspi include_realm=0 host all teamcity_agent ::1/128 sspi include_realm=0 See this page on SSPI . Install a TeamCity-dedicated Postgresql cluster TeamCity itself requires an SQL database, but we don't want it to run in the same environment as that used for the unit tests. So choosing the latest stable Postgresql version (9.6 at time of writing), we create a new Postgresql cluster: initdb -U postgres -W c:\\dev\\TeamcityPostgresData Next we set up a Windows service that starts up the new cluster: pg_ctl register -N postgresql-9.6-teamcity -U teamcity -P <password> -D c:\\dev\\TeamcityPostgresData Finally, create a a user and database and point TeamCity to it. Install .NET SDKs for all supported .NET versions .NET 4.0 (Windows 7 SDK): http://www.microsoft.com/en-us/download/details.aspx?id=8279 .NET 4.5 (Windows 8 SDK): http://msdn.microsoft.com/en-us/windows/hardware/hh852363.aspx .NET 4.5.1 (Windows 8.1 SDK): http://msdn.microsoft.com/en-us/windows/hardware/bg162891.aspx While installing the SDK for .NET 4.0, I had this problem: http://support.microsoft.com/kb/2717426 Give agent service start/stop permissions When upgrading TeamCity, the agent needs to be able to stop and start the Windows service. This is how you can grant a normal user specific permissions on specific services: Download and install subinacl from http://www.microsoft.com/en-us/download/details.aspx?id=23510 cd C:\\Program Files (x86)\\Windows Resource Kits\\Tools\\ subinacl /service TCBuildAgent /grant=teamcity_agent=TO Update build status back in github Download the plugin from https://github.com/jonnyzzz/TeamCity.GitHub , get the ZIP Drop the ZIP in the TeamCity content dir's plugins subdir Add the Build Feature \"Report change status to GitHub\". Configure everything appropriately, and be sure the user you set up has push access to the repository! Install assorted dev utilities GitVersion (with Chocolatey) WiX toolset (v3.10.1 at time of writing) Install WiX WiX 3.10 has a dependency on .NET Framework 3.5, but there's some issue blocking its installation on Windows Server 2012 R2 (at least on Azure). A good workaround is to simply install via Powershell ( Add-WindowsFeature NET-Framework-Core ), see https://msdn.microsoft.com/en-us/library/dn169001(v=nav.70).aspx#InstallNET35 . Note that ICE validation is disabled because apparently it requires an interactive account or admin privileges, which doesn't work in continuous integration."
  },
  "doc/keepalive.html": {
    "href": "doc/keepalive.html",
    "title": "Keepalive | Npgsql Documentation",
    "keywords": "Keepalive Some clients keep idle connections for long periods of time - this is especially frequent when waiting for PostgreSQL notifications. In this scenario, how can the client know the connection is still up, and hasn't been broken by a server or network outage? For this purpose, Npgsql has a keepalive feature, which makes it send periodic SELECT NULL queries. This feature is by default disabled, and must be enabled via the Keepalive connection string parameter, setting the number of seconds between each keepalive. When keepalive is enabled, Npgsql will emit an NpgsqlConnection.StateChange event if the keepalive fails. Note that you should only turn this feature on if you need it. Unless you know you'll have long-lived idle connections, and that your backend (or network equipment) will interfere with these connections, it's better to leave this off. TCP Keepalives The keepalive mechanism above is ideal for long-standing idle connections, but it cannot be used during query processing. With some PostgreSQL-like data warehouse products such as Amazon Redshift or Greenplum , it is not uncommon for a single SQL statement to take a long time to execute, and during that time it is not possible to send SELECT NULL . For these cases you may want to turn on TCP keepalive , which is quite different from the application-level keepalive described above. To better understand the different kinds of keepalives, see this blog post . As that article explains, TCP keepalive depends on networking stack support and might not always work, but it is your only option during query processing."
  },
  "doc/security.html": {
    "href": "doc/security.html",
    "title": "Security and Encryption | Npgsql Documentation",
    "keywords": "Security and Encryption Logging in The simplest way to log into PostgreSQL is by specifying a Username and a Password in your connection string. Depending on how your PostgreSQL is configured (in the pg_hba.conf file), Npgsql will send the password in MD5 or in cleartext (not recommended). For documentation about all auth methods supported by PostgreSQL, see this page . Note that Npgsql supports Unix-domain sockets (auth method local ), simply set your Host parameter to the absolute path of your PostgreSQL socket directory, as configred in your postgresql.conf . Integrated Security (GSS/SSPI/Kerberos) Logging in with a username and password isn't recommended, since your application must have access to your password. An alternate way of authenticating is \"Integrated Security\", which uses GSS or SSPI to negotiate Kerberos. The advantage of this method is that authentication is handed off to your operating system, using your already-open login session. Your application never needs to handle a password. You can use this method for a Kerberos login, Windows Active Directory or a local Windows session. Note that since 3.2, this method of authentication also works on non-Windows platforms. Instructions on setting up Kerberos and SSPI are available in the PostgreSQL auth methods docs . Some more instructions for SSPI are available here . Once your PostgreSQL is configured correctly, simply include Integrated Security=true in your connection string and drop the Password parameter. However, Npgsql must still send a username to PostgreSQL. If you specify a Username connection string parameter, Npgsql will send that as usual. If you omit it, Npgsql will attempt to detect your system username, including the Kerberos realm. Note that by default, PostgreSQL expects your Kerberos realm to be sent in your username (e.g. username@REALM ); you can have Npgsql detect the realm by setting Include Realm to true in your connection string. Alternatively, you can disable add include_realm=0 in your PostgreSQL's pg_hba.conf entry, which will make it strip the realm. You always have the possibility of explicitly specifying the username sent to PostgreSQL yourself. Encryption (SSL/TLS) By default PostgreSQL connections are unencrypted, but you can turn on SSL/TLS encryption if you wish. First, you have to set up your PostgreSQL to receive SSL/TLS connections as described here . Once that's done, specify SSL Mode in your connection string, setting it to either Require (connection will fail if the server isn't set up for encryption), or Prefer (use encryption if possible but fallback to unencrypted otherwise). Note that by default, Npgsql will verify that your server's certificate is valid. If you're using a self-signed certificate this will fail. You can instruct Npgsql to ignore this by specifying Trust Server Certificate=true in the connection string. To precisely control how the server's certificate is validated, you can register UserCertificateValidationCallback on NpgsqlConnection (this works just like on .NET's SSLStream ). You can also have Npgsql provide client certificates to the server by registering the ProvideClientCertificatesCallback on NpgsqlConnection (this works just like on .NET's SSLStream )."
  },
  "doc/types/datetime.html": {
    "href": "doc/types/datetime.html",
    "title": "Date and Time Handling | Npgsql Documentation",
    "keywords": "Date and Time Handling Handling date and time values usually isn't hard, but you must pay careful attention to differences in how the .NET types and PostgreSQL represent dates. It's worth reading the PostgreSQL date/time type documentation to familiarize yourself with PostgreSQL's types. Warning : a common mistake is for users to think that the PostgreSQL timestamp with timezone type stores the timezone in the database. This is not the case: only the timestamp is stored. There is no one PostgreSQL type that stores both a date/time and a timezone, similar to .NET DateTimeOffset . .NET types and PostgreSQL types The .NET and PostgreSQL types differ in the resolution and range they provide; the .NET type usually have a higher resolution but a lower range than the PostgreSQL types: PostgreSQL type Precision/Range .NET Native Type Precision/Range Npgsql .NET Provider-Specific Type timestamp 1 microsecond, 4713BC-294276AD DateTime 100 nanoseconds, 1AD-9999AD NpgsqlDateTime timestamp with timezone 1 microsecond, 4713BC-294276AD DateTime 100 nanoseconds, 1AD-9999AD NpgsqlDateTime date 1 day, 4713BC-5874897AD DateTime 100 nanoseconds, 1AD-9999AD NpgsqlDate time 1 microsecond, 0-24 hours TimeSpan 100 nanoseconds, -10,675,199 - 10,675,199 days N/A time with timezone 1 microsecond, 0-24 hours DateTimeOffset (ignore date) 100 nanoseconds, 1AD-9999AD N/A interval 1 microsecond, -178000000-178000000 years TimeSpan 100 nanoseconds, -10,675,199 - 10,675,199 days NpgsqlTimeSpan If your needs are met by the .NET native types, it is best that you use them directly with Npgsql. If, however, you require the extended range of a PostgreSQL type you can use Npgsql's provider-specific types, which represent PostgreSQL types in an exact way. Timezones It's critical to understand exactly how timezones and timezone conversions are handled between .NET types and PostgreSQL. In particular, .NET's DateTime has a Kind property which impacts how Npgsql reads and writes the value. For PostgreSQL timestamp without time zone and time (without time zone), the database value's timezone is unknown or undefined. Therefore, Npgsql does no timezone conversion whatsoever and always sends your DateTime as-is, regardless of its Kind. For DateTimeOffset, the timestamp component is sent as-is (i.e. the timezone component is discarded). For PostgreSQL timestamp with time zone , the database value's timezone is expected to always be UTC (the timezone isn't saved in the database). As a result, a DateTime Offset as well as a DateTime with Kind=Local Npgsql will be converted to UTC before being sent to the database. When reading a timestamptz, the database UTC timestamp will be returned as a DateTime with Kind=Local or a DateTimeOffset in the local timezone ( UNIMPLEMENTED? ). PostgreSQL time with time zone is the only date/time type which stores a timezone in the database. Accordingly, your DateTime's Kind will determine the the timezone sent to the database. Detailed Behavior: Sending values to the database .NET value PG type Action DateTime(Kind=UTC) timestamp Send as-is DateTime(Kind=Local) timestamp (default) Send as-is DateTime(Kind=Unspecified) timestamp (default) Send as-is DateTimeOffset timestamp Strip offset, send as-is DateTime(Kind=UTC) timestamptz (default) Send as-is DateTime(Kind=Local) timestamptz Convert to UTC locally before sending DateTime(Kind=Unspecified) timestamptz Send as-is DateTimeOffset timestamptz (default) Convert to UTC locally before sending TimeSpan time Send as-is DateTime(Kind=UTC) timetz Send time and UTC timezone DateTime(Kind=Local) timetz Send time and local system timezone DateTime(Kind=Unspecified) timetz Assume local, send time and local system timezone DateTimeOffset timetz Send time and timezone Detailed Behavior: Reading values from the database PG type .NET value Action timestamp DateTime (default) Kind=Unspecified timestamp DateTimeOffset Should throw an exception? timestamptz DateTime (default) Kind=Local (according to system tz) timestamptz DateTimeOffset Offset=UTC time TimeSpan (default) As-is time DateTime Use only time component time DateTimeOffset Exception? timetz TimeSpan Strip offset, read as-is timetz DateTime Use only time component, throw away time zone timetz DateTimeOffset (default) Use only time- and time zone component Further Reading If you're really interested in some of the mapping decisions above, check out this issue ."
  },
  "efcore/misc.html": {
    "href": "efcore/misc.html",
    "title": "Misc | Npgsql Documentation",
    "keywords": "Misc Setting up PostgreSQL extensions The provider allows you to specify PostgreSQL extensions that should be set up in your database. Simply use HasPostgresExtension in your context's OnModelCreating: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.HasPostgresExtension(\"hstore\"); } Optimistic Concurrency and Concurrency Tokens Entity Framework supports the concept of optimistic concurrency - a property on your entity is designated as a concurrency token, and EF detects concurrent modifications by checking whether that token has changed since the entity was read. You can read more about this in the EF docs . Although applications can update concurrency tokens themselves, we frequently rely on the database automatically updating a column on update - a \"last modified\" timestamp, an SQL Server rowversion , etc. Unfortunately PostgreSQL doesn't have such auto-updating columns - but there is one feature that can be used for concurrency token. All PostgreSQL tables have a set of implicit and hidden system columns , among which xmin holds the ID of the latest updating transaction. Since this value automatically gets updated every time the row is changed, it is ideal for use as a concurrency token. To enable this feature on an entity, insert the following code into your model's OnModelCreating method: modelBuilder.Entity<MyEntity>().ForNpgsqlUseXminAsConcurrencyToken(); PostgreSQL Index Methods PostgreSQL supports a number of index methods , or types . These are specified at index creation time via the USING _method_ clause, see the PostgreSQL docs for CREATE INDEX and this page for info on the different types. The Npgsql EF Core provider allows you to specify the index method to be used by specifying ForNpgsqlHasMethod() on your index in your context's OnModelCreating: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<Blog>() .HasIndex(b => b.Url) .ForNpgsqlHasMethod(\"gin\"); } Execution Strategy Since 2.0.0, the Npgsql provider provides a retrying execution strategy, which will attempt to detect most transient PostgreSQL/network errors and will automatically retry your operation. To enable, place the following code in your context's OnModelConfiguring : .UseNpgsql( \"<connection string>\", options => options.EnableRetryOnFailure()); This strategy relies on NpgsqlException's IsTransient property. Both this property and the retrying strategy are new and should be considered somewhat experimental - please report any issues. Comments PostgreSQL allows you to attach comments to database objects, which can help explain their purpose for someone examining the schema. The EF Core provider supports this for tables or columns, simply set the comment in your model's OnModelCreating as follows: modelBuilder.Entity<MyEntity>().ForNpgsqlHasComment(\"Some comment\"); Specifying the administrative db When the Npgsql EF Core provider creates or deletes a database (EnsureCreated(), EnsureDeleted()), it must connect to an administrative database which already exists (with PostgreSQL you always have to be connected to some database, even when creating/deleting another database). Up to now the postgres database was used, which is supposed to always be present. However, there are some PostgreSQL-like databases where the postgres database isn't available. For these cases you can specify the administrative database as follows: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) { optionsBuilder.UseNpgsql(\"<connection_string>\", b => b.UseAdminDatabase(\"<admin_db>\")); } Using a database template When creating a new database, PostgreSQL allows specifying another \"template database\" which will be copied as the basis for the new one. This can be useful for including database entities which aren't managed by Entity Framework. You can trigger this by using HasDatabaseTemplate in your context's OnModelCreating : modelBuilder.HasDatabaseTemplate(\"my_template_db\"); CockroachDB Interleave In Parent If you're using CockroachDB, the Npgsql provider exposes its \"interleave in parent\" feature . Use the following code: modelBuilder.Entity<Customer>() .ForCockroachDbInterleaveInParent(typeof(ParentEntityType), new List<string> { \"prefix_column_1\", \"prefix_column_2\" });"
  },
  "doc/wait.html": {
    "href": "doc/wait.html",
    "title": "Waiting for Notifications | Npgsql Documentation",
    "keywords": "Waiting for Notifications Note: This functionality replaces Npgsql 3.0's \"Continuous processing mode\" . PostgreSQL Asynchronous messages PostgreSQL has a feature whereby arbitrary notification messages can be sent between clients. For example, one client may wait until it is notified by another client of a task that it is supposed to perform. Notifications are, by their nature, asynchronous - they can arrive at any point. For more detail about this feature, see the PostgreSQL NOTIFY command . Some other asynchronous message types are notices (e.g. database shutdown imminent) and parameter changes, see the PostgreSQL protocol docs for more details. Note that despite the word \"asynchronous\", this page has nothing to do with ADO.NET async operations (e.g. ExecuteReaderAsync). Processing of Notifications Npgsql exposes notification messages via the Notification event on NpgsqlConnection. Since asynchronous notifications are rarely used and processing can be complex, Npgsql only processes notification messages as part of regular (synchronous) query interaction. That is, if an asynchronous notification is sent, Npgsql will only process it and emit an event to the user the next time a command is sent and processed. To receive notifications outside a synchronous request-response cycle, call NpgsqlConnection.Wait() . This will make your thread block until a single notification is received (note that a version with a timeout as well as an async version exist). Note that the notification is still delivered via the Notification event as before. var conn = new NpgsqlConnection(ConnectionString); conn.Open(); conn.Notification += (o, e) => Console.WriteLine(\"Received notification\"); while (true) { conn.Wait(); // Thread will block here } Keepalive You may want to turn on keepalives ."
  },
  "doc/migration/3.2.html": {
    "href": "doc/migration/3.2.html",
    "title": "Migration Notes | Npgsql Documentation",
    "keywords": "Npgsql 3.2 Npgsql 3.2 is out and available on nuget.org. This is a major release with substantial internal changes and should be deployed with care. For critical applications it may be advisable to wait until 3.2.1 is out. This release contains a large number of new features, but the main focus is performance - some usage scenarios may show dramatic improvements. See below for more details. Major Changes Prepared statements are now persistent (survive beyond pooled connection close/open), providing significant performance improvements for applications with short-lived connections, such as most webapps ( #483 ). Also, statements can optionally be prepared automatically by Npgsql based on use, unlocking prepared statement performance for O/RMs and data layers which don't prepare themselves, such as Dapper or Entity Framework Core ( #1237 ). See this blog post for more info . The internal I/O system has been overhauled to continue supporting sync and async I/O, but with a vastly better coding model. This should eliminate most protocol sync bugs, and make it much easier to maintain and write new type handlers ( #1326 ). Kerberos login (\"integrated security\") is now support on Linux/Mac ( #1079 ). Support for System.Transactions and distributed transactions has been rewritten, and should have fewer problems than before ( #122 ). Performance counters have been implemented, similar to what SqlClient provides . See the documentation for more information ( #619 ). The Visual Studio integration extension (DDEX) has been rewritten for a much better installation experience, and includes some new features as well ( #1407 ). See the docs for more info . If your application attempts to make use of more than one connection at the same time, an \"operation already in progress\" was thrown. This exception now provides more information to help you track down the bug ( #1248 ). Many other small changes have been made, especially with regards to performance. Here's the full list . Breaking Changes from 3.1 Connections can no longer be constructed with NpgsqlConnectionStringBuilder - only plain string connection strings are supported ( #1415 ). The Buffer Size connection string parameter has been replaced by Read Buffer Size and Write Buffer Size ."
  },
  "doc/index.html": {
    "href": "doc/index.html",
    "title": "Documentation | Npgsql Documentation",
    "keywords": "Getting Started The best way to use Npgsql is to install its nuget package . Npgsql aims to be fully ADO.NET-compatible, its API should feel almost identical to other .NET database drivers. Here's a basic code snippet to get you started. var connString = \"Host=myserver;Username=mylogin;Password=mypass;Database=mydatabase\"; using (var conn = new NpgsqlConnection(connString)) { conn.Open(); // Insert some data using (var cmd = new NpgsqlCommand()) { cmd.Connection = conn; cmd.CommandText = \"INSERT INTO data (some_field) VALUES (@p)\"; cmd.Parameters.AddWithValue(\"p\", \"Hello world\"); cmd.ExecuteNonQuery(); } // Retrieve all rows using (var cmd = new NpgsqlCommand(\"SELECT some_field FROM data\", conn)) using (var reader = cmd.ExecuteReader()) while (reader.Read()) Console.WriteLine(reader.GetString(0)); } You can find more info about the ADO.NET API in the MSDN docs or in many tutorials on the Internet. DbProviderFactory The example above involves some Npgsql-specific types ( NpgsqlConnection , NpgsqlCommand ...), which makes your application Npgsql-specific. If your code needs to be database-portable, you should use the ADO.NET DbProviderFactory API instead ( see this tutorial ). In a nutshell, you register Npgsql's provider factory in your application's App.config (or machines.config ) file, and then obtain it in your code without referencing any Npgsql-specific types. You can then use the factory to create a DbConnection (which NpgsqlConnection extends), and from there a DbCommand and so on. To do this, add the following to your App.config : <system.data> <DbProviderFactories> <add name=\"Npgsql Data Provider\" invariant=\"Npgsql\" description=\".Net Data Provider for PostgreSQL\" type=\"Npgsql.NpgsqlFactory, Npgsql, Culture=neutral, PublicKeyToken=5d8b90d52f46fda7\"/> </DbProviderFactories> </system.data> GAC Installation In some cases you'll want to install Npgsql into your Global Assembly Cache (GAC) . This is usually the case when you're using a generic database program that can work with any ADO.NET provider but doesn't come with Npgsql or reference it directly. For these cases, you can download the Npgsql Windows installer from our Github releases page : it will install Npgsql (and optionally the Entity Framework providers) into your GAC and add Npgsql's DbProviderFactory into your machine.config file. This is not the general recommended method of using Npgsql - always install via Nuget if possible. In addition to Npgsql.dll, this will also install System.Threading.Tasks.Extensions.dll into the GAC. Visual Studio Integration If you'd like to have Visual Studio Design-Time support, give our VSIX extension a try . Unstable Packages The Npgsql build server publishes CI nuget packages for every build. If a bug affecting you was fixed but there hasn't yet been a patch release, you can get a CI nuget at our stable MyGet feed . These packages are generally stable and safe to use (although it's better to wait for a release). We also publish CI packages for the next minor/major version at our unstable MyGet feed . These are definitely unstable and should be used with care."
  },
  "index.html": {
    "href": "index.html",
    "title": "Npgsql - .NET Access to PostgreSQL | Npgsql Documentation",
    "keywords": "Npgsql - .NET Access to PostgreSQL About Npgsql is an open source ADO.NET Data Provider for PostgreSQL, it allows programs written in C#, Visual Basic, F# to access the PostgreSQL database server. It is implemented in 100% C# code, is free and is open source. In addition, providers have been written for Entity Framework Core and for Entity Framework 6.x. Getting Help The best way to get help for Npgsql is to post a question to Stack Overflow and tag it with the npgsql tag. If you think you've encountered a bug or want to request a feature, open an issue in the appropriate project's github repository. License Npgsql is licensed under the PostgreSQL License , a liberal OSI-approved open source license. Thanks A special thanks to Rowan Miller, Scott Hanselman and Martin Woodward at Microsoft for generously donating an Azure subscription for Npgsql’s continuous integration platform. Contributors Current active contributors to Npgsql are: Shay Rojansky Emil Lenngren Francisco Figueiredo Jr. Kenji Uno Past contributors to Npgsql: Jon Asher Josh Cooley Federico Di Gregorio Jon Hanna Chris Morgan Dave Page Glen Parker Brar Piening Hiroshi Saito Developer Resources For information on development see this page ."
  },
  "doc/ddex.html": {
    "href": "doc/ddex.html",
    "title": "Visual Studio Integration | Npgsql Documentation",
    "keywords": "Visual Studio Integration Npgsql has a Visual Studio extension (VSIX) which integrates PostgreSQL access into Visual Studio. It allows connecting to PostgreSQL from within Visual Studio's Server Explorer, create an Entity Framework 6 model from an existing database, etc. The extension can be installed directly from the Visual Studio Marketplace page . The VSIX doesn't automatically add Npgsql to your GAC, App.config , machines.config or any other project or system-wide resource. It only allows accessing PostgreSQL from Visual Studio itself. Visual Studio Compatibility The VSIX extension has been tested and works on Visual Studio 2015 and 2017. It is probably compatible with versions all the way back to 2012, but these haven't been tested. Note that installing into pre-2015 versions will display a warning, although it should be safe to proceed. Upgrading from an older version Note that the extension has been pretty much rewritten for Npgsql 3.2 - if you encountered installation issues with previous versions, these issues should hopefully be gone. A summary of work done for 3.2 is available here . If you already have an earlier version of the VSIX (or MSI) installed, it's highly recommended that you uninstall them to avoid conflicts. It is no longer necessary or recommended to have Npgsql in your GAC, or to have Npgsql listed in your machines.config. Simply installing the VSIX should work just fine, and a GAC/machines.config may actually cause issues. If you previously installed Npgsql into your GAC/machines.config, it's recommended you uninstall it. If you have any entries (binding redirects, DbProviderFactory registrations) in either your machines.config or in your Visual Studio setup (e.g. App.config, devenv.exe.config ), please remove them The VSIX should work on a totally clean setup. Features The provider isn't feature comlete - please let us know of missing features or bugs by opening issues. Server Explorer You can add a PostgreSQL database in Server Explorer, explore tables and columns, send ad-hoc queries, etc. Entity Framework 6 The extension supports generating a model from an existing database. To do so, install EntityFramework6.Npgsql into your project, and then make sure you have the same version of Npgsql as your extension does. A mismatch between the version installed in your project and the VSIX's may cause issues. Development Development on the VSIX is currently possible only on Visual Studio 2017. Be sure to install the \"Visual Studio extension development\" workload."
  },
  "doc/migration/3.1.html": {
    "href": "doc/migration/3.1.html",
    "title": "Migration Notes | Npgsql Documentation",
    "keywords": "Migrating from 3.0 to 3.1 CommandTimeout used to be implemented with PostgreSQL's statement_timeout parameter, but this wasn't a very reliable method and has been removed. CommandTimeout is now implemented via socket timeouts only, see #689 for more details. Note that if a socket timeout occurs, the connection is broken and must be reopened. The Persist Security Info parameter has been implemented and is false by default. This means that once a connection has been opened, you will not be able to get its password. Removed ContinuousProcessing mode, and replaced it with Wait , a simpler and less bug-prone mechanism for consuming asynchronous notifications ( #1024 ). The Maximum Pool Size connection is parameter is now 100 default instead of 20 (this is default in SqlClient, pg_bouncer...). The Connection Lifetime parameter has been renamed to Connection Idle Lifetime , and its default has been changed from 15 to 300. Also, once the number of seconds has elapsed the connection is closed immediately; the previous behavior closed half of the connections. RegisterEnum and RegisterEnumGlobally have been renamed to MapEnum and MapEnumGlobally respectively. If you used enum mapping in 3.0, the strategy for translating between CLR and PostgreSQL type names has changed. In 3.0 Npgsql simply used the CLR name (e.g. SomeField) as the PostgreSQL name; Npgsql 3.1 uses a user-definable name translator, default to snake case (e.g. some_field). See #859 . The EnumLabel attribute has been replaced by the PgName attribute (which is also used for the new composite type support). When PostgreSQL sends an error, it is no longer raised by an NpgsqlException but by a PostgresException. PostgresException is a subclass of NpgsqlException so code catching NpgsqlException should still work, but the PostgreSQL-specific exception properties will only be available on PostgresException. The Code property on NpgsqlException has been renamed to SqlState. It has also been moved to PostgresException. NpgsqlNotice has been renamed to PostgresNotice For multistatement commands, PostgreSQL parse errors will now be thrown only when the user calls NextResult() and gets to the problematic statement. It is no longer possible to dispose a prepared statement while a reader is still open. Since disposing a prepared statement includes database interaction, the connection must be idle. Removed NpgsqlConnection.SupportsHexByteFormat . Renamed NpgsqlConnection.Supports_E_StringPrefix to SupportsEStringPrefix ."
  },
  "efcore/migration/1.1.html": {
    "href": "efcore/migration/1.1.html",
    "title": "Migrating to 1.1 | Npgsql Documentation",
    "keywords": "Migrating to 1.1 Version 1.1.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget. This version works with version 1.1.0 of Entity Framework Core , and contains some new Npgsql features as well. Note that if you're using the command-line tools, you'll have to modify your tools section as described in the EF Core release post: \"tools\": { \"Microsoft.EntityFrameworkCore.Tools.DotNet\": \"1.0.0-preview3-final\" }, New Features Aside from general EF Core features, version 1.1.0 of the Npgsql provider contains the following: Hilo key generation ( #5 ). This can be a much more efficient way to generate autoincrement key values. PostgreSQL array mapping ( #15 ). This allows you to have plain CLR arrays on your entities, and have those arrays mapped to native PostgreSQL array columns . Optimistic concurrency with PostgreSQL's xmin column ( #19 ). Simply specify .UseXminAsConcurrencyToken() on an entity to start using this, see the EF docs for more details . Cleanup of how serial (autoincrement) and generated GUID/UUID columns are managed. Here's the full list of issues . Please report any problems to https://github.com/npgsql/Npgsql.EntityFrameworkCore.PostgreSQL . Upgrading from 1.0.x If you've used 1.0.x without migrations, you can simply upgrade and everything should just work. Unfortunately, if you already have migrations from 1.0.x you'll have to do some manual fixups because of some bad decisions that were previously made. If deleting your old migrations and starting over (e.g. non-production database) is an option, you may wish to do so. The following are instructions for fixing up 1.0.x migrations. First, Npgsql 1.0.x used a problematic method to identify serial (autoincrement) columns in migrations. If you look at your migration code you'll see .Annotation(\"Npgsql:ValueGeneratedOnAdd\", true) on various columns. Unfortunately this annotation is also present on non-serial columns, e.g. columns with default values. This causes various issues and has been replaced in 1.1. However, you'll have to manually remove .Annotation(\"Npgsql:ValueGeneratedOnAdd\", true) , and replace it with .Annotation(\"Npgsql:ValueGenerationStrategy\", NpgsqlValueGenerationStrategy.SerialColumn) but only on columns which should be serial (e.g. not on columns with defaults). If you attempt to run a migration that has the old annotation, Npgsql will throw an exception and refuse to run your migrations. Unfortunately, this change will cause some incorrect changes the first time you add a migration after the upgrade. To avoid this, simply add a dummy migration right after upgrading to 1.1 and then delete the two new files generated for the dummy migration, but keep the changes made to your ModelSnapshot.cs . From this point on everything should be fine. Make sure you have no pending changes to your model before doing this! . Apologies for this problematic upgrade procedure, it should at least keep things clean going forward."
  },
  "doc/types/enums_and_composites.html": {
    "href": "doc/types/enums_and_composites.html",
    "title": "Accessing PostgreSQL Enums and Composites | Npgsql Documentation",
    "keywords": "Accessing PostgreSQL Enums and Composites PostgreSQL supports enum types and composite types as database columns, and Npgsql supports mapping these to your CLR types. This allows you to seamlessly and efficiently read and write enum values to the database without worrying about conversions. Note: composite type support was introduced in Npgsql 3.1 Mapping CLR types to PostgreSQL enums and composites In order to use enum and composite types with Npgsql, you must map your CLR type to the PostgreSQL enum or composite. This must be done in advance, before reading or writing. The easiest way to do this is to call MapEnumGlobally or MapCompositeGlobally before opening any connections; the following will set up a mapping between your CLR SomeEnum type to a PostgreSQL enum called some_enum (more on the name translation later). enum SomeEnum { ... } NpgsqlConnection.MapEnumGlobally<SomeEnum>(); Similarly, the following will set up a mapping between CLR type SomeType and a PostgreSQL composite called some_type . The CLR type can be a class or struct. class SomeType { ... } NpgsqlConnection.MapCompositeGlobally<SomeType>(); Note that the PostgreSQL types must have been create in advance (with CREATE TYPE ). If you don't want to map an enum for all your connections, you can register a mapping for one connection only by calling MapEnum on an NpgsqlConnection instance. Name Translation Since Npgsql 3.1, pluggable name translators are used to map CLR type and field names to PostgreSQL ones. The default translation scheme is NpgsqlSnakeCaseNameTranslator , which maps names like SomeType to some_type. However, when calling the mapping methods you can pass your own name translator which implements INpgsqlNameTranslator. Finally, you may control mappings on a field-by-field basis via the [PgName] attribute. This will override the name translator. using NpgsqlTypes; enum SomeEnum { [PgName(\"happy\")] Good, [PgName(\"sad\")] Bad } Reading and Writing Once your mapping is set up, you can read and write enums and composites like any other type: // Writing using (var cmd = new NpgsqlCommand(\"INSERT INTO some_table (some_enum, some_type) VALUES (@p1, @p2)\", Conn)) { cmd.Parameters.AddWithValue(\"p1\", SomeEnum.Good); cmd.Parameters.AddWithValue(\"p2\", new SomeType { ... }); cmd.ExecuteNonQuery(); } // Reading using (var cmd = new NpgsqlCommand(\"SELECT some_enum, some_type FROM some_table\", Conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); var enumValue = reader.GetFieldValue<SomeEnum>(0); var compositeValue = reader.GetFieldValue<SomeType>(1); }"
  },
  "ef6/index.html": {
    "href": "ef6/index.html",
    "title": "Entity Framework 6 | Npgsql Documentation",
    "keywords": "Npgsql has an Entity Framework 6 provider. You can use it by installing the EntityFramework6.Npgsql nuget. Guid Support Npgsql EF migrations support uses uuid_generate_v4() function to generate guids. In order to have access to this function, you have to install the extension uuid-ossp through the following command: create extension \"uuid-ossp\"; If you don't have this extension installed, when you run Npgsql migrations you will get the following error message: ERROR: function uuid_generate_v4() does not exist If the database is being created by Npgsql Migrations, you will need to run the create extension command in the template1 database . This way, when the new database is created, the extension will be installed already. Template Database When the Entity Framework 6 provider creates a database, it issues a simple CREATE DATABASE command. In PostgreSQL, this implicitly uses template1 as the template - anything existing in template1 will be copied to your new database. If you wish to change the database used as a template, you can specify the EF Template Database connection string parameter. For more info see the PostgreSQL docs ."
  },
  "dev/types.html": {
    "href": "dev/types.html",
    "title": "PostgreSQL Types | Npgsql Documentation",
    "keywords": "Overview The following are notes by Emil Lenngren on PostgreSQL wire representation of types: bool: text: t or f binary: a byte: 1 or 0 bytea: text: either \\x followed by hex-characters (lowercase by default), or plain characters, where non-printable characters (between 0x20 and 0x7e, inclusive) are written as \\nnn (octal) and \\ is written as \\\\ binary: the bytes as they are char: This type holds a single char/byte. (Not to be confused with bpchar (blank-padded char) which is PostgreSQL's alias to the SQL standard's char). The char may be the null-character text: the char as a byte, encoding seems to be ignored binary: the char as a byte name: A null-padded string of NAMEDATALEN (currently 64) bytes (the last byte must be a null-character). Used in pg catalog. text: the name as a string binary: the name as a string int2/int4/int8: text: text representation in base 10 binary: binary version of the integer int2vector: non-null elements, 0-indexed, 1-dim text: 1 2 3 4 binary: same as int2[] oidvector: non-null elements, 0-indexed, 1-dim text: 1 2 3 4 binary: same as oid[] regproc: internally just an OID (UInt32) text: -, name of procedure, or numeric if not found binary: only the OID in binary regprocedure/regoper/regoperator/regclass/regconfig/regdictionary: similar to regproc text: text: the string as it is binary: the string as it is oid: A 32-bit unsigned integer used for internal object identification. text: the text-representation of this integer in base 10 binary: the UInt32 tid: tuple id Internally a tuple of a BlockNumber (UInt32) and an OffsetNumber (UInt16) text: (blockNumber,offsetNumber) binary: the block number in binary followed by offset number in binary xid: transaction id Internally just a TransactionId (UInt32) text: the number binary: the number in binary cid: command id Internally just a CommandId (UInt32) text: the number binary: the number in binary json: json text: the json an text binary: the json as text jsonb: json internally stored in an efficient binary format text: the json as text binary: An Int32 (version number, currently 1), followed by data (currently just json as text) xml: Xml. It is probably most efficient to use the text format, especially when receiving from client. text: the xml as text (when sent from the server: encoding removed, when receiving: assuming database encoding) binary: the xml as text (when sent from the server: in the client's specified encoding, when receiving: figures out itself) pg_node_tree: used as type for the column typdefaultbin in pg_type does not accept input text: text binary: text smgr: storage manager can only have the value \"magnetic disk\" text: magnetic disk binary: not available point: A tuple of two float8 text: (x,y) The floats are interpreted with the C strtod function. The floats are written with the snprintf function, with %.*g format. NaN/-Inf/+Inf can be written, but not interpretability depends on platform. The extra_float_digits setting is honored. For linux, NaN, [+-]Infinity, [+-]Inf works, but not on Windows. Windows also have other output syntax for these special numbers. (1.#QNAN for example) binary: the two floats lseg: A tuple of two points text: [(x1,y1),(x2,y2)] see point for details binary: the four floats in the order x1, y1, x2, y2 path: A boolean whether the path is opened or closed + a vector of points. text: [(x1,y1),...] for open path and ((x1,y1),...) for closed paths. See point for details. binary: first a byte indicating open (0) or close (1), then the number of points (Int32), then a vector of points box: A tuple of two points. The coordinates will be reordered so that the first is the upper right and the second is the lower left. text: (x1,y1),(x2,y2) see point for details binary: the four floats in the order x1, y1, x2, y2 (doesn't really matter since they will be reordered) polygon: Same as path but with two differences: is always closed and internally stores the bounding box. text: same as closed path binary: the number of points (Int32), then a vector of points line (version 9.4): Ax + By + C = 0. Stored with three float8. Constraint: A and B must not both be zero (only checked on text input, not binary). text: {A,B,C} see point for details about the string representation of floats. Can also use the same input format as a path with two different points, representing the line between those. binary: the three floats circle: <(x,y),r> (center point and radius), stored with three float8. text: <(x,y),r> see point for details about the string representation of floats. binary: the three floats x, y, r in that order float4/float8: text: (leading/trailing whitespace is skipped) interpreted with the C strtod function, but since it has problems with NaN, [+-]Infinity, [+-]Inf, those strings are identified (case-insensitively) separately. when outputting: NaN, [+-]Infinity is treated separately, otherwise the string is printed with snprintf %.*g and the extra_float_digits setting is honored. binary: the float abstime: A unix timestamp stored as a 32-bit signed integer with seconds-precision (seconds since 1970-01-01 00:00:00), in UTC Has three special values: Invalid (2^31-1), infinity (2^31-3), -infinity (-2^31) text: same format as timestamptz, or \"invalid\", \"infinity\", \"-infinity\" binary: Int32 reltime: A time interval with seconds-precision (stored as an 32-bit signed integer) text: same as interval binary: Int32 tinterval: Consists of a status (Int32) and two abstimes. Status is valid (1) iff both abstimes are valid, else 0. Note that the docs incorrectly states that ' is used as quote instead of \" text: [\"<abstime>\" \"<abstime>\"] binary: Int32 (status), Int32 (abstime 1), Int32 (abstime 2) unknown: text: text binary: text money: A 64-bit signed integer. For example, $123.45 is stored as the integer 12345. Number of fraction digits is locale-dependent. text: a locale-depedent string binary: the raw 64-bit integer macaddr: 6 bytes text: the 6 bytes in hex (always two characters per byte) separated by : binary: the 6 bytes appearing in the same order as when written in text inet/cidr: Struct of Family (byte: ipv4=2, ipv6=3), Netmask (byte with number of bits in the netmask), Ipaddr bytes (16) Text: The IP-address in text format and /netmask. /netmask is omitted in inet if the netmask is the whole address. Binary: family byte, netmask byte, byte (cidr=1, inet=0), number of bytes in address, bytes of the address aclitem: Access list item used in pg_class Text: Something like postgres=arwdDxt/postgres Binary: not available bpchar: Blank-padded char. The type modifier is used to blank-pad the input. text: text binary: text varchar: Variable-length char. The type modifier is used to check the input's length. text: text binary: text date: A signed 32-bit integer of a date. 0 = 2000-01-01. Infinity: INT_MAX, -Infinity: INT_MIN Text: Date only using the specified date style Binary: Int32 time: A signed 64-bit integer representing microseconds from 00:00:00.000000. (Legacy uses 64-bit float). Negative values are not allowed. Max value is 24:00:00.000000. text: hh:mm:ss or hh:mm:ss.ffffff where the fraction part is between 1 and 6 digits (trailing zeros are not written) binary: the 64-bit integer timetz: A struct of Time: A signed 64-bit integer representing microseconds from 00:00:00.000000. (Legacy uses 64-bit float). Negative values are not allowed. Max value is 24:00:00.000000. Zone: A signed 32-bit integer representing the zone (in seconds). Note that the sign is inverted. So GMT+1h is stored as -1h. text: hh:mm:ss or hh:mm:ss.ffffff where the fraction part is between 1 and 6 digits (trailing zeros are not written) binary: the 64-bit integer followed by the 32-bit integer timestamp: A signed 64-bit integer representing microseconds from 2000-01-01 00:00:00.000000 Infinity is LONG_MAX and -Infinity is LONG_MIN (Infinity would be 294277-01-09 04:00:54.775807) Earliest possible timestamp is 4714-11-24 00:00:00 BC. Even earlier would be possible, but due to internal calculations those are forbidden. text: dependent on date style binary: the 64-bit integer timestamptz: A signed 64-bit integer representing microseconds from 2000-01-01 00:00:00.000000 UTC. (Time zone is not stored). Infinity is LONG_MAX and -Infinity is LONG_MIN text: first converted to the time zone in the db settings, then printed according to the date style binary: the 64-bit integer interval: A struct of Time (Int64): all time units other than days, months and years (microseconds) Day (Int32): days, after time for alignment Month (Int32): months and years, after time for alignment text: Style dependent, but for example: \"-11 mons +15435 days -11111111:53:00\" binary: all fields in the struct bit/varbit: First a signed 32-bit integer containing the number of bits (negative length not allowed). Then all the bits in big end first. So a varbit of length 1 has the first (and only) byte set to either 0x80 or 0x00. Last byte is assumed (and is automatically zero-padded in recv) to be zero-padded. text: when sending from backend: all the bits, written with 1s and 0s. when receiving from client: (optionally b or B followed by) all the bits as 1s and 0s, or a x or X followed by hexadecimal digits (upper- or lowercase), big endian first. binary: the 32-bit length followed by the bytes containing the bits numeric: A variable-length numeric value, can be negative. text: NaN or first - if it is negative, then the digits with . as decimal separator binary: first a header of 4 16-bit signed integers: number of digits in the digits array that follows (can be 0, but not negative), weight of the first digit (10000^weight), can be both negative, positive or 0, sign: negative=0x4000, positive=0x0000, NaN=0xC000 dscale: number of digits (in base 10) to print after the decimal separator then the array of digits: The digits are stored in base 10000, where each digit is a 16-bit integer. Trailing zeros are not stored in this array, to save space. The digits are stored such that, if written as base 10000, the decimal separator can be inserted between two digits in base 10000, i.e. when this is to be printed in base 10, only the first digit in base 10000 can (possibly) be printed with less than 4 characters. Note that this does not apply for the digits after the decimal separator; the digits should be printed out in chunks of 4 characters and then truncated with the given dscale. refcursor: uses the same routines as text record: Describes a tuple. Is also the \"base class\" for composite types (i.e. it uses the same i/o functions). text: ( followed by a list of comma-separated text-encoded values followed by ). Empty element means null. Quoted with \" and \" if necessary. \" is escaped with \"\" and \\ is escaped with \\\\ (this differs from arrays where \" is escaped with \\\"). Must be quoted if it is an empty string or contains one of \"\\,() or a space. binary: First a 32-bit integer with the number of columns, then for each column: An OID indicating the type of the column The length of the column (32-bit integer), or -1 if null The column data encoded as binary cstring: text/binary: all characters are sent without the trailing null-character void: Used for example as return value in SELECT * FROM func_returning_void() text: an empty string binary: zero bytes uuid: A 16-byte uuid. text: group of 8, 4, 4, 4, 12 hexadecimal lower-case characters, separated by -. The first byte is written first. It is allowed to surround it with {}. binary: the 16 bytes txid_snapshot: (txid is a UInt64) A struct of UInt32 nxip (size of the xip array) txid xmin (no values in xip is smaller than this) txid xmax (no values in xip is larger than or equal this) txid[] xip (is ordered in ascending order) text: xmin:xmax:1,2,3,4 binary: all fields in the structure tsvector: Used for text searching. Example of tsvector: 'a':1,6,10 'on':5 'and':8 'ate':9A 'cat':3 'fat':2,11 'mat':7 'rat':12 'sat':4 Max length for each lexeme string is 2046 bytes (excluding the trailing null-char) The words are sorted when parsed, and only written once. Positions are also sorted and only written once. For some reason, the unique check does not seem to be made for binary input, only text input... text: As seen above. ' is escaped with '' and \\ is escaped with \\\\. binary: UInt32 number of lexemes for each lexeme: lexeme text in client encoding, null-terminated UInt16 number of positions for each position: UInt16 WordEntryPos, where the most significant 2 bits is weight, and the 14 least significant bits is pos (can't be 0). Weights 3,2,1,0 represent A,B,C,D tsquery: A tree with operands and operators (&, |, !). Operands are strings, with optional weight (bitmask of ABCD) and prefix search (yes/no, written with *). text: the tree written in infix notation. Example: ( 'abc':*B | 'def' ) & !'ghi' binary: the tree written in prefix notation: First the number of tokens (a token is an operand or an operator). For each token: UInt8 type (1 = val, 2 = oper) UInt8 weight + UInt8 prefix (1/0) + null-terminated string, or UInt8 oper (1 = not, 2 = and, 3 = or) enum: Simple text gtsvector: GiST for tsvector. Probably internal type. int4range/numrange/tsrange/tstzrange/daterange/int8range and user-defined range types: /* A range's flags byte contains these bits: */ #define RANGE_EMPTY 0x01 /* range is empty */ #define RANGE_LB_INC 0x02 /* lower bound is inclusive */ #define RANGE_UB_INC 0x04 /* upper bound is inclusive */ #define RANGE_LB_INF 0x08 /* lower bound is -infinity */ #define RANGE_UB_INF 0x10 /* upper bound is +infinity */ #define RANGE_LB_NULL 0x20 /* lower bound is null (NOT USED) */ #define RANGE_UB_NULL 0x40 /* upper bound is null (NOT USED) */ #define RANGE_CONTAIN_EMPTY 0x80/* marks a GiST internal-page entry whose * subtree contains some empty ranges */ A range has no lower bound if any of RANGE_EMPTY, RANGE_LB_INF (or RANGE_LB_NULL, not used anymore) is set. The same applies for upper bounds. text: A range with RANGE_EMPTY is just written as the string \"empty\". Inclusive bounds are written with [ and ], else ( and ) is used. The two values are comma-separated. Missing bounds are written as an empty string (without quotes). Each value is quoted with \" if necessary. Quotes are necessary if the string is either the empty string or contains \"\\,()[] or spaces. \" is escaped with \"\" and \\ is escaped with \\\\. Example: [18,21] binary: First the flag byte. Then, if has lower bound: 32-bit length + binary-encoded data. Then, if has upper bound: 32-bit length + binary-encoded data. hstore: Key/value-store. Both keys and values are strings. text: Comma-space separated string, where each item is written as \"key\"=>\"value\" or \"key\"=>NULL. \" and \\ are escaped as \\\" and \\\\. Example: \"a\"=>\"b\", \"c\"=>NULL, \"d\"=>\"q\" binary: Int32 count for each item: Int32 keylen string of the key (not null-terminated) Int32 length of item (or -1 if null) the item as a string ghstore: internal type for indexing hstore domain types: mapped types used in information_schema: cardinal_number: int4 (must be nonnegative or null) character_data: varchar sql_identifier: varchar time_stamp: timestamptz yes_or_no: varchar(3) (must be \"YES\" or \"NO\" or null) intnotnull: when an int4 is cast to this type, it is checked that the int4 is not null, but it still returns an int4 and not intnotnull..."
  },
  "doc/connection-string-parameters.html": {
    "href": "doc/connection-string-parameters.html",
    "title": "Connection String Parameters | Npgsql Documentation",
    "keywords": "Connection String Parameters Parameter keywords are case-insensitive. Basic Connection Parameter Description Default Host Specifies the host name of the machine on which the server is running. If the value begins with a slash, it is used as the directory for the Unix-domain socket (specifying a Port is still required). Required Port The TCP port of the PostgreSQL server. 5432 Database The PostgreSQL database to connect to. Same as Username Username The username to connect with. Not required if using IntegratedSecurity. Password The password to connect with. Not required if using IntegratedSecurity. Security and Encryption Parameter Description Default SSL Mode Controls whether SSL is used, depending on server support. Can be Require , Disable , or Prefer . See docs for more info . Disable Trust Server Certificate Whether to trust the server certificate without validating it. See docs for more info . false Use SSL Stream Npgsql uses its own internal implementation of TLS/SSL. Turn this on to use .NET SslStream instead. false Check Certificate Revocation Whether to check the certificate revocation list during authentication. False by default. false Integrated Security Whether to use integrated security to log in (GSS/SSPI), currently supported on Windows only. See docs for more info . false Persist Security Info Gets or sets a Boolean value that indicates if security-sensitive information, such as the password, is not returned as part of the connection if the connection is open or has ever been in an open state. Since 3.1 only. false Kerberos Service Name The Kerberos service name to be used for authentication. See docs for more info . postgres Include Realm The Kerberos realm to be used for authentication. See docs for more info . Pooling Parameter Description Default Pooling Whether connection pooling should be used. true Minimum Pool Size The minimum connection pool size. 1 Maximum Pool Size The maximum connection pool size. 100 since 3.1, 20 previously Connection Idle Lifetime The time (in seconds) to wait before closing idle connections in the pool if the count of all connections exceeds MinPoolSize. Since 3.1 only. 300 Connection Pruning Interval How many seconds the pool waits before attempting to prune idle connections that are beyond idle lifetime (see ConnectionIdleLifetime ). Since 3.1 only. 10 Timeouts and Keepalive Parameter Description Default Timeout The time to wait (in seconds) while trying to establish a connection before terminating the attempt and generating an error. 15 Command Timeout The time to wait (in seconds) while trying to execute a command before terminating the attempt and generating an error. Set to zero for infinity. 30 Internal Command Timeout The time to wait (in seconds) while trying to execute a an internal command before terminating the attempt and generating an error. -1 uses CommandTimeout, 0 means no timeout. -1 Keepalive The number of seconds of connection inactivity before Npgsql sends a keepalive query. disabled Tcp Keepalive Time The number of milliseconds of connection inactivity before a TCP keepalive query is sent. Use of this option is discouraged, use KeepAlive instead if possible. Supported only on Windows. disabled Tcp Keepalive Interval The interval, in milliseconds, between when successive keep-alive packets are sent if no acknowledgement is received. TcpKeepAliveTime must be non-zero as well. Supported only on Windows. value of TcpKeepAliveTime Performance Parameter Description Default Max Auto Prepare The maximum number SQL statements that can be automatically prepared at any given point. Beyond this number the least-recently-used statement will be recycled. Zero disables automatic preparation. 0 Auto Prepare Min Usages The minimum number of usages an SQL statement is used before it's automatically prepared. 5 Use Perf Counters Makes Npgsql write performance information about connection use to Windows Performance Counters. Read the docs for more info. Read Buffer Size Determines the size of the internal buffer Npgsql uses when reading. Increasing may improve performance if transferring large values from the database. 8192 Write Buffer Size Determines the size of the internal buffer Npgsql uses when writing. Increasing may improve performance if transferring large values to the database. 8192 Socket Receive Buffer Size Determines the size of socket receive buffer. System-dependent Socket Send Buffer Size Determines the size of socket send buffer. System-dependent No Reset On Close Improves performance in some cases by not resetting the connection state when it is returned to the pool, at the cost of leaking state. Use only if benchmarking shows a performance improvement false Misc Parameter Description Default Application Name The optional application name parameter to be sent to the backend during connection initiation. Enlist Whether to enlist in an ambient TransactionScope. false Search Path Sets the schema search path. Client Encoding Gets or sets the client_encoding parameter. Since 3.1. EF Template Database The database template to specify when creating a database in Entity Framework. template1 Compatibility Parameter Description Default Server Compatibility Mode A compatibility mode for special PostgreSQL server types. Currently only \"Redshift\" is supported. none Convert Infinity DateTime Makes MaxValue and MinValue timestamps and dates readable as infinity and negative infinity. false"
  },
  "doc/transactions.html": {
    "href": "doc/transactions.html",
    "title": "Transactions | Npgsql Documentation",
    "keywords": "Transactions Basic Transactions Transactions can be started by calling the standard ADO.NET method NpgsqlConnection.BeginTransaction() . PostgreSQL doesn't support nested or concurrent transactions - only one transaction may be in progress at any given moment. Calling BeginTransaction() while a transaction is already in progress will throw an exception. Because of this, it isn't necessary to pass the NpgsqlTransaction object returned from BeginTransaction() to commands you execute - calling BeginTransaction() means that all subsequent commands will automatically participate in the transaction, until either a commit or rollback is performed. However, for maximum portability it's recommended to set the transaction on your commands. Although concurrent transactions aren't supported, PostgreSQL supports the concept of savepoints - you may set named savepoints in a transaction and roll back to them later without rolling back the entire transaction. Savepoints can be created, rolled back to, and released via NpgsqlTransaction.Save(name) , NpgsqlTransaction.Rollback(name) and NpgsqlTransaction.Release(name) respectively. See the PostgreSQL documentation for more details. . When calling BeginTransaction() , you may optionally set the isolation level . See the docs for more details. System.Transactions and Distributed Transactions In addition to DbConnection.BeginTransaction() , .NET includes System.Transactions, an alternative API for managing transactions - read the MSDN docs to understand the concepts involved . Npgsql fully supports this API. To participate in a TransactionScope, either add Enlist=true in your connection string, or manually call .Enlist() on your connection. When more than one connection (or resource) enlists in the same transaction, the transaction is said to be distributed . Distributed transactions allow you to perform changes atomically across more than one database (or resource) via a two-phase commit protocol - here is the MSDN documentation . Npgsql supports distributed transactions - support has been rewritten for version 3.2, fixing many previous issues. However, at this time Npgsql enlists as a volatile resource manager , meaning that if your application crashes while performing, recovery will not be managed properly. For more information about this, see this page and the related ones . If you would like to see better distributed transaction recovery (i.e. durable resource manager enlistment), please say so on this issue and subscribe to it for updates. Note that if you open and close connections to the same database inside an ambient transaction, without ever having two connections open at the same time , Npgsql will internally reuse the same connection, avoiding the escalation to a full-blown distributed transaction. This is better for performance and for general simplicity."
  },
  "efcore/index.html": {
    "href": "efcore/index.html",
    "title": "Getting Started | Npgsql Documentation",
    "keywords": "Getting Started Npgsql has an Entity Framework Core provider. It mostly behaves like a any other EFCore provider (e.g. SQL Server) - all the information in the general EF Core docs applies. If you're just getting started with EF Core, those docs are the best place to start. Development happens in the Npgsql.EntityFrameworkCore.PostgreSQL repository, all issues should be reported there. Using the Npgsql EF Core Provider To use the Npgsql EF Core provider, simply add a dependency on Npgsql.EntityFrameworkCore.PostgreSQL . You can follow the instructions in the general EF Core Getting Started docs . Following is an example (new-style) csproj using Npgsql EF Core: <Project Sdk=\"Microsoft.NET.Sdk\"> <PropertyGroup> <OutputType>Exe</OutputType> <TargetFramework>netcoreapp2.0</TargetFramework> </PropertyGroup> <ItemGroup> <PackageReference Include=\"Npgsql.EntityFrameworkCore.PostgreSQL\" Version=\"2.0.0\" /> <PackageReference Include=\"Microsoft.EntityFrameworkCore.Design\" Version=\"2.0.0\" /> </ItemGroup> <ItemGroup> <DotNetCliToolReference Include=\"Microsoft.EntityFrameworkCore.Tools.DotNet\" Version=\"2.0.0\" /> </ItemGroup> </Project> Using the Npgsql provider public class BlogContext : DbContext { // When used with ASP.net core, add these lines to Startup.cs // var connectionString = Configuration.GetConnectionString(\"BlogContext\"); // services.AddEntityFrameworkNpgsql().AddDbContext<BlogContext>(options => options.UseNpgsql(connectionString)); // and add this to appSettings.json // \"ConnectionStrings\": { \"BlogContext\": \"Server=localhost;Database=blog\" } public BlogContext(DbContextOptions<BlogContext> options) : base(options) { } public DbSet<BlogPost> BlogPosts { get; set; } } Using an Existing Database (Database-First) The Npgsql EF Core provider also supports reverse-engineering a code model from an existing PostgreSQL database (\"database-first\"). To do so, add a dependency on Npgsql.EntityFrameworkCore.PostgreSQL.Design. Then, execute the following if you're using dotnet cli: dotnet ef dbcontext scaffold \"Host=localhost;Database=mydatabase;Username=myuser;Password=mypassword\" Npgsql.EntityFrameworkCore.PostgreSQL Or with Powershell: Scaffold-DbContext \"Host=localhost;Database=mydatabase;Username=myuser;Password=mypassword\" Npgsql.EntityFrameworkCore.PostgreSQL"
  },
  "efcore/migration/2.0.html": {
    "href": "efcore/migration/2.0.html",
    "title": "2.0 Release Notes | Npgsql Documentation",
    "keywords": "2.0 Release Notes Version 2.0.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget. This version works with version 2.0.0 of Entity Framework Core (LINK), and contains some new Npgsql features as well. New Features Aside from general EF Core features new in 2.0.0 (LINK), the Npgsql provider contains the following major new features: PostgreSQL array operation translation ( #120 ). While array properties have been supported since 1.1, operations on those arrays where client-evaluated. Version 2.0 will now translate array indexing, .Contains() , .SequenceEquals() and .Length . See the mapping and translation docs for more details. A retrying execution strategy ( #155 ), which will automatically retry operations on exceptions which are considered transient. PostgreSQL extensions are now included in scaffolded models ( #102 ). More LINQ operations are translated to SQL, and more database scenarios are scaffolded correctly. Here's the full list of issues . Please report any problems to https://github.com/npgsql/Npgsql.EntityFrameworkCore.PostgreSQL . Upgrading from 1.x Previously an Npgsql.EntityFrameworkCore.PostgreSQL.Design nuget package existed alongside the main package. Its contents have been merged into the main Npgsql.EntityFrameworkCore.PostgreSQL and no new version has been released. Specifying versions when specifying PostgreSQL extensions on your model is no longer supported - this was a very rarely-used feature which interfered with extension scaffolding."
  },
  "efcore/mapping-and-translation.html": {
    "href": "efcore/mapping-and-translation.html",
    "title": "Type Mapping | Npgsql Documentation",
    "keywords": "Type Mapping The EF Core provider can transparently map any type supported by Npgsql at the ADO.NET level. This means you can use PostgreSQL-specific types, such as inet or circle , directly in your entities - this wasn't possible in EF 6.x. Simply define your properties just as if they were a simple type, such as a string: public class MyEntity { public int Id { get; set; } public string Name { get; set; } public IPAddress IPAddress { get; set; } public NpgsqlCircle Circle { get; set; } public int[] SomeInts { get; set; } } Note that mapping array properties to PostgreSQL arrays is supported. However, operations such as indexing the array, searching for elements in it, etc. aren't yet translated to SQL and will be evaluated client-side. This will probably be fixed in 1.2. PostgreSQL composite types , while supported at the ADO.NET level, aren't yet supported in the EF Core provider. This is tracked by #22 . Explicitly Specifying Datatypes (e.g. JSON) In some cases, your .NET property type can be mapped to several PostgreSQL datatypes; a good example is a string, which will be mapped to text by default, but can also be mapped to jsonb . You can explicitly specify the PostgreSQL datatype by adding the following to your model's OnModelCreating : builder.Entity<Blog>() .Property(b => b.SomeStringProperty) .HasColumnType(\"jsonb\"); Or, if you prefer annotations, use the [Column] attribute: [Column(TypeName=\"jsonb\")] public string SomeStringProperty { get; set; } Operation Translation to SQL Entity Framework Core allows providers to translate query expressions to SQL for database evaluation. For example, PostgreSQL supports regular expression operations , and the Npgsql EF Core provider automatically translates .NET's Regex.IsMatch() to use this feature. Since evaluation happens at the server, table data doesn't need to be transferred to the client (saving bandwidth), and in some cases indices can be used to speed things up. The same C# code on other providers will trigger client evaluation. Below are some Npgsql-specific translations, many additional standard ones are supported as well. This C# expression... ... gets translated to this SQL .Where(c => Regex.IsMatch(c.Name, \"^A+\") WHERE \"c\".\"Name\" ~ '^A+' .Where(c => c.SomeArray[1] = \"foo\") WHERE \"c\".\"SomeArray\"[1] = 'foo' .Where(c => c.SomeArray.SequenceEqual(new[] { 1, 2, 3 }) WHERE \"c\".\"SomeArray\" = ARRAY[1, 2, 3]) .Where(c => c.SomeArray.Contains(3)) WHERE 3 = ANY(\"c\".\"SomeArray\") .Where(c => c.SomeArray.Length == 3) WHERE array_length(\"c\".\"SomeArray, 1) == 3 .Where(c => EF.Functions.Like(c.Name, \"foo%\") WHERE \"c\".\"Name\" LIKE 'foo%' .Where(c => EF.Functions.ILike(c.Name, \"foo%\") WHERE \"c\".\"Name\" ILIKE 'foo%' (case-insensitive LIKE)"
  },
  "dev/index.html": {
    "href": "dev/index.html",
    "title": "Tests | Npgsql Documentation",
    "keywords": "Tests We maintain a large regression test suite, if you're planning to submit code, please provide a test that reproduces the bug or tests your new feature. See this page for information on the Npgsql test suite. Build Server We have a TeamCity build server running continuous integration builds on commits pushed to our github repository. The Npgsql testsuite is executed over all officially supported PostgreSQL versions to catch errors as early as possible. CI NuGet packages are automatically pushed to our unstable feed at MyGet . For some information about the build server setup, see this page . Thanks to Dave Page at PostgreSQL for donating a VM for this! Release Checklist These are the steps needed to publish release 3.0.6: Merge --no-ff hotfix/3.0.6 into master Tag master with v3.0.6 Push both master and v3.0.6 to Github Wait for the build to complete In TeamCity, go to the artifacts for the build and download them all as a single ZIP Nuget push the packages Write release notes on npgsql.org, publish Create release on github, pointing to npgsql.org Upload MSI to the github release Delete hotfix/3.0.6 both locally and on github Create new branch hotfix/3.0.7 off of master, push to github Close the Github 3.0.6 milestone, create new 3.0.7 milestone Twitter Other stuff Emil compiled a list of PostgreSQL types and their wire representations ."
  }
}