{
  "doc/release-notes/4.0.html": {
    "href": "doc/release-notes/4.0.html",
    "title": "Npgsql 4.0 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 4.0 Npgsql 4.0 is out and available at nuget.org. This is a major version with significant changes, upgrade with care, consult the breaking changes section below and test well before deploying to production. A special thanks goes out to @YohDeadfall for his many contributions and reviews. Thanks also (alphabetically) to @austindrenski, @Brar, @kspeakman, @rwasef1830, @shortspider, @StillLearnin, @uhayat for their valuable contributions. High performance A concentrated effort has substantially increased Npgsql performance, especially in highly concurrent, low-latency scenarios. Improvements include: Rewriting of the connection pool to be lock-free, since contention started to be an issue in highly concurrent, short-lived connection scenarios ( #1839 ). Significant reduction of allocations through more recycling and other techniques. New API for generically providing parameters, avoiding boxing of value types ( #1639 ). Avoiding numerous internal async calls where they weren't needed. ... many others In round 16 of the TechEmpower benchmark, .NET Core/ASP.NET Core came in 7th place running with Npgsql , making it one of the fastest mainstream web stacks available - see this blog post for more info . Please let us know how the new version works for you - both positive and negative comments are welcome. If you're interested in Npgsql performance and haven't yet seen the performance page , it's a good opportunity to check it out (it's valid also for 3.2 users). Improved spatial support (PostGIS) Previous versions have allowed basic usage of PostGIS's spatial types via built-in Npgsql types, which were limited in many ways. Thanks to a new plugin infrastructure, you can now use the Npgsql.NetTopologySuite plugin, which maps PostGIS types to the NetTopologySuite spatial library's types. NetTopologySuite's types are more complete, and support a variety of spatial operations and conversions you can perform after loading your spatial data from PostgreSQL. If you prefer to use JSON for your spatial types, the Npgsql.GeoJSON plugin maps PostGIS types to GeoJSON.NET types . GeoJSON is a standard JSON format for spatial data. Finally, if you prefer to use the previous Npgsql types (e.g. PostgisPoint ), these are available via the Npgsql.LegacyPostgis plugin. Thanks to @YohDeadfall for implementing both the NetTopologySuite and GeoJSON plugins. NodaTime date/time support NodaTime is a powerful alternative to .NET's built-in date/time types, such as DateTime . The built-in types are flawed in many ways: they have problematic support for timezones, don't have a date-only or time-only types, and promote problematic programming but not making the right distinctions. If your application handles dates and times in anything but the most basic way, you should seriously consider using NodaTime. To learn more read this blog post by Jon Skeet . You can now use the new Npgsql.NodaTime to have Npgsql map PostgreSQL date/time types to NodaTime types. Json.NET Another plugin, Npgsql.Json.NET , works with Newtonsoft Json.NET to automatically serialize and deserialize PostgreSQL's jsonb and json types to your objects, providing a seamless database JSON programming experience. Instead of working with strings which you have to serialize and deserialize, Npgsql does it for you. Other improvements Fix the binary COPY API to make it interact better with exceptions ( #1646 ). Npgsql better supports working with enums and composites, even without mapping them, and better supports new types introduced via plugins ( #1792 ). Better \"reflection\" capabilities. Continuing work from 3.2, Npgsql now exposes more information about PostgreSQL types, allowing you to dynamically reflect on columns types returned by queries, or required as parameters ( #1276 , #1779 ). Derive parameters for queries. You can now also use NpgsqlCommandBuilder to dynamically understand which parameters and types are required for arbitrary queries (previously supported only for functions) ( #1698 , thanks @Brar!). Allow reading a single character from a PostgreSQL text column ( #1188 ). Decimals read from PostgreSQL will now have the correct scale ( #1925 ). Thanks @StillLearnin and @YohDeadfall. In addition to more documentation, several blog posts are planned to explain the above in more details (to be announced on @shayrojansky ). Breaking changes from 3.2 Caution The date/time behavior has changed in the following ways: DateTime is always sent as timestamp by default, regardless of its kind. You can still specify NpgsqlDbType.TimestampTz , in which case local DateTime gets converted to UTC before sending. When reading timestamptz as a DateTimeOffset , the machine local offset will be used. Previously a DateTimeOffset in UTC was returned. It is no longer possible to read or write DateTimeOffset as timestamp , only as timestamptz . Caution The API for binary import (COPY IN) has changed substantially in a breaking way, and code from 3.2 will not work as-is on 4.0. You must now call NpgsqlBinaryImporter.Complete() to save your imported data; not doing so will roll the operation back. NpgsqlBinaryImporter.Cancel() has been removed - simply closing/disposing the importer will implicitly cancel the import. This is similar to how TransactionScope works and is necessary to prevent accidental commit of data on exception. See #1646 . Caution If you're using decimal/numeric numbers (not floating-point), there's a chance your data needs to be fixed (previous versions incorrectly inserted a scale larger than 28, which is the maximum allowed by .NET decimal ). If you're having trouble reading data previously inserted by Npgsql, consider running this fixup code . If your data really does contain more than 28/29 fractional digits and you need to keep that precision, see the workarounds proposed in this comment for loading these values. .NET Standard 1.3 is no longer supported. .NET Standard 2.0 is the lowest supported version. Npgsql used to use its own internal TLS/SSL due to issues with some server. As these issues have been resolved, the standard .NET SslStream is now used by default ( #1482 ), but you can still set Use SSL Stream=false to keep using the internal implementation (please report why you need this, as it's likely the internal implementation will be removed in a future release). The reader instances returned by NpgsqlCommand.ExecuteReader() are now recycled, to reduce memory allocations ( #1649 ). You should not keep a reference or interact with a reader after its command has been disposed (such interaction was limited in any case). The Min Pool Size parameter will no longer make the pool create new connections internally - it will only have an effect on how many connections are pruned. Previously, in various points the pool would check if the current number of connections was below Min Pool Size , and if so, automatically created new ones - this no longer happens. Parameter types have become more strict. Previous versions allowed to you pass arbitrary value types, such as writing CLR string to int columns, or anything that implemented IConvertible. Although some implicit conversions are still supported (e.g. long -> int, short -> int), some have been removed. Data type names returned from NpgsqlDataReader.GetDataTypeName() and other APIs are now more standards-conforming (e.g. integer[] instead of _int4 ), and properly include type modifiers (e.g. character varying(10) ) ( #1919 ). NpgsqlParameter.EnumType and NpgsqlParameter.SpecificType have been removed. See Composites and Enums for more details. Parameter names are no longer trimmed, set your names to the exact parameter name specified in your SQL. If a parameter's name isn't set, it will no longer default to Parameter1, Parameter2, etc. The following APIs \"connection capability\" APIs have been removed from NpgsqlConnection: UseConformantStrings , SupportsEStringPrefix , UseSslStream . The default name translator, NpgsqlSnakeCaseNameTranslator , has been changed to handle acronyms better. Given the property name IsJSON , the old translator algorithm would output is_j_s_o_n , while the new outputs is_json . To revert back to the old algorithm, create a NpgsqlSnakeCaseNameTranslator instance with legacyMode: true and pass it when calling the MapComposite and MapEnum methods. If you are reading tables as composites ( #990 ), you will have to add the new Load Table Composites to your connection string. NpgsqlConnection.GetSchema() will no longer return system tables (i.e. tables in schemas pg_catalog and information_schema ), #1831 . You may no longer have multiple streams or text readers open on a reader (this was previously supported with non-sequential readers). Accessing a new column closes any open stream or text reader. The DateTimeOffset instances returned for PostgreSQL timetz now have their date set to 0001-01-02 instead of the previous 0001-01-01 ( #1924 )."
  },
  "doc/wait.html": {
    "href": "doc/wait.html",
    "title": "Waiting for Notifications | Npgsql Documentation",
    "keywords": "Waiting for Notifications Note: This functionality replaces Npgsql 3.0's \"Continuous processing mode\" . PostgreSQL Asynchronous messages PostgreSQL has a feature whereby arbitrary notification messages can be sent between clients. For example, one client may wait until it is notified by another client of a task that it is supposed to perform. Notifications are, by their nature, asynchronous - they can arrive at any point. For more detail about this feature, see the PostgreSQL NOTIFY command . Some other asynchronous message types are notices (e.g. database shutdown imminent) and parameter changes, see the PostgreSQL protocol docs for more details. Note that despite the word \"asynchronous\", this page has nothing to do with ADO.NET async operations (e.g. ExecuteReaderAsync). Processing of Notifications Npgsql exposes notification messages via the Notification event on NpgsqlConnection. Since asynchronous notifications are rarely used and processing can be complex, Npgsql only processes notification messages as part of regular (synchronous) query interaction. That is, if an asynchronous notification is sent, Npgsql will only process it and emit an event to the user the next time a command is sent and processed. To receive notifications outside a synchronous request-response cycle, call NpgsqlConnection.Wait() . This will make your thread block until a single notification is received (note that a version with a timeout as well as an async version exist). Note that the notification is still delivered via the Notification event as before. var conn = new NpgsqlConnection(ConnectionString); conn.Open(); conn.Notification += (o, e) => Console.WriteLine(\"Received notification\"); using (var cmd = new NpgsqlCommand(\"LISTEN channel_name\", conn)) { cmd.ExecuteNonQuery(); } while (true) { conn.Wait(); // Thread will block here } Keepalive You may want to turn on keepalives ."
  },
  "doc/performance.html": {
    "href": "doc/performance.html",
    "title": "Performance | Npgsql Documentation",
    "keywords": "Performance Prepared Statements One of the most important (and easy) ways to improve your application's performance is to prepare your commands. Even if you're not coding against ADO.NET directly (e.g. using Dapper or an O/RM), Npgsql has an automatic preparation feature which allows you to benefit from the performance gains associated with prepared statements. See this blog post and/or the documentation for more details. Batching/Pipelining When you execute a command, Npgsql executes a roundtrip to the database. If you execute multiple commands (say, inserting 3 rows or performing multiple selects), you're executing multiple roundtrips; each command has to complete before the next command can start execution. Depending on your network latency, this can considerably degrade your application's performance. You can batch multiple SQL statements in a single command, executing them a single roundtrip: using (var cmd = new NpgsqlCommand(\"SELECT ...; SELECT ...\")) using (var reader = cmd.ExecuteReader()) { while (reader.Read()) { // Read first resultset } reader.NextResult(); while (reader.Read()) { // Read second resultset } } Performance Counters Npgsql 3.2 includes support for performance counters , which provide visibility into connections and the connection pool - this helps you understand what your application is doing in real-time, whether there's a connection leak, etc. Npgsql counter support is very similar to that of other ADO.NET providers, such as SqlClient , it's recommended that your read that page first. Using performance counters first involves setting them up on your Windows system. To do this you will need to install Npgsql's MSI, which is available on the github releases page . Note that GAC installation isn't necessary (or recommended). Once the counters are installed, fire up the Windows Performance Monitor and look for the category \".NET Data Provider for PostgreSQL (Npgsql)\". In addition, you will need to pass Use Perf Counters=true on your connection string. Once you start your Npgsql application with this addition, you should start seeing real-time data in the Performance Monitor. Performance counters are currently only available on Windows with .NET Framework (.NET Core doesn't include performance counters yet). Disable enlisting to TransactionScope By default, Npgsql will enlist to ambient transactions. This occurs when a connection is opened while inside a TransactionScope , and can provide a powerful programming model for working with transactions. However, this involves checking whether an ambient transaction is in progress each time a (pooled) connection is open, an operation that takes more time than you'd think. Scenarios where connections are very short-lived and open/close happens very frequently can benefit from removing this check - simply include Enlist=false in the connection string. Note that you can still enlist manually by calling NpgsqlConnection.Enlist() . Pooled Connection Reset When a pooled connection is closed, Npgsql will arrange for its state to be reset the next time it's used. This prevents leakage of state from one usage cycle of a physical connection to another one. For example, you may change certain PostgreSQL parameters (e.g. statement_timeout ), and it's undesirable for this change to persist when the connection is closed. Connection reset happens via the PostgreSQL DISCARD ALL command , or, if there are any prepared statements at the time of closing, by a combination of the equivalent statements described in the docs (to prevent closing those statements). Note that these statements aren't actually sent when closing the connection - they're written into Npgsql's internal write buffer, and will be sent with the first user statement after the connection is reopened. This prevents a costly database roundtrip. If you really want to squeeze every last bit of performance from PostgreSQL, you may disable connect reset by specifying No Reset On Close on your connection string - this will slightly improve performance in scenarios where connection are very short-lived, and especially if prepared statements are in use. Reading Large Values When reading results from PostgreSQL, Npgsql first reads raw binary data from the network into an internal read buffer, and then parses that data as you call methods such as NpgsqlDataReader.GetString() . While this allows for efficient network reads, it's worth thinking about the size of this buffer, which is 8K by default. Under normal usage,, Npgsql attempts to read each row into the buffer; if that entire row fits in 8K, you'll have optimal performance. However, if a row is bigger than 8K, Npgsql will allocate an \"oversize buffer\", which will be used until the connection is closed or returned to the pool. If you're not careful, this can create significant memory churn that will slow down your application. To avoid this, if you know you're going to be reading 16k rows, you can specify Read Buffer Size=18000 in your connection string (leaving some margin for protocol overhead), this will ensure that the read buffer is reused and no extra allocation occur. Another option is to pass CommandBehavior.SequentialAccess to NpgsqlCommand.ExecuteReader() . Sequential mode means that Npgsql will no longer read entire rows into its buffer, but will rather fill up the buffer as needed, reading more data only when it's empty. The same 8K read buffer will be used regardless of the row's total size, and Npgsql will take care of the details. In sequential mode, however, you must read the row's fields in the order in which you specified them; you cannot read the 2nd field and then go back to the 1st field, and trying to do so will generate an exception. Similarly, you cannot read the same field twice - once you've read a field, it has been consumed. For more information on CommandBehavior.SequentialAccess , see this page . If you decide to use this feature, be aware that it isn't used as often and may therefore contain bugs. You can also control the socket's receive buffer size (not to be confused with Npgsql's internal buffer) by setting the Socket Receive Buffer Size connection string parameter. Writing Large Values Writing is somewhat similar - Npgsql has an internally write buffer (also 8K by default). When writing your query's SQL and parameters to PostgreSQL, Npgsql always writes \"sequentially\", that is, filling up the 8K buffer and flushing it when full. You can use Write Buffer Size to control the buffer's size. You can also control the socket's receive buffer size (not to be confused with Npgsql's internal buffer) by setting the Socket Receive Buffer Size connection string parameter. Avoiding boxing when writing parameter values See this page . Unix Domain Socket If you're on Linux or macOS and are connecting to a PostgreSQL server on the same machine, you can boost performance a little by connecting via Unix domain socket rather than via a regular TCP/IP socket. To do this, simply specify the directory of your PostgreSQL sockets in the Host connection string parameter - if this parameter starts with a slash, it will be taken to mean a filesystem path."
  },
  "doc/types/basic.html": {
    "href": "doc/types/basic.html",
    "title": "Supported Types and their Mappings | Npgsql Documentation",
    "keywords": "Supported Types and their Mappings The following lists the built-in mappings when reading and writing CLR types to PostgreSQL types. Note that in addition to the below, enum and composite mappings are documented in a separate page . Note also that several plugins exist to add support for more mappings (e.g. spatial support for PostGIS), these are listed in the Types menu. Read mappings The following shows the mappings used when reading values. The default type is returned when using NpgsqlCommand.ExecuteScalar() , NpgsqlDataReader.GetValue() and similar methods. You can read as other types by calling NpgsqlDataReader.GetFieldValue<T>() . Provider-specific types are returne by NpgsqlDataReader.GetProviderSpecificValue() . PostgreSQL type Default .NET type Provider-specific type Other .NET types boolean bool smallint short byte, sbyte, int, long, float, double, decimal integer int byte, short, long, float, double, decimal bigint long long, byte, short, int, float, double, decimal real float double double precision double numeric decimal byte, short, int, long, float, double money decimal text string char[] character varying string char[] character string char[] citext string char[] json string char[] jsonb string char[] xml string char[] point NpgsqlPoint lseg NpgsqlLSeg path NpgsqlPath polygon NpgsqlPolygon line NpgsqlLine circle NpgsqlCircle box NpgsqlBox bit(1) bool BitArray bit(n) BitArray bit varying BitArray hstore IDictionary<string, string> uuid Guid cidr ValueTuple<IPAddress, int> NpgsqlInet inet IPAddress ValueTuple<IPAddress, int> NpgsqlInet macaddr PhysicalAddress tsquery NpgsqlTsQuery tsvector NpgsqlTsVector date DateTime NpgsqlDate interval TimeSpan NpgsqlTimeSpan timestamp DateTime NpgsqlDateTime timestamp with time zone DateTime NpgsqlDateTime DateTimeOffset time TimeSpan time with time zone DateTimeOffset DateTimeOffset, DateTime, TimeSpan bytea byte[] oid uint xid uint cid uint oidvector uint[] name string char[] (internal) char char byte, short, int, long geometry (PostGIS) PostgisGeometry record object[] composite types T range subtypes NpgsqlRange<TElement> enum types TEnum array types Array (of child element type) The Default .NET type column specifies the data type NpgsqlDataReader.GetValue() will return. NpgsqlDataReader.GetProviderSpecificValue will return a value of a data type specified in the Provider-specific type column, or the Default .NET type if there is no specialization. Finally, the third column specifies other CLR types which Npgsql supports for the PostgreSQL data type. These can be retrieved by calling NpgsqlDataReader.GetBoolean() , GetByte() , GetDouble() etc. or via GetFieldValue<T>() . Write mappings There are three rules that determine the PostgreSQL type sent for a parameter: If the parameter's NpgsqlDbType is set, it is used. If the parameter's DataType is set, it is used. If the parameter's DbType is set, it is used. If none of the above is set, the backend type will be inferred from the CLR value type. Note that for DateTime and NpgsqlDateTime , the Kind attribute determines whether to use timestamp or timestamptz . NpgsqlDbType DbType PostgreSQL type Accepted .NET types Boolean Boolean boolean bool Smallint Int16 smallint short Integer Int32 integer int Bigint Int64 bigint long Real Single real float Double Double double precision double Numeric Decimal, VarNumeric numeric decimal Money Currency money decimal Text String, StringFixedLength, AnsiString, AnsiStringFixedLength text string, char[], char Varchar character varying string, char[], char Char character string, char[], char Citext citext string, char[], char Json json string, char[], char Jsonb jsonb string, char[], char Xml xml string, char[], char Point point NpgsqlPoint LSeg lseg NpgsqlLSeg Path path NpgsqlPath Polygon polygon NpgsqlPolygon Line line NpgsqlLine Circle circle NpgsqlCircle Box box NpgsqlBox Bit bit BitArray, bool, string Varbit bit varying BitArray, bool, string Hstore hstore IDictionary<string, string> Uuid uuid Guid Cidr cidr ValueTuple<IPAddress, int>, IPAddress, NpgsqlInet Inet inet ValueTuple<IPAddress, int>, IPAddress, NpgsqlInet MacAddr macaddr PhysicalAddress TsQuery tsquery NpgsqlTsQuery TsVector tsvector NpgsqlTsVector Date Date date DateTime, NpgsqlDate Interval interval TimeSpan, NpgsqlTimeSpan Timestamp DateTime, DateTime2 timestamp DateTime, DateTimeOffset, NpgsqlDateTime TimestampTz DateTimeOffset timestamp with time zone DateTime, DateTimeOffset, NpgsqlDateTime Time Time time TimeSpan TimeTz time with time zone DateTimeOffset, DateTime, TimeSpan Bytea Binary bytea byte[], ArraySegment<byte> Oid oid uint Xid xid uint Cid cid uint Oidvector oidvector uint[] Name name string, char[], char InternalChar (internal) char byte Composite composite types T Range | (other NpgsqlDbType) range types NpgsqlRange<TElement> Enum enum types TEnum Array | (other NpgsqlDbType) array types Array, IList<T>, IList Notes when using Range and Array, bitwise-or NpgsqlDbType.Range or NpgsqlDbType.Array with the child type. For example, to construct the NpgsqlDbType for a int4range , write NpgsqlDbType.Range | NpgsqlDbType.Integer . To construct the NpgsqlDbType for an int[] , write NpgsqlDbType.Array | NpgsqlDbType.Integer . For information about enums, see the Enums and Composites page . .NET type Auto-inferred PostgreSQL type bool boolean byte smallint sbyte smallint short smallint int integer long bigint float real double double precision decimal numeric string text char[] text char text NpgsqlPoint point NpgsqlLSeg lseg NpgsqlPath path NpgsqlPolygon polygon NpgsqlLine line NpgsqlCircle circle NpgsqlBox box BitArray bit varying Guid uuid IPAddress inet NpgsqlInet inet PhysicalAddress macaddr NpgsqlTsQuery tsquery NpgsqlTsVector tsvector NpgsqlDate date NpgsqlDateTime timestamp DateTime timestamp DateTimeOffset timestamp with time zone TimeSpan time byte[] bytea Custom composite type composite types NpgsqlRange<TElement> range types Enum types enum types Array types array types"
  },
  "doc/index.html": {
    "href": "doc/index.html",
    "title": "Documentation | Npgsql Documentation",
    "keywords": "Getting Started The best way to use Npgsql is to install its nuget package . Npgsql aims to be fully ADO.NET-compatible, its API should feel almost identical to other .NET database drivers. Here's a basic code snippet to get you started. var connString = \"Host=myserver;Username=mylogin;Password=mypass;Database=mydatabase\"; using (var conn = new NpgsqlConnection(connString)) { conn.Open(); // Insert some data using (var cmd = new NpgsqlCommand()) { cmd.Connection = conn; cmd.CommandText = \"INSERT INTO data (some_field) VALUES (@p)\"; cmd.Parameters.AddWithValue(\"p\", \"Hello world\"); cmd.ExecuteNonQuery(); } // Retrieve all rows using (var cmd = new NpgsqlCommand(\"SELECT some_field FROM data\", conn)) using (var reader = cmd.ExecuteReader()) while (reader.Read()) Console.WriteLine(reader.GetString(0)); } You can find more info about the ADO.NET API in the MSDN docs or in many tutorials on the Internet. DbProviderFactory The example above involves some Npgsql-specific types ( NpgsqlConnection , NpgsqlCommand ...), which makes your application Npgsql-specific. If your code needs to be database-portable, you should use the ADO.NET DbProviderFactory API instead ( see this tutorial ). In a nutshell, you register Npgsql's provider factory in your application's App.config (or machines.config ) file, and then obtain it in your code without referencing any Npgsql-specific types. You can then use the factory to create a DbConnection (which NpgsqlConnection extends), and from there a DbCommand and so on. To do this, add the following to your App.config : <system.data> <DbProviderFactories> <add name=\"Npgsql Data Provider\" invariant=\"Npgsql\" description=\".Net Data Provider for PostgreSQL\" type=\"Npgsql.NpgsqlFactory, Npgsql, Culture=neutral, PublicKeyToken=5d8b90d52f46fda7\"/> </DbProviderFactories> </system.data> GAC Installation In some cases you'll want to install Npgsql into your Global Assembly Cache (GAC) . This is usually the case when you're using a generic database program that can work with any ADO.NET provider but doesn't come with Npgsql or reference it directly. For these cases, you can download the Npgsql Windows installer from our Github releases page : it will install Npgsql (and optionally the Entity Framework providers) into your GAC and add Npgsql's DbProviderFactory into your machine.config file. This is not the general recommended method of using Npgsql - always install via Nuget if possible. In addition to Npgsql.dll, this will also install System.Threading.Tasks.Extensions.dll into the GAC. Visual Studio Integration If you'd like to have Visual Studio Design-Time support, give our VSIX extension a try . Unstable Packages The Npgsql build server publishes CI nuget packages for every build. If a bug affecting you was fixed but there hasn't yet been a patch release, you can get a CI nuget at our stable MyGet feed . These packages are generally stable and safe to use (although it's better to wait for a release). We also publish CI packages for the next minor/major version at our unstable MyGet feed . These are definitely unstable and should be used with care."
  },
  "doc/types/jsonnet.html": {
    "href": "doc/types/jsonnet.html",
    "title": "Json.NET Type Plugin | Npgsql Documentation",
    "keywords": "Json.NET Type Plugin Since 4.0, Npgsql supports type plugins , which are external nuget packages that modify how Npgsql maps PostgreSQL values to CLR types. One of these is the Json.NET plugin, which allows Npgsql to automatically make use of Newtonsoft Json.NET when reading and writing JSON data. PostgreSQL natively supports two JSON types : jsonb and json . Out of the box, Npgsql allows reading and writing these types as strings and provides no further processing to avoid taking a dependency on an external JSON library, forcing Npgsql users to serialize and deserialize JSON values themselves. The Json.NET plugin removes this burden from users by perform serialization/deserialization within Npgsql itself. Setup To use the Json.NET plugin, simply add a dependency on Npgsql.Json.NET and set it up: using Npgsql; // Place this at the beginning of your program to use Json.NET everywhere (recommended) NpgsqlConnection.GlobalTypeMapper.UseJsonNet(); // Or to temporarily use JsonNet on a single connection only: conn.TypeMapper.UseJsonNet(); Arbitrary CLR Types Once the plugin is set up, you can transparently read and write CLR objects as JSON values - the plugin will automatically have them serialized/deserialized: // Write arbitrary CLR types as JSON using (var cmd = new NpgsqlCommand(@\"INSERT INTO mytable (my_json_column) VALUES (@p)\", conn)) { cmd.Parameters.Add(new NpgsqlParameter(\"p\", NpgsqlDbType.Jsonb) { Value = MyClrType }); cmd.ExecuteNonQuery(); } // Read arbitrary CLR types as JSON using (var cmd = new NpgsqlCommand(@\"SELECT my_json_column FROM mytable\", conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); var someValue = reader.GetFieldValue<MyClrType>(0); } Note that in the example above, you must still specify NpgsqlDbType.Json (or Jsonb ) to tell Npgsql that the parameter type is JSON. If you have several CLR types which you'll be using, you have the option of mapping them to JSON: NpgsqlConnection.GlobalTypeMapper.UseJsonNet(new[] { typeof(MyClrType) }); Note that the UseJsonNet() method accepts two type arrays: the first for types to map to jsonb , the second for types to map to json . JObject/JArray You can also read and write Json.NET's JObject/JArray types directly: var value = new JObject { [\"Foo\"] = 8 }; using (var cmd = new NpgsqlCommand(@\"INSERT INTO mytable (my_json_column) VALUES (@p)\", conn)) { cmd.Parameters.Add(new NpgsqlParameter(\"p\", NpgsqlDbType.Jsonb) { Value = value }); cmd.ExecuteNonQuery(); } using (var cmd = new NpgsqlCommand(@\"SELECT my_json_column FROM mytable\", conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); var someValue = reader.GetFieldValue<JObject>(0); } CLR Arrays You can even read and write native CLR arrays as JSON: using (var cmd = new NpgsqlCommand(@\"INSERT INTO mytable (my_json_column) VALUES (@p)\", conn)) { cmd.Parameters.Add(new NpgsqlParameter(\"p\", NpgsqlDbType.Jsonb) { Value = new[] { 1, 2, 3} }); cmd.ExecuteNonQuery(); } using (var cmd = new NpgsqlCommand(@\"SELECT my_json_column FROM mytable\", conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); var someValue = reader.GetFieldValue<int[]>(0); } And for extra credit, you can specify JSON by default for array types just like for regular CLR types: NpgsqlConnection.GlobalTypeMapper.UseJsonNet(new[] { typeof(int[] }); This overwrites the default array mapping (which sends PostgreSQL arrays ), making Npgsql send int arrays as JSON by default."
  },
  "doc/large-objects.html": {
    "href": "doc/large-objects.html",
    "title": "Large Objects | Npgsql Documentation",
    "keywords": "Large Objects The Large Objects feature is a way of storing large files in a PostgreSQL database. Files can normally be stored in bytea columns but there are two downsides; a file can only be 1 GB and the backend buffers the whole file when reading or writing a column, which may use significant amounts of RAM on the backend. With the Large Objects feature, objects are instead stored in a separate system table in smaller chunks and provides a streaming API for the user. Each object is given an integral identifier that is used for accessing the object, that can, for example, be stored in a user's table containing information about this object. Example // Retrieve a Large Object Manager for this connection var manager = new NpgsqlLargeObjectManager(Conn); // Create a new empty file, returning the identifier to later access it uint oid = manager.Create(); // Reading and writing Large Objects requires the use of a transaction using (var transaction = Conn.BeginTransaction()) { // Open the file for reading and writing using (var stream = manager.OpenReadWrite(oid)) { var buf = new byte[] { 1, 2, 3 }; stream.Write(buf, 0, buf.Length); stream.Seek(0, System.IO.SeekOrigin.Begin); var buf2 = new byte[buf.Length]; stream.Read(buf2, 0, buf2.Length); // buf2 now contains 1, 2, 3 } // Save the changes to the object transaction.Commit(); } See also See the PostgreSQL documentation for more information. All functionality are implemented and wrapped in the classes NpgsqlLargeObjectManager and NpgsqlLargeObjectStream using standard .NET Stream as base class."
  },
  "doc/types/geojson.html": {
    "href": "doc/types/geojson.html",
    "title": "PostGIS/GeoJSON Type Plugin | Npgsql Documentation",
    "keywords": "PostGIS/GeoJSON Type Plugin Before 4.0, Npgsql has supported reading and writing PostGIS types via some bundled .NET classes: PostgisPoint , PostgisLineString , etc. While this model provided some basic support, a proper representation of spatial types is a complicated task that's beyond Npgsql's scope, and should be handled by a specialized spatial library instead. The Npgsql.GeoJSON plugin makes Npgsql read and write PostGIS spatial types as GeoJSON (RFC7946) types , via the GeoJSON.NET library. As an alternative, you can use Npgsql.NetTopologySuite , which is a full-fledged .NET spatial library with many features. If you prefer to work with the pre-4.0 types, you can still do so by using the Npgsql.LegacyPostgis plugin . Setup To use the GeoJSON plugin, simply add a dependency on Npgsql.GeoJSON and set it up: using Npgsql; // Place this at the beginning of your program to use NetTopologySuite everywhere (recommended) NpgsqlConnection.GlobalTypeMapper.UseGeoJSON(); // Or to temporarily use GeoJSON on a single connection only: conn.TypeMapper.UseGeoJSON(); Reading and Writing Geometry Values When reading PostGIS values from the database, Npgsql will automatically return the appropriate GeoJSON types: Point , LineString , and so on. Npgsql will also automatically recognize GeoJSON's types in parameters, and will automatically send the corresponding PostGIS type to the database. The following code demonstrates a roundtrip of a GeoJSON Point to the database: var point = new Point(new Position(51.899523, -2.124156)); conn.ExecuteNonQuery(\"CREATE TEMP TABLE data (geom GEOMETRY)\"); using (var cmd = new NpgsqlCommand(\"INSERT INTO data (geom) VALUES (@p)\", conn)) { cmd.Parameters.AddWithValue(\"@p\", point); cmd.ExecuteNonQuery(); } using (var cmd = new NpgsqlCommand(\"SELECT geom FROM data\", conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); Assert.That(reader[0], Is.EqualTo(point)); } You may also explicitly specify a parameter's type by setting NpgsqlDbType.Geometry . Geography (geodetic) Support PostGIS has two types: geometry (for Cartesian coordinates) and geography (for geodetic or spherical coordinates). You can read about the geometry/geography distinction in the PostGIS docs or in this blog post . In a nutshell, geography is much more accurate when doing calculations over long distances, but is more expensive computationally and supports only a small subset of the spatial operations supported by geometry . Npgsql uses the same GeoJSON types to represent both geometry and geography - the Point type represents a point in either Cartesian or geodetic space. You usually don't need to worry about this distinction because PostgreSQL will usually cast types back and forth as needed. However, it's worth noting that Npgsql sends Cartesian geometry by default, because that's the usual requirement. You have the option of telling Npgsql to send geography instead by specifying NpgsqlDbType.Geography : using (var cmd = new NpgsqlCommand(\"INSERT INTO data (geog) VALUES (@p)\", conn)) { cmd.Parameters.AddWithValue(\"@p\", NpgsqlDbType.Geography, point); cmd.ExecuteNonQuery(); } If you prefer to use geography everywhere by default, you can also specify that when setting up the plugin: NpgsqlConnection.GlobalTypeMapper.UseGeoJSON(geographyAsDefault: true);"
  },
  "doc/release-notes/3.1.html": {
    "href": "doc/release-notes/3.1.html",
    "title": "Npgsql 3.1 Release Notes | Npgsql Documentation",
    "keywords": "Migrating from 3.0 to 3.1 CommandTimeout used to be implemented with PostgreSQL's statement_timeout parameter, but this wasn't a very reliable method and has been removed. CommandTimeout is now implemented via socket timeouts only, see #689 for more details. Note that if a socket timeout occurs, the connection is broken and must be reopened. The Persist Security Info parameter has been implemented and is false by default. This means that once a connection has been opened, you will not be able to get its password. Removed ContinuousProcessing mode, and replaced it with Wait , a simpler and less bug-prone mechanism for consuming asynchronous notifications ( #1024 ). The Maximum Pool Size connection is parameter is now 100 default instead of 20 (this is default in SqlClient, pg_bouncer...). The Connection Lifetime parameter has been renamed to Connection Idle Lifetime , and its default has been changed from 15 to 300. Also, once the number of seconds has elapsed the connection is closed immediately; the previous behavior closed half of the connections. RegisterEnum and RegisterEnumGlobally have been renamed to MapEnum and MapEnumGlobally respectively. If you used enum mapping in 3.0, the strategy for translating between CLR and PostgreSQL type names has changed. In 3.0 Npgsql simply used the CLR name (e.g. SomeField) as the PostgreSQL name; Npgsql 3.1 uses a user-definable name translator, default to snake case (e.g. some_field). See #859 . The EnumLabel attribute has been replaced by the PgName attribute (which is also used for the new composite type support). When PostgreSQL sends an error, it is no longer raised by an NpgsqlException but by a PostgresException. PostgresException is a subclass of NpgsqlException so code catching NpgsqlException should still work, but the PostgreSQL-specific exception properties will only be available on PostgresException. The Code property on NpgsqlException has been renamed to SqlState. It has also been moved to PostgresException. NpgsqlNotice has been renamed to PostgresNotice For multistatement commands, PostgreSQL parse errors will now be thrown only when the user calls NextResult() and gets to the problematic statement. It is no longer possible to dispose a prepared statement while a reader is still open. Since disposing a prepared statement includes database interaction, the connection must be idle. Removed NpgsqlConnection.SupportsHexByteFormat . Renamed NpgsqlConnection.Supports_E_StringPrefix to SupportsEStringPrefix ."
  },
  "doc/compatibility.html": {
    "href": "doc/compatibility.html",
    "title": "Compatibility Notes | Npgsql Documentation",
    "keywords": "Compatibility Notes This page centralizes Npgsql's compatibility status with PostgreSQL and other components, and documents some important gotchas. We aim to be compatible with all currently supported PostgreSQL versions , which means 5 years back. Earlier versions may still work but we don't perform continuous testing on them or commit to resolving issues on them. PostgreSQL We aim to be compatible with all currently supported PostgreSQL versions , which means 5 years back. Earlier versions may still work but we don't perform continuous testing on them or commit to resolving issues on them. ADO.NET Npgsql is an ADO.NET-compatible provider, so it has the same APIs as other .NET database drivers and should behave the same. Please let us know if you notice any non-standard behavior. .NET Framework/.NET Core/mono Npgsql 3.1 targets .NET Framework 4.5 and 4.5.1, as well as the .NET Standard 1.3 which allows it to run on .NET Core. It is also tested and runs well on mono. Here is a sample project.json to get you started with .NET Core: { \"buildOptions\": { \"emitEntryPoint\": \"true\" }, \"dependencies\": { \"Npgsql\" : \"3.1.8\" }, \"frameworks\": { \"net451\": { \"frameworkAssemblies\": { \"System.Data\": { \"version\": \"4.0.0.0\", \"type\": \"build\" } } }, \"netcoreapp1.0\": { \"dependencies\": { \"Microsoft.NETCore.App\": { \"version\": \"1.0.1\", \"type\": \"platform\" } } } } } Note that netcoreapp1.0 can be replaced with netstandard13 (or up) to create a library. Amazon Redshift Amazon Redshift is a cloud-based data warehouse originally based on PostgreSQL 8.0.2. In addition, due to its nature some features have been removed and others changed in ways that make them incompatible with PostgreSQL. We try to support Redshift as much as we can, please let us know about issues you run across. First, check out Amazon's page about Redshift and PostgreSQL which contains lots of useful compatibility information. Additional known issues: If you want to connect over SSL, your connection string must contain Server Compatibility Mode=Redshift , otherwise you'll get a connection error about ssl_renegotiation_limit . Entity Framework with database-computed identity values don't work with Redshift, since it doesn't support sequences (see issue #544 ). DigitalOcean Managed Database DigitalOcean's Managed Database services requires you to connect to PostgreSQL over SSL. Unfortunately when you enable it in your connection string, you will get the same error regarding ssl_renegotiation_limit as Amazon Redshift. The Redshift compatibility mode setting resolves the issue on DigitalOcean. pgbouncer Npgsql works well with PgBouncer, but there are some quirks to be aware of. In many cases, you'll want to turn off Npgsql's internal connection pool by specifying Pooling=false on the connection string. If you decide to keep Npgsql pooling on along with PgBouncer, and are using PgBouncer's transaction or statement mode, then you need to specify No Reset On Close=true on the connection string. This disables Npgsql's connection reset logic ( DISCARD ALL ), which gets executed when a connection is return to Npgsql's pool, and which makes no sense in these modes. Prior to version 3.1, Npgsql sends the statement_timeout startup parameter when it connects, but this parameter isn't supported by pgbouncer. You can get around this by specifying CommandTimeout=0 on the connection string, and then manually setting the CommandTimeout property on your NpgsqlCommand objects. Version 3.1 no longer sends statement_timeout ."
  },
  "efcore/release-notes/2.2.html": {
    "href": "efcore/release-notes/2.2.html",
    "title": "2.2 Release Notes | Npgsql Documentation",
    "keywords": "2.2 Release Notes Version 2.2.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget. This version works with version 2.2.0 of Entity Framework Core , and contains some new Npgsql features as well. This release was result of hard work by @roji , @austindrenski , @yohdeadfall and @khellang . New Features Aside from general EF Core features new in 2.2.0, the Npgsql EF Core provider contains the following major new features: PostgreSQL 11 covering indexes PostgreSQL 11 introduced covering indexes feature , which allow you to include \"non-key\" columns in your indexes. This allows you to perform index-only scans and can provide a significant performance boost. Support has been added in ( #697 ): protected override void OnConfiguring(DbContextOptionsBuilder builder) => modelBuilder.Entity<Blog>() .ForNpgsqlHasIndex(b => b.Id) .ForNpgsqlInclude(b => b.Name); This will create an index for searching on Id , but containing also the column Name , so that reading the latter will not involve accessing the table. See the documentation for more details . Thanks to @khellang for contributing this! PostgreSQL user-defined ranges The provider already supported PostgreSQL range types , but prior to 2.2 that support was limited to the built-in range types which come with PostgreSQL. #329 extends that support to range types which you define: protected override void OnConfiguring(DbContextOptionsBuilder builder) => builder.UseNpgsql(\"...\", b => b.MapRange<float>(\"floatrange\")); protected override void OnModelCreating(ModelBuilder builder) => builder.ForNpgsqlHasRange(\"floatrange\", \"real\"); This will make the provider create a PostgreSQL range called floatrange , over the PostgreSQL type real . Any property with type NpgsqlRange<float> will be seamlessly mapped to it. See the documentation for more details . Seeding for Npgsql-specific types When using some Npgsql-specific types, it wasn't possible to seed values for those types. With EF Core support for seeding any type, #667 allows seeding values for network, bit and range types (more are coming). PostgreSQL index operator classes PostgreSQL allows you to specify operator classes on your indexes , to allow tweaking how the index should work. #481 adds support for managing these. See the documentation for more details . Thanks to @khellang for contributing this! Other features Various issues with enum and range types were fixed, including upper/lower case, quoting and schema management. Many new SQL translations were added, so more of your LINQ expressions can run in the database. We'll be working on our documentation to make these more discoverable. The full list of issues for this release is available here ."
  },
  "efcore/mapping/nts.html": {
    "href": "efcore/mapping/nts.html",
    "title": "Spatial Mapping with NetTopologySuite | Npgsql Documentation",
    "keywords": "Spatial Mapping with NetTopologySuite Note It's recommended that you start by reading the general Entity Framework Core docs on spatial support . PostgreSQL supports spatial data and operations via the PostGIS extension , which is a mature and feature-rich database spatial implementation. .NET doesn't provide a standard spatial library, but NetTopologySuite is quite a good candidate. The Npgsql EF Core provider has a plugin which allows you to map NetTopologySuite's types PostGIS columns, and even translate many useful spatial operations to SQL. This is the recommended way to interact with spatial types in Npgsql. Note that the EF Core NetTopologySuite plugin depends on the Npgsql ADO.NET NetTopology plugin , which provides NetTopologySuite support at the lower level. The EF Core plugin automatically arranged for the ADO.NET plugin to be set up. Setup To set up the NetTopologySuite plugin, add the Npgsql.EntityFrameworkCore.PostgreSQL.NetTopologySuite nuget to your project. Then, make the following modification to your UseNpgsql() line: protected override void OnConfiguring(DbContextOptionsBuilder builder) { builder.UseNpgsql(\"Host=localhost;Database=test;Username=npgsql_tests;Password=npgsql_tests\", o => o.UseNetTopologySuite()); } This will set up all the necessary mappings and operation translators. In addition, to make sure that the PostGIS extension is installed in your database, add the following to your DbContext: protected override void OnModelCreating(ModelBuilder builder) { builder.HasPostgresExtension(\"postgis\"); } At this point spatial support is set up. You can now use NetTopologySuite types as regular properties in your entities, and even perform some operations: public class City { public int Id { get; set; } public string Name { get; set; } public Point Location { get; set; } } var nearbyCities = context.Cities.Where(c => c.Location.Distance(somePoint) < 100); Constraining your type names With the code above, the provider will create a database column of type geometry . This is perfectly fine, but be aware that this type accepts any geometry type (point, polygon...), with any coordinate system (XY, XYZ...). It's good practice to constrain the column to the exact type of data you will be storing, but unfortunately the provider isn't aware of your required coordinate system and therefore can't do that for you. Consider explicitly specifying your column types on your properties as follows: [Column(TypeName=\"geometry (point)\")] public Point Location { get; set; } This will constrain your column to XY points only. The same can be done via the fluent API with HasColumnType() . Operation translation The following table lists NetTopologySuite operations which are translated to PostGIS SQL operations. This allows you to use these NetTopologySuite methods and members efficiently - evaluation will happen on the server side. Since evaluation happens at the server, table data doesn't need to be transferred to the client (saving bandwidth), and in some cases indexes can be used to speed things up. Note that the plugin is far from covering all spatial operations. If an operation you need is missing, please open an issue to request for it. This C# expression... ... gets translated to this SQL .Where(c => c.Polygon.Area() > x) WHERE ST_Area(c.\"Polygon\") > x .Where(c => c.Polygon.AsText() = x WHERE ST_AsText(c.\"Polygon\") = x .Where(c => c.Polygon.Boundary = x WHERE ST_AsCoundary(c.\"Polygon\") = x .Where(c => c.Polygon.Contains(x)) WHERE ST_Contains(c.\"Polygon\", x) .Where(c => c.Polygon.Covers(x)) WHERE ST_Covers(c.\"Polygon\", x) .Where(c => c.Polygon.CoveredBy(x)) WHERE ST_CoveredBy(c.\"Polygon\", x) .Where(c => c.Polygon.Crosses(x)) WHERE ST_Crosses(c.\"Polygon\", x) .Where(c => c.Polygon.Difference(x) = y) WHERE ST_Difference(c.\"Polygon\", x) = y .Where(c => c.Polygon.Disjoint(x)) WHERE ST_Disjoint(c.\"Polygon\", x) .Where(c => c.Point.Distance(x) > y) WHERE ST_Distance(c.\"Polygon\", x) > y .Where(c => c.Polygon.Equals(x)) WHERE c.\"Polygon\" = x .Where(c => c.Polygon.EqualsExact(x)) WHERE c.\"Polygon\" = x .Where(c => c.Polygon.EqualsTopologically(x)) WHERE ST_Equals(c.\"Polygon\", x) .Where(c => c.Geometry.GeometryType() = x) WHERE GeometryType(c.\"GeomCollection\") = x .Where(c => c.GeomCollection.GetGeometryN(2) = x) WHERE ST_GeometryN(c.\"GeomCollection\", 3) = x .Where(c => c.Polygon.Intersection(x) = y) WHERE ST_Intersection(c.\"Polygon\", x) = y .Where(c => c.Polygon.Intersects(x)) WHERE ST_Intersects(c.\"Polygon\", x) .Where(c => c.LineString.IsClosed()) WHERE ST_IsClosed(c.\"LineString\") .Where(c => c.GeomCollection.IsEmpty()) WHERE ST_IsEmpty(c.\"GeomCollection\") .Where(c => c.Polygon.IsSimple()) WHERE ST_IsSimple(c.\"Polygon\") .Where(c => c.Polygon.IsValid()) WHERE ST_IsValid(c.\"Polygon\") .Where(c => c.LineString.Length > x) WHERE ST_Length(c.\"LineString\") > x .Where(c => c.GeomCollection.NumGeometries > x) WHERE ST_NumGeometries(c.\"GeomCollection\") > x .Where(c => c.LineString.NumPoints > x) WHERE ST_NumPoints(c.\"LineString\") > x .Where(c => c.Polygon.Overlaps(x)) WHERE ST_Overlaps(c.\"Polygon\", x) .Where(c => c.Polygon.Relate(x) == y) WHERE ST_Relate(c.\"Polygon\", x) = y .Where(c => c.LineString.Reverse() == x) WHERE ST_Reverse(c.\"Polygon\") = x .Where(c => c.Polygon.SymmetricDifference(x) == y) WHERE ST_SymDifference(c.\"Polygon\", x) = y .Where(c => c.Polygon.Touches(x)) WHERE ST_Touches(c.\"Polygon\", x) .Where(c => c.Polygon.ToText() = x) WHERE ST_AsText(c.\"Polygon\") = x .Where(c => c.Polygon.Union(x) = y) WHERE ST_Union(c.\"Polygon\", x) = y .Where(c => c.Polygon.Within(x)) WHERE ST_Within(c.\"Polygon\", x) .Where(c => c.Point.X == 3) WHERE ST_X(c.\"Point\") = 3 .Where(c => c.Point.Y == 3) WHERE ST_Y(c.\"Point\") = 3 .Where(c => c.Point.Z == 3) WHERE ST_Z(c.\"Point\") = 3 Geography (geodetic) support PostGIS has two types: geometry (for Cartesian coordinates) and geography (for geodetic or spherical coordinates). You can read about the geometry/geography distinction in the PostGIS docs or in this blog post . In a nutshell, geography is much more accurate when doing calculations over long distances, but is more expensive computationally and supports only a small subset of the spatial operations supported by geometry . The Npgsql provider will be default map all NetTopologySuite types to PostGIS geometry . However, you can instruct it to map certain properties to geography instead: protected override void OnModelCreating(ModelBuilder builder) { builder.Entity<City>().Property(b => b.Location).HasColumnType(\"geography (point)\"); } or via an attribute: public class City { public int Id { get; set; } public string Name { get; set; } [Column(TypeName=\"geography\")] public Point Location { get; set; } } Once you do this, your column will be created as geography , and spatial operations will behave as expected."
  },
  "efcore/mapping/full-text-search.html": {
    "href": "efcore/mapping/full-text-search.html",
    "title": "Full Text Search | Npgsql Documentation",
    "keywords": "Full Text Search PostgreSQL has built-in support for full-text search , which allows you to conveniently and efficiently query natural language documents. Mapping PostgreSQL full text search types are mapped onto .NET types built-in to Npgsql. The tsvector type is mapped to NpgsqlTsVector and tsquery is mapped to NpgsqlTsQuery . This means you can use properties of type NpgsqlTsVector directly in your model to create tsvector columns. The NpgsqlTsQuery type on the other hand, is used in LINQ queries. public class BlogPost { public string Title { get; set; } public string Content { get; set; } public NpgsqlTsVector SearchVector { get; set; } } Operation translation Almost all PostgreSQL full text search functions can be called through LINQ queries. All supported EF Core LINQ methods are defined in extension classes in the Microsoft.EntityFrameworkCore namespace, so simply referencing the Npgsql provider will light up these methods. Here is a table showing translations for some operations; if an operation you need is missing, please open an issue to request for it. This C# expression... ... gets translated to this SQL .Select(c => EF.Functions.ToTsVector(\"english\", c.Name)) SELECT to_tsvector('english'::regconfig, c.\"Name\") .Select(c => NpgsqlTsVector.Parse(\"b\")) SELECT CAST('b' AS tsvector) .Select(c => EF.Functions.ToTsQuery(\"english\", \"pgsql\")) SELECT to_tsquery('english'::regconfig, 'pgsql')` .Select(c => NpgsqlTsQuery.Parse(\"b\")) SELECT CAST('b' AS tsquery) .Where(c => c.SearchVector.Matches(\"Npgsql\")) WHERE c.\"SearchVector\" @@ 'Npgsql' .Select(c => EF.Functions.ToTsQuery(c.SearchQuery).ToNegative()) SELECT !! to_tsquery(c.\"SearchQuery\") .Select(c => EF.Functions.ToTsVector(c.Name).SetWeight(NpgsqlTsVector.Lexeme.Weight.A)) SELECT setweight(to_tsvector(c.\"Name\"), 'A') Setting up and querying a full text search index on an entity As the PostgreSQL documentation explains, full-text search requires an index to run efficiently. This section will show two ways to do this, both (currently) requiring raw SQL in your migrations. Read the PostgreSQL docs for more information on the different approaches. Method 1: Expression index The simpler method to use full-text search is to set up an expression index. Let's take the following entity: public class Product { public int Id { get; set; } public string Name { get; set; } public string Description { get; set; } } Create a migration which will contain the index creation SQL ( dotnet ef migrations add ... ). At this point, open the generated migration with your editor and add the following: protected override void Up(MigrationBuilder migrationBuilder) { migrationBuilder.Sql(@\"CREATE INDEX fts_idx ON \"\"Product\"\" USING GIN (to_tsvector('english', \"\"Name\"\" || ' ' || \"\"Description\"\"));\"); } protected override void Down(MigrationBuilder migrationBuilder) migrationBuilder.Sql(@\"DROP INDEX fts_idx;\"); } This will create a full-text search index on the Name and Description columns. You can query as follows: var context = new ProductDbContext(); var npgsql = context.Products .Where(p => EF.Functions.ToTsVector(\"english\", p.Name + \" \" + p.Description).Matches(\"Npgsql\")) .ToList(); Method 2: tsvector column Instead of an expression index, this method will add a tsvector column on your table that updates itself with a trigger. First, add an NpgsqlTsVector property to your entity: public class Product { public int Id { get; set; } public string Name { get; set; } public string Description { get; set; } public NpgsqlTsVector SearchVector { get; set; } } and modify the OnModelCreating() of your context class to add an index as follows: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<Product>() .HasIndex(p => p.SearchVector) .ForNpgsqlHasMethod(\"GIN\"); // Index method on the search vector (GIN or GIST) } Now generate a migration ( dotnet ef migrations add .... ), and open it with your favorite editor, adding the following: public partial class CreateProductTable : Migration { protected override void Up(MigrationBuilder migrationBuilder) { // Migrations for creation of the column and the index will appear here, all we need to do is set up the trigger to update the column: migrationBuilder.Sql( @\"CREATE TRIGGER product_search_vector_update BEFORE INSERT OR UPDATE ON \"\"Products\"\" FOR EACH ROW EXECUTE PROCEDURE tsvector_update_trigger(\"\"SearchVector\"\", 'pg_catalog.english', \"\"Name\"\", \"\"Description\"\");\"); // If you were adding a tsvector to an existing table, you should populate the column using an UPDATE // migrationBuilder.Sql(\"UPDATE \\\"Products\\\" SET \\\"Name\\\" = \\\"Name\\\";\"); } protected override void Down(MigrationBuilder migrationBuilder) { // Migrations for dropping of the column and the index will appear here, all we need to do is drop the trigger: migrationBuilder.Sql(\"DROP TRIGGER product_search_vector\"); } } Any inserts or updates on the Products table will now update the SearchVector column and maintain it automatically. You can query it as follows: var context = new ProductDbContext(); var npgsql = context.Products .Where(p => p.SearchVector.Matches(\"Npgsql\")) .ToList();"
  },
  "efcore/index.html": {
    "href": "efcore/index.html",
    "title": "Getting Started | Npgsql Documentation",
    "keywords": "Getting Started Npgsql has an Entity Framework (EF) Core provider. It behaves like other EF Core providers (e.g. SQL Server), so the general EF Core docs apply here as well. If you're just getting started with EF Core, those docs are the best place to start. Development happens in the Npgsql.EntityFrameworkCore.PostgreSQL repository, all issues should be reported there. Configuring the project file To use the Npgsql EF Core provider, add a dependency on Npgsql.EntityFrameworkCore.PostgreSQL . You can follow the instructions in the general EF Core Getting Started docs . Below is a .csproj file for a console application that uses the Npgsql EF Core provider: <Project Sdk=\"Microsoft.NET.Sdk\"> <PropertyGroup> <TargetFramework>netcoreapp2.2</TargetFramework> </PropertyGroup> <ItemGroup> <PackageReference Include=\"Npgsql.EntityFrameworkCore.PostgreSQL\" Version=\"2.2.0\" /> </ItemGroup> </Project> Defining a DbContext using System.Collections.Generic; using Microsoft.EntityFrameworkCore; namespace ConsoleApp.PostgreSQL { public class BloggingContext : DbContext { public DbSet<Blog> Blogs { get; set; } public DbSet<Post> Posts { get; set; } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql(\"Host=my_host;Database=my_db;Username=my_user;Password=my_pw\"); } public class Blog { public int BlogId { get; set; } public string Url { get; set; } public List<Post> Posts { get; set; } } public class Post { public int PostId { get; set; } public string Title { get; set; } public string Content { get; set; } public int BlogId { get; set; } public Blog Blog { get; set; } } } Additional configuration for ASP.NET Core applications Modify the ConfigureServices method in Startup.cs : public IServiceProvider ConfigureServices(IServiceCollection services) => services.AddEntityFrameworkNpgsql() .AddDbContext<BloggingContext>() .BuildServiceProvider(); Using an Existing Database (Database-First) The Npgsql EF Core provider also supports reverse-engineering a code model from an existing PostgreSQL database (\"database-first\"). To do so, use dotnet CLI to execute the following: dotnet ef dbcontext scaffold \"Host=my_host;Database=my_db;Username=my_user;Password=my_pw\" Npgsql.EntityFrameworkCore.PostgreSQL"
  },
  "efcore/mapping/range.html": {
    "href": "efcore/mapping/range.html",
    "title": "Range Type Mapping | Npgsql Documentation",
    "keywords": "Range Type Mapping PostgreSQL has the unique feature of supporting range data types . Ranges represent a range of numbers, dates or other data types, and allow you to easily query ranges which contain a value, perform set operations (e.g. query ranges which contain other ranges), and other similar operations. The range operations supported by PostgreSQL are listed in this page . The Npgsql EF Core provider allows you to seemlessly map PostgreSQL ranges, and even perform operations on them that get translated to SQL for server evaluation. Mapping ranges Npgsql maps PostgreSQL ranges to the generic CLR type NpgqslRange<T> : public class Event { public int Id { get; set; } public string Name { get; set; } public NpgsqlRange<DateTime> Duration { get; set; } } This will create a column of type daterange in your database. You can similarly have properties of type NpgsqlRange<int> , NpgsqlRange<long> , etc. User-defined ranges Note This feature was introduced in version 2.2 PostgreSQL comes with 6 built-in ranges: int4range , int8range , numrange , tsrange , tstzrange , daterange ; these can be used simply by adding the appropriate NpgsqlRange<T> property in your entities as shown above. You can also define your own range types over arbitrary types, and use those in EF Core as well. To make the EF Core type mapper aware of your user-defined range, call the MapRange() method in your context's OnConfiguring() method as follows: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql( \"<connection_string>\", options => options.MapRange<float>(\"floatrange\")); This allows you to have properties of type NpgsqlRange<float> , which will be mapped to PostgreSQL floatrange . The above does not create the floatrange type for you. In order to do that, include the following in your context's OnModelCreating() : protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.ForNpgsqlHasRange(\"floatrange\", \"real\"); This will cause the appropriate CREATE TYPE ... AS RANGE statement to be generated in your migrations, ensuring that your range is created and ready for use. Note that ForNpgsqlHasRange() supports additional parameters as supported by PostgreSQL CREATE TYPE . Operation translation Ranges can be queried via extensions methods on NpgsqlRange : var events = context.Events.Where(p => p.Duration.Contains(someDate)); This will translate to an SQL operation using the PostgreSQL @> operator, evaluating at the server and saving you from transfering the entire Events table to the client. Note that you can (and probably should) create indexes to make this operation more efficient, see the PostgreSQL docs for more info. The following table lists the range operations that currently get translated. If you run into a missing operation, please open an issue. C# expression SQL generated by Npgsql .Where(c => c.SomeRange.Contains(3)) WHERE x.\"SomeRange\" @> 3 .Where(c => c.SomeRange.Contains(otherRange)) WHERE x.\"SomeRange\" @> @__otherRange_0 .Where(c => c.SomeRange.ContainedBy(otherRange)) WHERE x.\"SomeRange\" <@ @__otherRange_0 .Where(c => c.SomeRange == otherRange) WHERE x.\"SomeRange\" = @__otherRange_0 .Where(c => c.SomeRange != otherRange) WHERE x.\"SomeRange\" <> @__otherRange_0 .Where(c => c.SomeRange.Overlaps(otherRange)) WHERE x.\"SomeRange\" && @__otherRange_0 .Where(c => c.SomeRange.IsStrictlyLeftOf(otherRange)) WHERE x.\"SomeRange\" << @__otherRange_0 .Where(c => c.SomeRange.IsStrictlyRightOf(otherRange)) WHERE x.\"SomeRange\" >> @__otherRange_0 .Where(c => c.SomeRange.DoesNotExtendLeftOf(otherRange)) WHERE x.\"SomeRange\" &> @__otherRange_0 .Where(c => c.SomeRange.DoesNotExtendRightOf(otherRange)) WHERE x.\"SomeRange\" <& @__otherRange_0 .Where(c => c.SomeRange.IsAdjacentTo(otherRange)) WHERE x.\"SomeRange\" -\\|- @__otherRange_0 .Select(c => c.SomeRange.Union(otherRange)) SELECT x.\"SomeRange\" + @__otherRange_0 .Select(c => c.SomeRange.Intersect(otherRange)) SELECT x.\"SomeRange\" * @__otherRange_0 .Select(c => c.SomeRange.Except(otherRange)) SELECT x.\"SomeRange\" - @__otherRange_0"
  },
  "efcore/mapping/general.html": {
    "href": "efcore/mapping/general.html",
    "title": "Type mapping | Npgsql Documentation",
    "keywords": "Type mapping The EF Core provider transparently maps the types supported by Npgsql at the ADO.NET level - see the Npgsql ADO type mapping page . This means that you can use PostgreSQL-specific types, such as inet or circle , directly in your entities. Simply define your properties just as if they were a simple type, such as a string : public class MyEntity { public int Id { get; set; } public string Name { get; set; } public IPAddress IPAddress { get; set; } public NpgsqlCircle Circle { get; set; } public int[] SomeInts { get; set; } } Special types such as arrays and enums have their own documentation pages with more details. PostgreSQL composite types , while supported at the ADO.NET level, aren't yet supported in the EF Core provider. This is tracked by #22 . Explicitly specifying data types In some cases, your .NET property type can be mapped to several PostgreSQL data types; a good example is a string , which will be mapped to text by default, but can also be mapped to jsonb . You can explicitly specify the PostgreSQL data type by adding the following to your model's OnModelCreating : builder.Entity<Blog>() .Property(b => b.SomeStringProperty) .HasColumnType(\"jsonb\"); Or, if you prefer annotations, use a ColumnAttribute : [Column(TypeName=\"jsonb\")] public string SomeStringProperty { get; set; } Operation translation to SQL Entity Framework Core allows providers to translate query expressions to SQL for database evaluation. For example, PostgreSQL supports regular expression operations , and the Npgsql EF Core provider automatically translates .NET's Regex.IsMatch() to use this feature. Since evaluation happens at the server, table data doesn't need to be transferred to the client (saving bandwidth), and in some cases indexes can be used to speed things up. The same C# code on other providers will trigger client evaluation. Below are some Npgsql-specific translations, many additional standard ones are supported as well. See the other pages in the mapping section for more supported types and operations. C# expression SQL generated by Npgsql .Where(c => Regex.IsMatch(c.Name, \"^A+\") WHERE \"c\".\"Name\" ~ '^A+' .Where(c => EF.Functions.Like(c.Name, \"foo%\") WHERE \"c\".\"Name\" LIKE 'foo%' .Where(c => EF.Functions.ILike(c.Name, \"foo%\") WHERE \"c\".\"Name\" ILIKE 'foo%' (case-insensitive LIKE) .Select(c => EF.Functions.ToTsVector(\"english\", c.Name)) SELECT to_tsvector('english'::regconfig, \"c\".\"Name\") .Select(c => EF.Functions.ToTsQuery(\"english\", \"pgsql\")) SELECT to_tsquery('english'::regconfig, 'pgsql') .Where(c => c.SearchVector.Matches(\"Npgsql\")) WHERE \"c\".\"SearchVector\" @@ 'Npgsql' .Select(c => EF.Functions.ToTsQuery(c.SearchQuery).ToNegative()) SELECT (!! to_tsquery(\"c\".\"SearchQuery\")) .Select(c => EF.Functions.ToTsVector(c.Name).SetWeight(NpgsqlTsVector.Lexeme.Weight.A)) SELECT setweight(to_tsvector(\"c\".\"Name\"), 'A')"
  },
  "doc/types/datetime.html": {
    "href": "doc/types/datetime.html",
    "title": "Date and Time Handling | Npgsql Documentation",
    "keywords": "Date and Time Handling Note Since 4.0 the recommended way of working with date/time types is the NodaTime plugin . Handling date and time values usually isn't hard, but you must pay careful attention to differences in how the .NET types and PostgreSQL represent dates. It's worth reading the PostgreSQL date/time type documentation to familiarize yourself with PostgreSQL's types. .NET types and PostgreSQL types Warning A common mistake is for users to think that the PostgreSQL timestamp with timezone type stores the timezone in the database. This is not the case: only the timestamp is stored. There is no single PostgreSQL type that stores both a date/time and a timezone, similar to .NET DateTimeOffset . The .NET and PostgreSQL types differ in the resolution and range they provide; the .NET type usually have a higher resolution but a lower range than the PostgreSQL types: PostgreSQL type Precision/Range .NET Native Type Precision/Range Npgsql .NET Provider-Specific Type timestamp 1 microsecond, 4713BC-294276AD DateTime 100 nanoseconds, 1AD-9999AD NpgsqlDateTime timestamp with timezone 1 microsecond, 4713BC-294276AD DateTime 100 nanoseconds, 1AD-9999AD NpgsqlDateTime date 1 day, 4713BC-5874897AD DateTime 100 nanoseconds, 1AD-9999AD NpgsqlDate time 1 microsecond, 0-24 hours TimeSpan 100 nanoseconds, -10,675,199 - 10,675,199 days N/A time with timezone 1 microsecond, 0-24 hours DateTimeOffset (ignore date) 100 nanoseconds, 1AD-9999AD N/A interval 1 microsecond, -178000000-178000000 years TimeSpan 100 nanoseconds, -10,675,199 - 10,675,199 days NpgsqlTimeSpan If your needs are met by the .NET native types, it is best that you use them directly with Npgsql. If, however, you require the extended range of a PostgreSQL type you can use Npgsql's provider-specific types, which represent PostgreSQL types in an exact way. Timezones It's critical to understand exactly how timezones and timezone conversions are handled between .NET types and PostgreSQL. In particular, .NET's DateTime has a Kind property which impacts how Npgsql reads and writes the value. By default, DateTime is sent to PostgreSQL as a timestamp without time zone - no timezone conversion of any kind will occur, and your DateTime instance will be transferred as-is to PostgreSQL. This is the recommended way to store timestamps in the database. Note that you may still send DateTime as timestamp with time zone by setting NpgsqlDbType.TimestampTz on your NpgsqlParameter ; in this case, if the Kind is Local , Npgsql will convert the value to UTC before sending it to PostgreSQL. Otherwise, it will be sent as-is. You can also send DateTimeOffset values, which are written as timestamptz and are converted to UTC before sending. PostgreSQL time with time zone is the only date/time type which actually stores a timezone in the database. You can use a DateTimeOffset to send one to PostgreSQL, in which case the date component is dropped and the time and timezone are preserved. You can also send a DateTime , in which case the Kind will determine the the timezone sent to the database. Detailed Behavior: Sending values to the database .NET value NpgsqlDbType Action DateTime NpgsqlDbType.Timestamp (default) Send as-is DateTime(Kind=UTC,Unspecified) NpgsqlDbType.TimestampTz Send as-is DateTime(Kind=Local) NpgsqlDbType.TimestampTz Convert to UTC locally before sending DateTimeOffset NpgsqlDbType.TimestampTz (default) Convert to UTC locally before sending TimeSpan NpgsqlDbType.Time (default) Send as-is DateTimeOffset NpgsqlDbType.TimeTz Send time and timezone DateTime(Kind=UTC) NpgsqlDbType.TimeTz Send time and UTC timezone DateTime(Kind=Local) NpgsqlDbType.TimeTz Send time and local system timezone DateTime(Kind=Unspecified) NpgsqlDbType.TimeTz Assume local, send time and local system timezone Detailed Behavior: Reading values from the database PG type .NET value Action timestamp DateTime (default) Kind=Unspecified timestamptz DateTime (default) Kind=Local (according to system timezone) timestamptz DateTimeOffset In local timezone offset time TimeSpan (default) As-is timetz DateTimeOffset (default) Date component is empty timetz TimeSpan Strip offset, read as-is timetz DateTime Strip offset, date is empty Further Reading If you're really interested in some of the mapping decisions above, check out this issue ."
  },
  "doc/release-notes/3.0.html": {
    "href": "doc/release-notes/3.0.html",
    "title": "Npgsql 3.0 Release Notes | Npgsql Documentation",
    "keywords": "Migrating from 2.2 to 3.0 Version 3.0 represents a near-total rewrite of Npgsql. In addition to changing how Npgsql works internally and communicates with PostgreSQL, a conscious effort was made to better align Npgsql with the ADO.NET specs/standard and with SqlClient where that made sense. This means that you cannot expect to drop 3.0 as a replacement to 2.2 and expect things to work - upgrade cautiously and test extensively before deploying anything to production. The following is a non-exhaustive list of things that changed. If you run against a breaking change not documented here, please let us know and we'll add it. Major Support for .NET 2.0, .NET 3.5 and .NET 4.0 has been dropped - you will have to upgrade to .NET 4.5 to use Npgsql 3.0. We'll continue to do bugfixes on the 2.2 branch for a while on a best-effort basis. The Entity Framework provider packages have been renamed to align with Microsoft's new naming. The new packages are EntityFramework5.Npgsql and EntityFramework6.Npgsql . EntityFramework7.Npgsql is in alpha. A brand-new bulk copy API has been written, using binary encoding for much better performance. See the docs . Composite (custom) types aren't supported yet, but this is a high-priority feature for us. See #441 . SSL Npgsql 2.2 didn't perform validation on the server's certificate by default, so self-signed certificate were accepted. The new default is to perform validation. Specify the Trust Server Certificate connection string parameter to get back previous behavior. The \"SSL\" connection string parameter has been removed, use \"SSL Mode\" instead. The \"SSL Mode\" parameter's Allow option has been removed, as it wasn't doing anything. Type Handling Previously, Npgsql allowed writing a NULL by setting NpgsqlParameter.Value to null . This is not allowed in ADO.NET and is no longer supported, set to DBNull.Value instead. In some cases, you will now be required to explicitly set a parameter's type although you didn't have to before (you'll get an error 42804 explaining this). This can happen especially in Dapper custom custom type handlers ( #694 ). Simply set the NpgsqlDbType property on the parameter. Removed support for writing a parameter with an IEnumerable<T> value, since that would require Npgsql to enumerate it multiple times internally. IList<T> and IList are permitted. It is no longer possible to write a .NET enum to an integral PostgreSQL column (e.g. int4). Proper enum support has been added which allows writing to PostgreSQL enum columns (see the docs . To continue writing enums to integral columns as before, simply add an explicit cast to the integral type in your code. NpgsqlMacAddress has been removed and replaced by the standard .NET PhysicalAddress. Npgsql's BitString has been removed and replaced by the standard .NET BitArray. NpgsqlTime has been removed and replaced by the standard .NET TimeSpan. NpgsqlTimeZone has been removed. NpgsqlTimeTZ now holds 2 TimeSpans, rather than an NpgsqlTime and an NpgsqlTimeZone. NpgsqlTimeStamp no longer maps DateTime.{Max,Min}Value to {positive,negative} infinity. Use NpgsqlTimeStamp.Infinity and NpgsqlTimeStamp.MinusInfinity explicitly for that. You can also specify the \"Convert Infinity DateTime\" connection string parameter to retain the old behavior. Renamed NpgsqlInet's addr and mask to Address and Mask. NpgsqlPoint now holds Doubles instead of Singles ( #437 ). NpgsqlDataReader.GetFieldType() and GetProviderSpecificFieldType() now return Array for arrays. Previously they returned int[], even for multidimensional arrays. NpgsqlDataReader.GetDataTypeName() now returns the name of the PostgreSQL type rather than its OID. Retired features Removed the \"Preload Reader\" feature, which loaded the entire resultset into memory. If you require this (inefficient) behavior, read the result into memory outside Npgsql. We plan on working on MARS support, see #462 . The \"Use Extended Types\" parameter is no longer needed and isn't supported. To access PostgreSQL values that can't be represented by the standard CLR types, use the standard ADO.NET NpgsqlDataReader.GetProviderSpecificValue or even better, the generic NpgsqlDataReader.GetFieldValue<T> . Removed the feature where Npgsql automatically \"dereferenced\" a resultset of refcursors into multiple resultsets (this was used to emulate returning multiple resultsets from stored procedures). Note that if your function needs to return a single resultset, it should be simply returning a table rather than a cursor (see RETURNS TABLE ). See #438 . Removed the AlwaysPrepare connection string parameter Removed the Encoding connection string parameter, which was obsolete and unused anyway (UTF8 was always used regardless of what was specified) Removed the Protocol connection string parameter, which was obsolete and unused anyway (protocol 3 was always used) Removed NpgsqlDataReader.LastInsertedOID, it did not allow accessing individual OIDs in multi-statement commands. Replaced with NpgsqlDataReader.Statements, which provides OID and affected row information on a statement-by-statement basis. Removed NpgsqlDataReader.HasOrdinal , was a badly-named non-standard API without a serious use case. GetName() can be used as a workaround. Other It is no longer possible to create database entities (tables, functions) and then use them in the same multi-query command - you must first send a command creating the entity, and only then send commands using it. See #641 for more details. Previously, Npgsql set DateStyle=ISO, lc_monetary=C and extra_float_digits=3 on all connections it created. This is no longer case, if you rely on these parameters you must send them yourself. NpgsqlConnection.Clone() will now only return a new connection with the same connection string as the original. Previous versions returned an open connection if the original was open, and copied the Notice event listeners as well. Note: NpgsqlConnection.Clone() was accidentally missing from 3.0.0 and 3.0.1. Removed the obsolete NpgsqlParameterCollection.Add(name, value) method. Use AddWithValue() instead, which also exists in SqlClient. The savepoint manipulation methods on NpgsqlTransaction have been renamed from Save , and Rollback to CreateSavepoint and RollbackToSavepoint . This broke the naming conventions for these methods across other providers (SqlClient, Oracle...) and so in 3.0.2 the previous names were returned and the new names marked as obsolete. 3.1 will remove the the new names and leaves only Save and Rollback . See #738 . The default CommandTimeout has changed from 20 seconds to 30 seconds, as in ADO.NET . CommandType.TableDirect now requires CommandText to contain the name of a table, as per the MSDN docs . Multiple tables (join) aren't supported. CommandType.StoredProcedure now requires CommandText contain only the name of a function, without parentheses or parameter information, as per the MSDN docs . Moved the LastInsertedOID property from NpgsqlCommand to NpgsqlReader, like the standard ADO.NET RecordsAffected ."
  },
  "efcore/miscellaneous.html": {
    "href": "efcore/miscellaneous.html",
    "title": "Miscellaneous | Npgsql Documentation",
    "keywords": "Miscellaneous PostgreSQL extensions The Npgsql EF Core provider allows you to specify PostgreSQL extensions that should be set up in your database. Simply use HasPostgresExtension in your context's OnModelCreating method: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.HasPostgresExtension(\"hstore\"); Optimistic Concurrency and Concurrency Tokens Entity Framework Core supports the concept of optimistic concurrency - a property on your entity is designated as a concurrency token, and EF Core detects concurrent modifications by checking whether that token has changed since the entity was read. You can read more about this in the EF Core docs . Although applications can update concurrency tokens themselves, we frequently rely on the database automatically updating a column on update - a \"last modified\" timestamp, an SQL Server rowversion , etc. Unfortunately PostgreSQL doesn't have such auto-updating columns - but there is one feature that can be used for concurrency token. All PostgreSQL tables have a set of implicit and hidden system columns , among which xmin holds the ID of the latest updating transaction. Since this value automatically gets updated every time the row is changed, it is ideal for use as a concurrency token. To enable this feature on an entity, insert the following code into your context's OnModelCreating method: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>() .ForNpgsqlUseXminAsConcurrencyToken(); Execution Strategy Since 2.0.0, the Npgsql EF Core provider provides a retrying execution strategy, which will attempt to detect most transient PostgreSQL/network errors and will automatically retry your operation. To enable, place the following code in your context's OnModelConfiguring : protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql( \"<connection_string>\", options => options.EnableRetryOnFailure()); This strategy relies on the IsTransient property of NpgsqlException . Both this property and the retrying strategy are new and should be considered somewhat experimental - please report any issues. Comments PostgreSQL allows you to attach comments to database objects, which can help explain their purpose for someone examining the schema. The Npgsql EF Core provider supports this for tables or columns, simply set the comment in your model's OnModelCreating as follows: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<MyEntity>() .ForNpgsqlHasComment(\"Some comment\"); Certificate authentication The Npgsql allows you to provide a callback for verifying the server-provided certificates, and to provide a callback for providing certificates to the server. The latter, if properly set up on the PostgreSQL side, allows you to do client certificate authentication - see the Npgsql docs and also the PostgreSQL docs on setting this up. The Npgsql EF Core provider allows you to set these two callbacks on the DbContextOptionsBuilder as follows: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql( \"<connection_string>\", options => { options.RemoteCertificateValidationCallback(MyCallback1); options.ProvideClientCertificatesCallback(MyCallback2); }); You may also consider passing Trust Server Certificate=true in your connection string to make Npgsql accept whatever certificate your PostgreSQL provides (useful for self-signed certificates). Database Creation Specifying the administrative db When the Npgsql EF Core provider creates or deletes a database ( EnsureCreated() , EnsureDeleted() ), it must connect to an administrative database which already exists (with PostgreSQL you always have to be connected to some database, even when creating/deleting another database). Up to now the postgres database was used, which is supposed to always be present. However, there are some PostgreSQL-like databases where the postgres database is not available. For these cases you can specify the administrative database as follows: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql( \"<connection_string>\", options => options.UseAdminDatabase(\"my_admin_db\")); Using a database template When creating a new database, PostgreSQL allows specifying another \"template database\" which will be copied as the basis for the new one. This can be useful for including database entities which are not managed by Entity Framework Core. You can trigger this by using HasDatabaseTemplate in your context's OnModelCreating : protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.HasDatabaseTemplate(\"my_template_db\"); Setting a tablespace PostgreSQL allows you to locate your database in different parts of your filesystem, via tablespaces . The Npgsql EF Core provider allows you to specify your database's namespace: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.ForNpgsqlUseTablespace(\"my_tablespace\"); You must have created your tablespace prior to this via the CREATE TABLESPACE command - the Npgsql EF Core provider does not do this for you. Note also that specifying a tablespace on specific tables is not supported. CockroachDB Interleave In Parent If you're using CockroachDB, the Npgsql EF Core provider exposes its \"interleave in parent\" feature . Use the following code: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Customer>() .ForCockroachDbInterleaveInParent( typeof(ParentEntityType), new List<string> { \"prefix_column_1\", \"prefix_column_2\" });"
  },
  "doc/types/legacy-postgis.html": {
    "href": "doc/types/legacy-postgis.html",
    "title": "PostGIS Legacy Type Plugin | Npgsql Documentation",
    "keywords": "PostGIS Legacy Type Plugin Since 4.0, Npgsql supports type plugins , which are external nuget packages that modify how Npgsql maps PostgreSQL values to CLR types. The previous support for PostGIS spatial types has been moved out of Npgsql and into the plugin Npgsql.LegacyPostgis. The recommended way to read and write spatial types is now Npgsql.NetTopologySuite , which maps PostGIS types to NetTopologySuite , a more complete library dedicated to spatial. The NetTopologySuite plugin is now the recommended way to do spatial in Npgsql, but the Npgsql.LegacyPostgis still exists to provide support for the previous types. At this time, the legacy types only support geometry, not geography, and only XY (not XYZ, XYM or XYZM). Setup To use the PostGIS legacy plugin, simply add a dependency on Npgsql.LegacyPostgis and set it up: using Npgsql; // Place this at the beginning of your program to use legacy PostGIS everywhere (recommended): NpgsqlConnection.GlobalTypeMapper.UseLegacyPostgis(); // Or to temporarily use legacy PostGIS on a single connection only: conn.TypeMapper.UseLegacyPostgis(); Usage If you've used the internal PostGIS types in Npgsql 3.2 or earlier, the plugin works in the same way: NpgsqlConnection.GlobalTypeMapper.UseLegacyPostgis(); // Write var cmd = new NpgsqlCommand(\"INSERT INTO table (pg_point, pg_polygon) VALUES (@point, @polygon)\", conn); cmd.Parameters.AddWithValue(\"point\", new PostgisPoint(3.5, 4.5)); cmd.ExecuteNonQuery(); // Read var cmd = new NpgsqlCommand(\"SELECT * FROM table\", conn); var reader = cmd.ExecuteReader(); while (reader.Read()) { var point = reader.GetFieldValue<PostgisPoint>(0); var polygon = reader.GetFieldValue<PostgisPolygon>(1); }"
  },
  "doc/release-notes/3.2.html": {
    "href": "doc/release-notes/3.2.html",
    "title": "Npgsql 3.2 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 3.2 Npgsql 3.2 is out and available on nuget.org. This is a major release with substantial internal changes and should be deployed with care. For critical applications it may be advisable to wait until 3.2.1 is out. This release contains a large number of new features, but the main focus is performance - some usage scenarios may show dramatic improvements. See below for more details. Major Changes Prepared statements are now persistent (survive beyond pooled connection close/open), providing significant performance improvements for applications with short-lived connections, such as most webapps ( #483 ). Also, statements can optionally be prepared automatically by Npgsql based on use, unlocking prepared statement performance for O/RMs and data layers which don't prepare themselves, such as Dapper or Entity Framework Core ( #1237 ). See this blog post for more info . The internal I/O system has been overhauled to continue supporting sync and async I/O, but with a vastly better coding model. This should eliminate most protocol sync bugs, and make it much easier to maintain and write new type handlers ( #1326 ). Kerberos login (\"integrated security\") is now support on Linux/Mac ( #1079 ). Support for System.Transactions and distributed transactions has been rewritten, and should have fewer problems than before ( #122 ). Performance counters have been implemented, similar to what SqlClient provides . See the documentation for more information ( #619 ). The Visual Studio integration extension (DDEX) has been rewritten for a much better installation experience, and includes some new features as well ( #1407 ). See the docs for more info . If your application attempts to make use of more than one connection at the same time, an \"operation already in progress\" was thrown. This exception now provides more information to help you track down the bug ( #1248 ). Many other small changes have been made, especially with regards to performance. Here's the full list . Breaking Changes from 3.1 Connections can no longer be constructed with NpgsqlConnectionStringBuilder - only plain string connection strings are supported ( #1415 ). The Buffer Size connection string parameter has been replaced by Read Buffer Size and Write Buffer Size ."
  },
  "dev/build-server.html": {
    "href": "dev/build-server.html",
    "title": "Build Server Notes | Npgsql Documentation",
    "keywords": "This page describes the steps used to set up the Npgsql build server. If you're upgrading the TeamCity version, see \"Give agent service start/stop permissions\" below. Install all supported versions of the Postgresql backend At the time of writing, this means 9.1, 9.2, 9.3, 9.4, 9.5. They are configured on ports 5491, 5492, 5493, 5494, 5495. For SSPI/GSS tests, you need to set up a user with the same name as the user that will be running the tests (i.e. teamcity_agent). You must also add the following lines at the top of each PG's pg_hba.conf to set up SSPI/GSS for that user: host all teamcity_agent 127.0.0.1/32 sspi include_realm=0 host all teamcity_agent ::1/128 sspi include_realm=0 See this page on SSPI . Install a TeamCity-dedicated Postgresql cluster TeamCity itself requires an SQL database, but we don't want it to run in the same environment as that used for the unit tests. So choosing the latest stable Postgresql version (9.6 at time of writing), we create a new Postgresql cluster: initdb -U postgres -W c:\\dev\\TeamcityPostgresData Next we set up a Windows service that starts up the new cluster: pg_ctl register -N postgresql-9.6-teamcity -U teamcity -P <password> -D c:\\dev\\TeamcityPostgresData Finally, create a a user and database and point TeamCity to it. Install .NET SDKs for all supported .NET versions .NET 4.0 (Windows 7 SDK): http://www.microsoft.com/en-us/download/details.aspx?id=8279 .NET 4.5 (Windows 8 SDK): http://msdn.microsoft.com/en-us/windows/hardware/hh852363.aspx .NET 4.5.1 (Windows 8.1 SDK): http://msdn.microsoft.com/en-us/windows/hardware/bg162891.aspx While installing the SDK for .NET 4.0, I had this problem: http://support.microsoft.com/kb/2717426 Give agent service start/stop permissions When upgrading TeamCity, the agent needs to be able to stop and start the Windows service. This is how you can grant a normal user specific permissions on specific services: Download and install subinacl from http://www.microsoft.com/en-us/download/details.aspx?id=23510 cd C:\\Program Files (x86)\\Windows Resource Kits\\Tools\\ subinacl /service TCBuildAgent /grant=teamcity_agent=TO Update build status back in github Download the plugin from https://github.com/jonnyzzz/TeamCity.GitHub , get the ZIP Drop the ZIP in the TeamCity content dir's plugins subdir Add the Build Feature \"Report change status to GitHub\". Configure everything appropriately, and be sure the user you set up has push access to the repository! Install assorted dev utilities GitVersion (with Chocolatey) WiX toolset (v3.10.1 at time of writing) Install WiX WiX 3.10 has a dependency on .NET Framework 3.5, but there's some issue blocking its installation on Windows Server 2012 R2 (at least on Azure). A good workaround is to simply install via Powershell ( Add-WindowsFeature NET-Framework-Core ), see https://msdn.microsoft.com/en-us/library/dn169001(v=nav.70).aspx#InstallNET35 . Note that ICE validation is disabled because apparently it requires an interactive account or admin privileges, which doesn't work in continuous integration."
  },
  "efcore/release-notes/1.1.html": {
    "href": "efcore/release-notes/1.1.html",
    "title": "Migrating to 1.1 | Npgsql Documentation",
    "keywords": "Migrating to 1.1 Version 1.1.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget. This version works with version 1.1.0 of Entity Framework Core , and contains some new Npgsql features as well. Note that if you're using the command-line tools, you'll have to modify your tools section as described in the EF Core release post: \"tools\": { \"Microsoft.EntityFrameworkCore.Tools.DotNet\": \"1.0.0-preview3-final\" }, New Features Aside from general EF Core features, version 1.1.0 of the Npgsql provider contains the following: Hilo key generation ( #5 ). This can be a much more efficient way to generate autoincrement key values. PostgreSQL array mapping ( #15 ). This allows you to have plain CLR arrays on your entities, and have those arrays mapped to native PostgreSQL array columns . Optimistic concurrency with PostgreSQL's xmin column ( #19 ). Simply specify .UseXminAsConcurrencyToken() on an entity to start using this, see the EF docs for more details . Cleanup of how serial (autoincrement) and generated GUID/UUID columns are managed. Here's the full list of issues . Please report any problems to https://github.com/npgsql/Npgsql.EntityFrameworkCore.PostgreSQL . Upgrading from 1.0.x If you've used 1.0.x without migrations, you can simply upgrade and everything should just work. Unfortunately, if you already have migrations from 1.0.x you'll have to do some manual fixups because of some bad decisions that were previously made. If deleting your old migrations and starting over (e.g. non-production database) is an option, you may wish to do so. The following are instructions for fixing up 1.0.x migrations. First, Npgsql 1.0.x used a problematic method to identify serial (autoincrement) columns in migrations. If you look at your migration code you'll see .Annotation(\"Npgsql:ValueGeneratedOnAdd\", true) on various columns. Unfortunately this annotation is also present on non-serial columns, e.g. columns with default values. This causes various issues and has been replaced in 1.1. However, you'll have to manually remove .Annotation(\"Npgsql:ValueGeneratedOnAdd\", true) , and replace it with .Annotation(\"Npgsql:ValueGenerationStrategy\", NpgsqlValueGenerationStrategy.SerialColumn) but only on columns which should be serial (e.g. not on columns with defaults). If you attempt to run a migration that has the old annotation, Npgsql will throw an exception and refuse to run your migrations. Unfortunately, this change will cause some incorrect changes the first time you add a migration after the upgrade. To avoid this, simply add a dummy migration right after upgrading to 1.1 and then delete the two new files generated for the dummy migration, but keep the changes made to your ModelSnapshot.cs . From this point on everything should be fine. Make sure you have no pending changes to your model before doing this! . Apologies for this problematic upgrade procedure, it should at least keep things clean going forward."
  },
  "efcore/value-generation.html": {
    "href": "efcore/value-generation.html",
    "title": "Value Generation (auto-increment) | Npgsql Documentation",
    "keywords": "Value Generation (auto-increment) See the general EF Docs on value generation to better understand the concepts described here. Serial and identity columns The traditional PostgreSQL autoincrement mechanism is \"serial columns\". Serial columns are simply regular columns with default values coming from a sequence. The are defined with a datatype of serial , but this is simply a shorthand for creating a column of type bigint and tying it to a sequence. More detail on serial columns can be found in the PostgreSQL docs . PostgreSQL 10 introduced new \"identity columns\", which conform to the SQL standard and provide some advantages over serial columns. With identity columns, the relationship between the column and its driving sequence is remembered, and further management of the column is much simpler. For an overview of the advantages of identity over serial, see this blog post . The Npgsql EF Core provider allows you to choose which of the above you want on a property-by-property basis, or globally on your model. The following \"value generation strategies\" are available: Serial : the traditional PostgreSQL serial column. This will create the column with the serial datatype. Identity always : an identity column whose values are always generated at the database - you cannot provide values from your application. This will generate the clause GENERATED ALWAYS AS IDENTITY on your column. Identity by default : an identity column whose values are by default generated at the database, but you can still override this behavior by providing values from your application. This will generate the clause GENERATED BY DEFAULT AS IDENTITY on your column. Sequence HiLo : See below To maintain backwards compatibility with existing EF Core models, serial columns are still the default: when ValueGeneratedOnAdd is specified on a short, int or long property, the Npgsql EF Core provider will automatically map it to a serial column. Note that EF Core will automatically recognize key properties by convention (e.g. a property called Id in your entity) and will implicitly set them to ValueGeneratedOnAdd , so if you set up a simple model with id columns, they will get created as serial columns. To use identity columns for all value-generated properties on a new model, simply place the following in your context's OnModelCreating() : protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.ForNpgsqlUseIdentityColumns(); This will create make all keys and other properties which have .ValueGeneratedOnAdd() have Identity by default . You can use ForNpgsqlUseIdentityAlwaysColumns() to have Identity always , and you can also specify identity on a property-by-property basis with UseNpgsqlIdentityColumn() and UseNpgsqlIdentityAlwaysColumn() . If you set identity for existing columns, or even for your entire existing model, the Npgsql EF Core provider will safely migrate you from serial to identity, preserving current sequence values. However, back up your database before you do this and test carefully, as migrating from identity to serial isn't supported at this time. Warning There was a significant and breaking change in 1.1. If you are upgrading from 1.0 and have existing migrations, please read the release notes . Standard Sequence-Driven Columns While serial sets up a sequence for you, you may want to manage sequence creation yourself. This can be useful for cases where you need to control the sequence's increment value (i.e. increment by 2), populate two columns from the same sequence, etc. Adding a sequence to your model is described in the general EF Core documentation ; once the sequence is specified, you can simply set a column's default value to extract the next value from that sequence. Note that the SQL used to fetch the next value from a sequence differs across databases (see the PostgreSQL docs ). Your models' OnModelCreating should look like this: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.HasSequence<int>(\"OrderNumbers\") .StartsAt(1000) .IncrementsBy(5); modelBuilder.Entity<Order>() .Property(o => o.OrderNo) .HasDefaultValueSql(\"nextval('\\\"OrderNumbers\\\"')\"); } HiLo Autoincrement Generation One disadvantage of database-generated values is that these values must be read back from the database after a row is inserted. If you're saving multiple related entities, this means you must perform multiple roundtrips as the first entity's generated key must be read before writing the second one. One solution to this problem is HiLo value generation: rather than relying on the database to generate each and every value, the application \"allocates\" a range of values, which it can then populate directly on new entities without any additional roundtrips. When the range is exhausted, a new range is allocated. In practical terms, this uses a sequence that increments by some large value (100 by default), allowing the application to insert 100 rows autonomously. To use HiLo, specify ForNpgsqlUseSequenceHiLo on a property in your model's OnModelCreating : protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>() .Property(b => b.Id) .ForNpgsqlUseSequenceHiLo(); You can also make your model use HiLo everywhere: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.ForNpgsqlUseSequenceHiLo(); Guid/UUID Generation By default, if you specify ValueGeneratedOnAdd on a Guid property, a random Guid value will be generated client-side and sent to the database. If you prefer to generate values in the database instead, you can do so by specifying HasDefaultValueSql on your property. Note that PostgreSQL doesn't include any Guid/UUID generation functions, you must add an extension such as uuid-ossp or pgcrypto . This can be done by placing the following code in your model's OnModelCreating : protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.HasPostgresExtension(\"uuid-ossp\") .Entity<Blog>() .Property(e => e.SomeGuidProperty) .HasDefaultValueSql(\"uuid_generate_v4()\"); See the PostgreSQL docs on UUID for more details . Computed Columns (On Add or Update) PostgreSQL does not support computed columns."
  },
  "doc/types/nts.html": {
    "href": "doc/types/nts.html",
    "title": "PostGIS/NetTopologySuite Type Plugin | Npgsql Documentation",
    "keywords": "PostGIS/NetTopologySuite Type Plugin Before 4.0, Npgsql has supported reading and writing PostGIS types via some bundled .NET classes: PostgisPoint , PostgisLineString , etc. While this model provided some basic support, a proper representation of spatial types is a complicated task that's beyond Npgsql's scope, and should be handled by a specialized spatial library instead. The leading spatial library in the .NET world is currently NetTopologySuite , and with the introduction of type plugins in Npgsql 4.0, it is now possible to map PostGIS types directly to NetTopologySuite types. This is now the recommended way to store and load PostGIS types. If you prefer to work with the pre-4.0 types, you can still do so by using the Npgsql.LegacyPostgis plugin . Adding the NetTopologySuite MyGet feed Unfortunately, the version of NetTopologySuite currently required by Npgsql (1.15.0) is still in preview. As a result, you will have to add the NetTopologySuite MyGet feed by dropping the following NuGet.Config file at the root of your project: <configuration> <packageSources> <add key=\"NuGet\" value=\"https://api.nuget.org/v3/index.json\" /> <add key=\"NetTopologySuite\" value=\"https://www.myget.org/F/nettopologysuite/api/v3/index.json\" /> </packageSources> </configuration> You will be able to remove this when NetTopologySuite make a final release. Setup To use the NetTopologySuite plugin, simply add a dependency on Npgsql.NetTopologySuite and set it up: using Npgsql; // Place this at the beginning of your program to use NetTopologySuite everywhere (recommended) NpgsqlConnection.GlobalTypeMapper.UseNetTopologySuite(); // Or to temporarily use NetTopologySuite on a single connection only conn.TypeMapper.UseNetTopologySuite(); By default the plugin handles only ordinates provided by the DefaultCoordinateSequenceFactory of GeometryServiceProvider.Instance . If GeometryServiceProvider is initialized automatically the X and Y ordinates are handled. To change the behavior specify the handleOrdinates parameter like in the following example: conn.TypeMapper.UseNetTopologySuite(handleOrdinates: Ordinates.XYZ); To process the M ordinate, you must initialize GeometryServiceProvider.Instance to a new NtsGeometryServices instance with coordinateSequenceFactory set to a DotSpatialAffineCoordinateSequenceFactory . Or you can specify the factory when calling UseNetTopologySuite . // Place this at the beginning of your program to use the specified settings everywhere (recommended) GeometryServiceProvider.Instance = new NtsGeometryServices( new DotSpatialAffineCoordinateSequenceFactory(Ordinates.XYM), new PrecisionModel(PrecisionModels.Floating), -1); // Or specify settings for Npgsql only conn.TypeMapper.UseNetTopologySuite( new DotSpatialAffineCoordinateSequenceFactory(Ordinates.XYM)); Reading and Writing Geometry Values When reading PostGIS values from the database, Npgsql will automatically return the appropriate NetTopologySuite types: Point , LineString , and so on. Npgsql will also automatically recognize NetTopologySuite's types in parameters, and will automatically send the corresponding PostGIS type to the database. The following code demonstrates a roundtrip of a NetTopologySuite Point to the database: var point = new Point(new Coordinate(1d, 1d)); conn.ExecuteNonQuery(\"CREATE TEMP TABLE data (geom GEOMETRY)\"); using (var cmd = new NpgsqlCommand(\"INSERT INTO data (geom) VALUES (@p)\", conn)) { cmd.Parameters.AddWithValue(\"@p\", point); cmd.ExecuteNonQuery(); } using (var cmd = new NpgsqlCommand(\"SELECT geom FROM data\", conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); Assert.That(reader[0], Is.EqualTo(point)); } You may also explicitly specify a parameter's type by setting NpgsqlDbType.Geometry . Geography (geodetic) Support PostGIS has two types: geometry (for Cartesian coordinates) and geography (for geodetic or spherical coordinates). You can read about the geometry/geography distinction in the PostGIS docs or in this blog post . In a nutshell, geography is much more accurate when doing calculations over long distances, but is more expensive computationally and supports only a small subset of the spatial operations supported by geometry . Npgsql uses the same NetTopologySuite types to represent both geometry and geography - the Point type represents a point in either Cartesian or geodetic space. You usually don't need to worry about this distinction because PostgreSQL will usually cast types back and forth as needed. However, it's worth noting that Npgsql sends Cartesian geometry by default, because that's the usual requirement. You have the option of telling Npgsql to send geography instead by specifying NpgsqlDbType.Geography : using (var cmd = new NpgsqlCommand(\"INSERT INTO data (geog) VALUES (@p)\", conn)) { cmd.Parameters.AddWithValue(\"@p\", NpgsqlDbType.Geography, point); cmd.ExecuteNonQuery(); } If you prefer to use geography everywhere by default, you can also specify that when setting up the plugin: NpgsqlConnection.GlobalTypeMapper.UseNetTopologySuite(geographyAsDefault: true);"
  },
  "doc/types/enums_and_composites.html": {
    "href": "doc/types/enums_and_composites.html",
    "title": "PostgreSQL enums and composites | Npgsql Documentation",
    "keywords": "PostgreSQL enums and composites PostgreSQL supports enum types and composite types as database columns, and Npgsql supports reading and writing these. This allows you to seamlessly read and write enum and composite values to the database without worrying about conversions. Mapping your CLR types The recommended way to work with enums and composites is to set up a mapping for your CLR types. Doing so provides the following advantages: To set up a global mapping for all your connections, put this code before your first open: NpgsqlConnection.GlobalTypeMapper.MapEnum<SomeEnum>(\"some_enum\"); NpgsqlConnection.GlobalTypeMapper.MapComposite<SomeType>(\"some_composite\"); This sets up a mapping between your CLR types SomeEnum and SomeType to the PostgreSQL types some_enum and some_composite . If you don't want to set up a mapping for all your connections, you can set it up one connection only: var conn = new NpgsqlConnection(...); conn.TypeMapper.MapEnum<SomeEnum>(\"some_enum\"); conn.TypeMapper.MapComposite<SomeType>(\"some_composite\"); After mapping, you can read and write your CLR types as usual: // Writing using (var cmd = new NpgsqlCommand(\"INSERT INTO some_table (some_enum, some_composite) VALUES (@p1, @p2)\", Conn)) { cmd.Parameters.Add(new NpgsqlParameter { ParameterName = \"some_enum\", Value = SomeEnum.Good }); cmd.Parameters.Add(new NpgsqlParameter { ParameterName = \"some_composite\", Value = new SomeType { ... } }); cmd.ExecuteNonQuery(); } // Reading using (var cmd = new NpgsqlCommand(\"SELECT some_enum, some_composite FROM some_table\", Conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); var enumValue = reader.GetFieldValue<SomeEnum>(0); var compositeValue = reader.GetFieldValue<SomeType>(1); } Note that your PostgreSQL enum and composites types ( pg_enum_type and pg_composite_type in the sample above) must be defined in your database before the first connection is created (see CREATE TYPE ). If you're creating PostgreSQL types within your program, call NpgsqlConnection.ReloadTypes() to make sure Npgsql becomes properly aware of them. Name Translation CLR type and field names are usually camelcase (e.g. SomeType ), whereas in PostgreSQL they are snake_case (e.g. some_type ). To help make the mapping for enums and composites seamless, pluggable name translators are used translate all names. The default translation scheme is NpgsqlSnakeCaseNameTranslator , which maps names like SomeType to some_type , but you can specify others. The default name translator can be set for all your connections via NpgsqlConnection.GlobalTypeMapper.DefaultNameTranslator , or for a specific connection for NpgsqlConnection.TypeMapper.DefaultNameTranslator . You also have the option of specifyin a name translator when setting up a mapping: NpgsqlConnection.GlobalTypeMapper.MapComposite<SomeType>(\"some_type\", new NpgsqlNullNameTranslator()); Finally, you may control mappings on a field-by-field basis via the [PgName] attribute. This will override the name translator. using NpgsqlTypes; enum SomeEnum { [PgName(\"happy\")] Good, [PgName(\"sad\")] Bad } Reading and Writing Dynamically (without CLR types) In some cases, it may be desirable to interact with PostgreSQL enums and composites without a pre-existing CLR type - this is useful mainly if your program doesn't know the database schema and types in advance, and needs to interact with any enum/composite type. Note that using CLR types is safer and faster (for composites), and should be preferred when possible. Enums can be read and written as simple strings: // Writing enum as string using (var cmd = new NpgsqlCommand(\"INSERT INTO some_table (some_enum) VALUES (@p1)\", Conn)) { cmd.Parameters.Add(new NpgsqlParameter { ParameterName = \"some_enum\", Value = \"Good\" DataTypeName = \"pg_enum_type\" }); cmd.ExecuteNonQuery(); } // Reading enum as string using (var cmd = new NpgsqlCommand(\"SELECT some_enum FROM some_table\", Conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); var enumValue = reader.GetFieldValue<string>(0); } Composites can be read and written as C# dynamic ExpandoObjects: // Writing composite as ExpandoObject using (var cmd = new NpgsqlCommand(\"INSERT INTO some_table (some_composite) VALUES (@p1)\", Conn)) { var someComposite = new ExpandoObject(); some_composite.Foo = 8; some_composite.Bar = \"hello\"; cmd.Parameters.Add(new NpgsqlParameter { ParameterName = \"some_enum\", Value = someComposite, DataTypeName = \"pg_enum_type\" }); cmd.ExecuteNonQuery(); } // Reading composite as ExpandoObject using (var cmd = new NpgsqlCommand(\"SELECT some_composite FROM some_table\", Conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); var compositeValue = (dynamic)reader.GetValue(0); Console.WriteLine(compositeValue.Foo); Console.WriteLine(compositeValue.Bar); } As long as you CLR types SomeEnum and SomeType contain fields/properties which correspond to the PostgreSQL type being read/written, everything will work as expected. Note that the default name translator is used (see the section about name translation)."
  },
  "doc/faq.html": {
    "href": "doc/faq.html",
    "title": "FAQ | Npgsql Documentation",
    "keywords": "FAQ How can I call a PostgreSQL 11 stored procedure? I tried doing so with CommandType.StoredProcedure and got an error... PostgreSQL 11 stored procedures can be called, but unfortunately not with CommandType.StoredProcedure . PostgreSQL has supported stored functions for a long while, and since these have acted as replacements for non-existing procedures, Npgsql's CommandType.StoredProcedure has been implemented to invoke them; this means that CommandType.StoredProcedure translates into SELECT * FROM my_stored_function() . The new stored procedures introduce a special invocation syntax - CALL my_stored_procedure() - which is incompatible with the existing stored function syntax. On the brighter side, it's very easy to invoke stored procedures (or functions) yourself - you don't really need CommandType.StoredProcedure . Simply create a regular command and set CommandText to CALL my_stored_procedure(@p1, @p2) , handling parameters like you would any other statement. In fact, with Npgsql and PostgreSQL, CommandType.StoredProcedure doesn't really have any added value over constructing the command yourself. I get an exception \"The field field1 has a type currently unknown to Npgsql (OID XXXXX). You can retrieve it as a string by marking it as unknown\". Npgsql has to implement support for each PostgreSQL type, and it seems you've stumbled upon an unsupported type. First, head over to our issues page and check if an issue already exists on your type, otherwise please open one to let us know. Then, as a workaround, you can have your type treated as text - it will be up to you to parse it in your program. One simple way to do this is to append ::TEXT in your query (e.g. SELECT 3::TEXT ). If you don't want to modify your query, Npgsql also includes an API for requesting types as text. The fetch all the columns in the resultset as text, using (var cmd = new NpgsqlCommand(...)) { cmd.AllResultTypesAreUnknown = true; var reader = cmd.ExecuteReader(); // Read everything as strings } You can also specify text only for some columns in your resultset: using (var cmd = new NpgsqlCommand(...)) { // Only the second field will be fetched as text cmd.UnknownResultTypeList = new[] { false, true }; var reader = cmd.ExecuteReader(); // Read everything as strings } I'm trying to write a JSONB type and am getting 'column \"XXX\" is of type jsonb but expression is of type text' When sending a JSONB parameter, you must explicitly specify its type to be JSONB with NpgsqlDbType: using (var cmd = new NpgsqlCommand(\"INSERT INTO foo (col) VALUES (@p)\", conn)) { cmd.Parameters.AddWithValue(\"p\", NpgsqlDbType.Jsonb, jsonText); } I'm trying to apply an Entity Framework 6 migration and I get Type is not resolved for member 'Npgsql.NpgsqlException,Npgsql' ! Unfortunately, a shortcoming of EF6 requires you to have Npgsql.dll in the Global Assembly Cache (GAC), otherwise you can't see migration-triggered exceptions. You can add Npgsql.dll to the GAC by opening a VS Developer Command Prompt as administator and running the command gacutil /i Npgsql.dll . You can remove it from the GAC with gacutil /u Npgsql ."
  },
  "doc/parameters.html": {
    "href": "doc/parameters.html",
    "title": "Parameters | Npgsql Documentation",
    "keywords": "Parameters Avoiding boxing with strongly-typed NpgsqlParameter In standard ADO.NET, data values are sent to the database via a subclass of DbParameter (NpgsqlParameter in Npgsql's case). This API requires you to set the data via DbParameter.Value which, being an object , will box value types such as int. If you're sending lots of value types to the database, this will create large amounts of useless heap allocations and strain the garbage collector. Npgsql 3.3 introduces a solution to this problem: NpgsqlParameter<T> . This generic class has a TypedValue member, which is similar to NpgsqlParameter.Value but which is strongly-typed, thus avoiding the boxing and heap allocation. Note that this strongly-typed parameter API is entirely Npgsql-specific, and will make your code non-portable to other database. See #8955 for an issue discussing this at the ADO.NET level."
  },
  "doc/keepalive.html": {
    "href": "doc/keepalive.html",
    "title": "Keepalive | Npgsql Documentation",
    "keywords": "Keepalive Some clients keep idle connections for long periods of time - this is especially frequent when waiting for PostgreSQL notifications. In this scenario, how can the client know the connection is still up, and hasn't been broken by a server or network outage? For this purpose, Npgsql has a keepalive feature, which makes it send periodic SELECT NULL queries. This feature is by default disabled, and must be enabled via the Keepalive connection string parameter, setting the number of seconds between each keepalive. When keepalive is enabled, Npgsql will emit an NpgsqlConnection.StateChange event if the keepalive fails. Note that you should only turn this feature on if you need it. Unless you know you'll have long-lived idle connections, and that your backend (or network equipment) will interfere with these connections, it's better to leave this off. TCP Keepalives The keepalive mechanism above is ideal for long-standing idle connections, but it cannot be used during query processing. With some PostgreSQL-like data warehouse products such as Amazon Redshift or Greenplum , it is not uncommon for a single SQL statement to take a long time to execute, and during that time it is not possible to send SELECT NULL . For these cases you may want to turn on TCP keepalive , which is quite different from the application-level keepalive described above. To better understand the different kinds of keepalives, see this blog post . As that article explains, TCP keepalive depends on networking stack support and might not always work, but it is your only option during query processing. On Linux, you turn keepalives simply by specifying Tcp Keepalive=true in your connection string. The default system-wide settings will be used (for interval, count...) - it is currently impossible to specify these at the application level. On Windows, you can also specify Tcp Keepalive Time and Tcp Keepalive Interval to tweak these settings."
  },
  "doc/security.html": {
    "href": "doc/security.html",
    "title": "Security and Encryption | Npgsql Documentation",
    "keywords": "Security and Encryption Logging in The simplest way to log into PostgreSQL is by specifying a Username and a Password in your connection string. Depending on how your PostgreSQL is configured (in the pg_hba.conf file), Npgsql will send the password in MD5 or in cleartext (not recommended). If a Password is not specified and your PostgreSQL is configured to request a password (plain or MD5), Npgsql will look for a standard PostgreSQL password file . If you specify the Passfile connection string parameter, the file it specifies will be used. If that parameter isn't defined, Npgsql will look under the path taken from PGPASSFILE environment variable. If the environment variable isn't defined, Npgsql will fall back to the system-dependent default directory which is $HOME/.pgpass for Unix and %APPDATA%\\postgresql\\pgpass.conf for Windows. For documentation about all auth methods supported by PostgreSQL, see this page . Note that Npgsql supports Unix-domain sockets (auth method local ), simply set your Host parameter to the absolute path of your PostgreSQL socket directory, as configred in your postgresql.conf . Integrated Security (GSS/SSPI/Kerberos) Logging in with a username and password isn't recommended, since your application must have access to your password. An alternate way of authenticating is \"Integrated Security\", which uses GSS or SSPI to negotiate Kerberos. The advantage of this method is that authentication is handed off to your operating system, using your already-open login session. Your application never needs to handle a password. You can use this method for a Kerberos login, Windows Active Directory or a local Windows session. Note that since 3.2, this method of authentication also works on non-Windows platforms. Instructions on setting up Kerberos and SSPI are available in the PostgreSQL auth methods docs . Some more instructions for SSPI are available here . Once your PostgreSQL is configured correctly, simply include Integrated Security=true in your connection string and drop the Password parameter. However, Npgsql must still send a username to PostgreSQL. If you specify a Username connection string parameter, Npgsql will send that as usual. If you omit it, Npgsql will attempt to detect your system username, including the Kerberos realm. Note that by default, PostgreSQL expects your Kerberos realm to be sent in your username (e.g. username@REALM ); you can have Npgsql detect the realm by setting Include Realm to true in your connection string. Alternatively, you can disable add include_realm=0 in your PostgreSQL's pg_hba.conf entry, which will make it strip the realm. You always have the possibility of explicitly specifying the username sent to PostgreSQL yourself. Encryption (SSL/TLS) By default PostgreSQL connections are unencrypted, but you can turn on SSL/TLS encryption if you wish. First, you have to set up your PostgreSQL to receive SSL/TLS connections as described here . Once that's done, specify SSL Mode in your connection string, setting it to either Require (connection will fail if the server isn't set up for encryption), or Prefer (use encryption if possible but fallback to unencrypted otherwise). Note that by default, Npgsql will verify that your server's certificate is valid. If you're using a self-signed certificate this will fail. You can instruct Npgsql to ignore this by specifying Trust Server Certificate=true in the connection string. To precisely control how the server's certificate is validated, you can register UserCertificateValidationCallback on NpgsqlConnection (this works just like on .NET's SSLStream ). You can also have Npgsql provide client certificates to the server by registering the ProvideClientCertificatesCallback on NpgsqlConnection (this works just like on .NET's SSLStream )."
  },
  "doc/connection-string-parameters.html": {
    "href": "doc/connection-string-parameters.html",
    "title": "Connection String Parameters | Npgsql Documentation",
    "keywords": "Connection String Parameters To connect to a database, the application provides a connection string which specifies parameters such as the host, the username, the password, etc. Connection strings have the form keyword1=value; keyword2=value; and are case-insensitive. Values containing special characters (e.g. semicolons) can be double-quoted. For more information, see the official doc page on connection strings . Below are the connection string parameters which Npgsql understands. Basic Connection Parameter Description Default Host Specifies the host name of the machine on which the server is running. If the value begins with a slash, it is used as the directory for the Unix-domain socket (specifying a Port is still required). Required Port The TCP port of the PostgreSQL server. 5432 Database The PostgreSQL database to connect to. Same as Username Username The username to connect with. Not required if using IntegratedSecurity. Password The password to connect with. Not required if using IntegratedSecurity. Passfile Path to a PostgreSQL password file (PGPASSFILE), from which the password is taken. Security and Encryption Parameter Description Default SSL Mode Controls whether SSL is used, depending on server support. Can be Require , Disable , or Prefer . See docs for more info . Disable Trust Server Certificate Whether to trust the server certificate without validating it. See docs for more info . false Use SSL Stream Npgsql uses its own internal implementation of TLS/SSL. Turn this on to use .NET SslStream instead. false Check Certificate Revocation Whether to check the certificate revocation list during authentication. False by default. false Integrated Security Whether to use integrated security to log in (GSS/SSPI), currently supported on Windows only. See docs for more info . false Persist Security Info Gets or sets a Boolean value that indicates if security-sensitive information, such as the password, is not returned as part of the connection if the connection is open or has ever been in an open state. Since 3.1 only. false Kerberos Service Name The Kerberos service name to be used for authentication. See docs for more info . postgres Include Realm The Kerberos realm to be used for authentication. See docs for more info . Pooling Parameter Description Default Pooling Whether connection pooling should be used. true Minimum Pool Size The minimum connection pool size. 1 Maximum Pool Size The maximum connection pool size. 100 since 3.1, 20 previously Connection Idle Lifetime The time (in seconds) to wait before closing idle connections in the pool if the count of all connections exceeds MinPoolSize. Since 3.1 only. 300 Connection Pruning Interval How many seconds the pool waits before attempting to prune idle connections that are beyond idle lifetime (see ConnectionIdleLifetime ). Since 3.1 only. 10 Timeouts and Keepalive Parameter Description Default Timeout The time to wait (in seconds) while trying to establish a connection before terminating the attempt and generating an error. 15 Command Timeout The time to wait (in seconds) while trying to execute a command before terminating the attempt and generating an error. Set to zero for infinity. 30 Internal Command Timeout The time to wait (in seconds) while trying to execute a an internal command before terminating the attempt and generating an error. -1 uses CommandTimeout, 0 means no timeout. -1 Keepalive The number of seconds of connection inactivity before Npgsql sends a keepalive query. disabled Tcp Keepalive Whether to use TCP keepalive with system defaults if overrides isn't specified. disabled Tcp Keepalive Time The number of milliseconds of connection inactivity before a TCP keepalive query is sent. Use of this option is discouraged, use KeepAlive instead if possible. Supported only on Windows. disabled Tcp Keepalive Interval The interval, in milliseconds, between when successive keep-alive packets are sent if no acknowledgement is received. TcpKeepAliveTime must be non-zero as well. Supported only on Windows. value of TcpKeepAliveTime Performance Parameter Description Default Max Auto Prepare The maximum number SQL statements that can be automatically prepared at any given point. Beyond this number the least-recently-used statement will be recycled. Zero disables automatic preparation. 0 Auto Prepare Min Usages The minimum number of usages an SQL statement is used before it's automatically prepared. 5 Use Perf Counters Makes Npgsql write performance information about connection use to Windows Performance Counters. Read the docs for more info. Read Buffer Size Determines the size of the internal buffer Npgsql uses when reading. Increasing may improve performance if transferring large values from the database. 8192 Write Buffer Size Determines the size of the internal buffer Npgsql uses when writing. Increasing may improve performance if transferring large values to the database. 8192 Socket Receive Buffer Size Determines the size of socket receive buffer. System-dependent Socket Send Buffer Size Determines the size of socket send buffer. System-dependent No Reset On Close Improves performance in some cases by not resetting the connection state when it is returned to the pool, at the cost of leaking state. Use only if benchmarking shows a performance improvement false Misc Parameter Description Default Application Name The optional application name parameter to be sent to the backend during connection initiation. Enlist Whether to enlist in an ambient TransactionScope. true Search Path Sets the schema search path. Client Encoding Gets or sets the client_encoding parameter. Since 3.1. Timezone Gets or sets the session timezone, PGTZ environment variable can be used instead. Since 3.3. EF Template Database The database template to specify when creating a database in Entity Framework. template1 Load Table Composites Load table composite type definitions, and not just free-standing composite types. false Compatibility Parameter Description Default Server Compatibility Mode A compatibility mode for special PostgreSQL server types. Currently \"Redshift\" is supported, as well as \"NoTypeLoading\", which will bypass the normal type loading mechanism from the PostgreSQL catalog tables and supports a hardcoded list of basic types . none Convert Infinity DateTime Makes MaxValue and MinValue timestamps and dates readable as infinity and negative infinity. false"
  },
  "efcore/modeling/indexes.html": {
    "href": "efcore/modeling/indexes.html",
    "title": "Indexes | Npgsql Documentation",
    "keywords": "Indexes PostgreSQL and the Npgsql provider support the standard index modeling described in the EF Core docs . This page describes some supported PostgreSQL-specific features. PostgreSQL covering indexes (INCLUDE) Since version 11, PostgreSQL supports covering indexes , which allow you to include \"non-key\" columns in your indexes. This allows you to perform index-only scans and can provide a significant performance boost: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>() .ForNpgsqlHasIndex(b => b.Id) .ForNpgsqlInclude(b => b.Name); This will create an index for searching on Id , but containing also the column Name , so that reading the latter will not involve accessing the table. The SQL generated is as follows: CREATE INDEX \"IX_Blog_Id\" ON blogs (\"Id\") INCLUDE (\"Name\"); PostgreSQL index methods PostgreSQL supports a number of index methods , or types . These are specified at index creation time via the USING _method_ clause, see the PostgreSQL docs for CREATE INDEX and this page for information on the different types. The Npgsql EF Core provider allows you to specify the index method to be used by calling ForNpgsqlHasMethod() on your index in your context's OnModelCreating method: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>() .HasIndex(b => b.Url) .ForNpgsqlHasMethod(\"gin\"); PostgreSQL index operator classes PostgreSQL allows you to specify operator classes on your indexes , to allow tweaking how the index should work. Use the following code to specify an operator class: protected override void OnConfiguring(DbContextOptionsBuilder builder) => modelBuilder.Entity<Blog>() .ForNpgsqlHasIndex(b => new { b.Id, b.Name }) .ForNpgsqlHasOperators(null, \"text_pattern_ops\"); Note that each operator class is used for the corresponding index column, by order. In the example above, the text_pattern_ops class will be used for the Name column, while the Id column will use the default class (unspecified), producing the following SQL: CREATE INDEX \"IX_blogs_Id_Name\" ON blogs (\"Id\", \"Name\" text_pattern_ops);"
  },
  "doc/transactions.html": {
    "href": "doc/transactions.html",
    "title": "Transactions | Npgsql Documentation",
    "keywords": "Transactions Basic Transactions Transactions can be started by calling the standard ADO.NET method NpgsqlConnection.BeginTransaction() . PostgreSQL doesn't support nested or concurrent transactions - only one transaction may be in progress at any given moment. Calling BeginTransaction() while a transaction is already in progress will throw an exception. Because of this, it isn't necessary to pass the NpgsqlTransaction object returned from BeginTransaction() to commands you execute - calling BeginTransaction() means that all subsequent commands will automatically participate in the transaction, until either a commit or rollback is performed. However, for maximum portability it's recommended to set the transaction on your commands. Although concurrent transactions aren't supported, PostgreSQL supports the concept of savepoints - you may set named savepoints in a transaction and roll back to them later without rolling back the entire transaction. Savepoints can be created, rolled back to, and released via NpgsqlTransaction.Save(name) , NpgsqlTransaction.Rollback(name) and NpgsqlTransaction.Release(name) respectively. See the PostgreSQL documentation for more details. . When calling BeginTransaction() , you may optionally set the isolation level . See the docs for more details. System.Transactions and Distributed Transactions In addition to DbConnection.BeginTransaction() , .NET includes System.Transactions, an alternative API for managing transactions - read the MSDN docs to understand the concepts involved . Npgsql fully supports this API, and starting with version 3.3 will automatically enlist to ambient TransactionScopes (you can disable enlistment by specifying Enlist=false in your connection string). When more than one connection (or resource) enlists in the same transaction, the transaction is said to be distributed . Distributed transactions allow you to perform changes atomically across more than one database (or resource) via a two-phase commit protocol - here is the MSDN documentation . Npgsql supports distributed transactions - support has been rewritten for version 3.2, fixing many previous issues. However, at this time Npgsql enlists as a volatile resource manager , meaning that if your application crashes while performing, recovery will not be managed properly. For more information about this, see this page and the related ones . If you would like to see better distributed transaction recovery (i.e. durable resource manager enlistment), please say so on this issue and subscribe to it for updates. Note that if you open and close connections to the same database inside an ambient transaction, without ever having two connections open at the same time , Npgsql will internally reuse the same connection, avoiding the escalation to a full-blown distributed transaction. This is better for performance and for general simplicity."
  },
  "ef6/index.html": {
    "href": "ef6/index.html",
    "title": "Entity Framework 6 | Npgsql Documentation",
    "keywords": "Npgsql has an Entity Framework 6 provider. You can use it by installing the EntityFramework6.Npgsql nuget. Basic Configuration To use Entity Framework with Npgsql, define a class that inherits from DbConfiguration in the same assembly as your class inheriting DbContext . Ensure that you configure provider services, a provider factory, a default connection factory as shown below: using Npgsql; using System.Data.Entity; class NpgSqlConfiguration : DbConfiguration { public NpgSqlConfiguration() { var name = \"Npgsql\"; SetProviderFactory(providerInvariantName: name, providerFactory: NpgsqlFactory.Instance); SetProviderServices(providerInvariantName: name, provider: NpgsqlServices.Instance); SetDefaultConnectionFactory(connectionFactory: new NpgsqlConnectionFactory()); } } Guid Support Npgsql EF migrations support uses uuid_generate_v4() function to generate guids. In order to have access to this function, you have to install the extension uuid-ossp through the following command: create extension \"uuid-ossp\"; If you don't have this extension installed, when you run Npgsql migrations you will get the following error message: ERROR: function uuid_generate_v4() does not exist If the database is being created by Npgsql Migrations, you will need to run the create extension command in the template1 database . This way, when the new database is created, the extension will be installed already. Template Database When the Entity Framework 6 provider creates a database, it issues a simple CREATE DATABASE command. In PostgreSQL, this implicitly uses template1 as the template - anything existing in template1 will be copied to your new database. If you wish to change the database used as a template, you can specify the EF Template Database connection string parameter. For more info see the PostgreSQL docs . Customizing DataReader Behavior You can use an Entity Framework 6 IDbCommandInterceptor to wrap the DataReader instance returned by Npgsql when Entity Framework executes queries. This is possible using a DbConfiguration class. Example use cases: Forcing all returned DateTime and DateTimeOffset values to be in the UTC timezone. Preventing accidental insertion of DateTime values having DateTimeKind.Unspecified . Forcing all postgres date/time types to be returned to Entity Framework as DateTimeOffset . [DbConfigurationType(typeof(AppDbContextConfiguration))] public class AppDbContext : DbContext { // ... } public class AppDbContextConfiguration : DbConfiguration { public AppDbContextConfiguration() { this.AddInterceptor(new MyEntityFrameworkInterceptor()); } } class MyEntityFrameworkInterceptor : DbCommandInterceptor { public override void ReaderExecuted( DbCommand command, DbCommandInterceptionContext<DbDataReader> interceptionContext) { if (interceptionContext.Result == null) return; interceptionContext.Result = new WrappingDbDataReader(interceptionContext.Result); } public override void ScalarExecuted( DbCommand command, DbCommandInterceptionContext<object> interceptionContext) { interceptionContext.Result = ModifyReturnValues(interceptionContext.Result); } static object ModifyReturnValues(object result) { // Transform and then return result; } } class WrappingDbDataReader : DbDataReader, IDataReader { // Wrap an existing DbDataReader, proxy all calls to the underlying instance, // modify return values and/or parameters as needed... public WrappingDbDataReader(DbDataReader reader) { } }"
  },
  "efcore/release-notes/2.1.html": {
    "href": "efcore/release-notes/2.1.html",
    "title": "2.1 Release Notes | Npgsql Documentation",
    "keywords": "2.1 Release Notes Version 2.1.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget. This version works with version 2.1.0 of Entity Framework Core , and contains some new Npgsql features as well. Thanks to @rwasef1830 and @austindrenski for their valuable contributions. New Features Aside from general EF Core features new in 2.1.0, the Npgsql provider contains the following major new features: Improved Spatial Support (PostGIS) Previous versions have allowed basic usage of PostGIS's spatial types via built-in Npgsql types such as NpgsqlPoint , NpgsqlLineString , etc. These types were limited in many ways, and no operation translation was supported. If you want to calculate, say, the distance between two points, you had to drop down to raw SQL. No more! Thanks to a new plugin infrastructure, the EF Core provider now has full-class support for PostGIS. You can now use the NetTopologySuite spatial library to map PostGIS types - NetTopologySuite's types are more complete, and best of all, the provider knows how to translate its operations to SQL. This allows you to write the following code: var nearbyCities = context.Cities.Where(c => c.Location.Distance(somePoint) < 100); See the full documentation for more information . Thanks to @YohDeadfall for implementing support for this at the ADO level. Full text search PostgreSQL has a powerful feature for efficient natural-language search across multiple columns and tables, see the PostgreSQL docs for more info . The EF Core provider now supports full-text search, allowing you to use .NET functions in your LINQ queries which will get translated to efficient PostgreSQL natural-language search queries. Read the full documentation for more information . Many thanks to @rwasef1830 for contributing this feature. NodaTime date/time support NodaTime is a powerful alternative to .NET's built-in date/time types, such as DateTime . The built-in types are flawed in many ways: they have problematic support for timezones, don't have a date-only or time-only types, and promote problematic programming but not making the right distinctions. If your application handles dates and times in anything but the most basic way, you should seriously consider using NodaTime. To learn more read this blog post by Jon Skeet . Thanks to a new plugin infrastructure, it is now possible to set up the EF Core provider to use NodaTime's types instead of the built-in .NET ones; instead of having DateTime properties on your entities, you can now have Instant properties instead. See the full documentation for more information . PostgreSQL 10 IDENTITY columns PostgreSQL 10 introduced a new IDENTITY column as an alternative to traditional SERIAL columns, and these are now supported by the EF Core provider. IDENTITY columns conform to the SQL standard and are in general safer than SERIAL columns, read this blog post for more info . It's recommended that all new projects use IDENTITY, but Npgsql even provides seamless migration of your existing SERIAL-based model!. Read the full documentation for more information . Enum support It is now possible to map your CLR enums to native PostgreSQL enums . This is a unique PostgreSQL feature that provides the best of both worlds: the enum is internally stored in the database as a number (minimal storage), but is handled like a string (more usable, no need to remember numeric values) and has type safety. See the full documentation for more information . Range support PostgreSQL supports native range types , which allow you to represent ranges of dates, ints and other data types in a single column. You can then efficiently perform queries on these types from LINQ, e.g. select all rows where a given date falls in the row's date range. See the full documentation for more information . Many thanks to @austindrenski for contributing the operation translations. Other notable features Several improvements have been made to the way arrays are mapped. For example, you can now map List<T> to PostgreSQL array (previously only T[] was supported) ( #392 ). In addition, change tracking now works for arrays, so EF Core will automatically detect when you change an element's array and will update the corresponding database column when saving. PostgreSQL's built-in range types can now be mapped ( #63 ), head over to the PostgreSQL docs to find out more about range types. Note that user-defined range types are not supported, if you're interested let us know at #329 . Properties of type char are now supported and will be mapped to character(1) in PostgreSQL ( #374 ). Identifiers in generated SQL will only be quoted if needed ( #327 ). This should make it much easier to read. You can now use client certificate authentication and provide a server certificate validation callback ( #270 ). See the doc for usage instructions . Added support for PostgreSQL 10 sequences with type int and smallint ( #301 ). You can now specify the tablespace when creating your databases ( #332 ). Here's the full list of issues . Please report any problems on https://github.com/npgsql/Npgsql.EntityFrameworkCore.PostgreSQL . Breaking changes Caution The provider's classes have been moved from the namespace Microsoft.EntityFrameworkCore to Npgsql.EntityFrameworkCore.PostgreSQL . As a result, any migrations already generated in your project (as well as the mode snapshot) will have to be manually updated to use the new namespace. You will need to add using Npgsql.EntityFrameworkCore.PostgreSQL.Metadata to all the relevant files. Columns of type timestamp with time zone / timestamptz will now be scaffolded as DateTime properties, and not DateTimeOffset properties. The general use of timestamp with time zone / timestamptz is discouraged (this type does not store the timezone in the database), consider using timestamp without time zone / timestamp instead. If you're specifying index methods with ForNpgsqlHasMethod() , then you will have to fix migrations which generate those indexes. In these migrations, you will find code such as .Annotation(\"Npgsql:Npgsql:IndexMethod\", \"gin\") . You must remove the extra Npgsql: , leaving .Annotation(\"Npgsql:IndexMethod\", \"gin\") . Specifying versions when specifying PostgreSQL extensions on your model is no longer supported - this was a very rarely-used feature which interfered with extension scaffolding. If you're still referencing the nuget package Npgsql.EntityFrameworkCore.PostgreSQL.Design, please remove it - it's no longer needed or up to date."
  },
  "doc/ddex.html": {
    "href": "doc/ddex.html",
    "title": "Visual Studio Integration | Npgsql Documentation",
    "keywords": "Visual Studio Integration Npgsql has a Visual Studio extension (VSIX) which integrates PostgreSQL access into Visual Studio. It allows connecting to PostgreSQL from within Visual Studio's Server Explorer, create an Entity Framework 6 model from an existing database, etc. The extension can be installed directly from the Visual Studio Marketplace page . The VSIX doesn't automatically add Npgsql to your GAC, App.config , machines.config or any other project or system-wide resource. It only allows accessing PostgreSQL from Visual Studio itself. Visual Studio Compatibility The VSIX extension has been tested and works on Visual Studio 2015 and 2017. It is probably compatible with versions all the way back to 2012, but these haven't been tested. Note that installing into pre-2015 versions will display a warning, although it should be safe to proceed. Upgrading from an older version Note that the extension has been pretty much rewritten for Npgsql 3.2 - if you encountered installation issues with previous versions, these issues should hopefully be gone. A summary of work done for 3.2 is available here . If you already have an earlier version of the VSIX (or MSI) installed, it's highly recommended that you uninstall them to avoid conflicts. It is no longer necessary or recommended to have Npgsql in your GAC, or to have Npgsql listed in your machines.config. Simply installing the VSIX should work just fine, and a GAC/machines.config may actually cause issues. If you previously installed Npgsql into your GAC/machines.config, it's recommended you uninstall it. If you have any entries (binding redirects, DbProviderFactory registrations) in either your machines.config or in your Visual Studio setup (e.g. App.config, devenv.exe.config ), please remove them The VSIX should work on a totally clean setup. Features The provider isn't feature comlete - please let us know of missing features or bugs by opening issues. Server Explorer You can add a PostgreSQL database in Server Explorer, explore tables and columns, send ad-hoc queries, etc. Entity Framework 6 The extension supports generating a model from an existing database. To do so, install EntityFramework6.Npgsql into your project, and then make sure you have the same version of Npgsql as your extension does. A mismatch between the version installed in your project and the VSIX's may cause issues. Development Development on the VSIX is currently possible only on Visual Studio 2017. Be sure to install the \"Visual Studio extension development\" workload."
  },
  "dev/index.html": {
    "href": "dev/index.html",
    "title": "Tests | Npgsql Documentation",
    "keywords": "Tests We maintain a large regression test suite, if you're planning to submit code, please provide a test that reproduces the bug or tests your new feature. See this page for information on the Npgsql test suite. Build Server We have a TeamCity build server running continuous integration builds on commits pushed to our github repository. The Npgsql testsuite is executed over all officially supported PostgreSQL versions to catch errors as early as possible. CI NuGet packages are automatically pushed to our unstable feed at MyGet . For some information about the build server setup, see this page . Thanks to Dave Page at PostgreSQL for donating a VM for this! Release Checklist These are the steps needed to publish release 3.0.6: Merge --no-ff hotfix/3.0.6 into master Tag master with v3.0.6 Push both master and v3.0.6 to Github Wait for the build to complete In TeamCity, go to the artifacts for the build and download them all as a single ZIP Nuget push the packages Write release notes on npgsql.org, publish Create release on github, pointing to npgsql.org Upload MSI to the github release Delete hotfix/3.0.6 both locally and on github Create new branch hotfix/3.0.7 off of master, push to github Close the Github 3.0.6 milestone, create new 3.0.7 milestone Twitter Other stuff Emil compiled a list of PostgreSQL types and their wire representations ."
  },
  "doc/contributing.html": {
    "href": "doc/contributing.html",
    "title": "Contributing to Npgsql | Npgsql Documentation",
    "keywords": "Contributing to Npgsql As a general rule, Npgsql makes no attempt to validate what it sends to PostgreSQL. For all cases where PostgreSQL would simply return a reasonable error, we prefer that to happen rather than checking replicating validation checks client-side."
  },
  "dev/types.html": {
    "href": "dev/types.html",
    "title": "PostgreSQL Types | Npgsql Documentation",
    "keywords": "Overview The following are notes by Emil Lenngren on PostgreSQL wire representation of types: bool: text: t or f binary: a byte: 1 or 0 bytea: text: either \\x followed by hex-characters (lowercase by default), or plain characters, where non-printable characters (between 0x20 and 0x7e, inclusive) are written as \\nnn (octal) and \\ is written as \\\\ binary: the bytes as they are char: This type holds a single char/byte. (Not to be confused with bpchar (blank-padded char) which is PostgreSQL's alias to the SQL standard's char). The char may be the null-character text: the char as a byte, encoding seems to be ignored binary: the char as a byte name: A null-padded string of NAMEDATALEN (currently 64) bytes (the last byte must be a null-character). Used in pg catalog. text: the name as a string binary: the name as a string int2/int4/int8: text: text representation in base 10 binary: binary version of the integer int2vector: non-null elements, 0-indexed, 1-dim text: 1 2 3 4 binary: same as int2[] oidvector: non-null elements, 0-indexed, 1-dim text: 1 2 3 4 binary: same as oid[] regproc: internally just an OID (UInt32) text: -, name of procedure, or numeric if not found binary: only the OID in binary regprocedure/regoper/regoperator/regclass/regconfig/regdictionary: similar to regproc text: text: the string as it is binary: the string as it is oid: A 32-bit unsigned integer used for internal object identification. text: the text-representation of this integer in base 10 binary: the UInt32 tid: tuple id Internally a tuple of a BlockNumber (UInt32) and an OffsetNumber (UInt16) text: (blockNumber,offsetNumber) binary: the block number in binary followed by offset number in binary xid: transaction id Internally just a TransactionId (UInt32) text: the number binary: the number in binary cid: command id Internally just a CommandId (UInt32) text: the number binary: the number in binary json: json text: the json an text binary: the json as text jsonb: json internally stored in an efficient binary format text: the json as text binary: An Int32 (version number, currently 1), followed by data (currently just json as text) xml: Xml. It is probably most efficient to use the text format, especially when receiving from client. text: the xml as text (when sent from the server: encoding removed, when receiving: assuming database encoding) binary: the xml as text (when sent from the server: in the client's specified encoding, when receiving: figures out itself) pg_node_tree: used as type for the column typdefaultbin in pg_type does not accept input text: text binary: text smgr: storage manager can only have the value \"magnetic disk\" text: magnetic disk binary: not available point: A tuple of two float8 text: (x,y) The floats are interpreted with the C strtod function. The floats are written with the snprintf function, with %.*g format. NaN/-Inf/+Inf can be written, but not interpretability depends on platform. The extra_float_digits setting is honored. For linux, NaN, [+-]Infinity, [+-]Inf works, but not on Windows. Windows also have other output syntax for these special numbers. (1.#QNAN for example) binary: the two floats lseg: A tuple of two points text: [(x1,y1),(x2,y2)] see point for details binary: the four floats in the order x1, y1, x2, y2 path: A boolean whether the path is opened or closed + a vector of points. text: [(x1,y1),...] for open path and ((x1,y1),...) for closed paths. See point for details. binary: first a byte indicating open (0) or close (1), then the number of points (Int32), then a vector of points box: A tuple of two points. The coordinates will be reordered so that the first is the upper right and the second is the lower left. text: (x1,y1),(x2,y2) see point for details binary: the four floats in the order x1, y1, x2, y2 (doesn't really matter since they will be reordered) polygon: Same as path but with two differences: is always closed and internally stores the bounding box. text: same as closed path binary: the number of points (Int32), then a vector of points line (version 9.4): Ax + By + C = 0. Stored with three float8. Constraint: A and B must not both be zero (only checked on text input, not binary). text: {A,B,C} see point for details about the string representation of floats. Can also use the same input format as a path with two different points, representing the line between those. binary: the three floats circle: <(x,y),r> (center point and radius), stored with three float8. text: <(x,y),r> see point for details about the string representation of floats. binary: the three floats x, y, r in that order float4/float8: text: (leading/trailing whitespace is skipped) interpreted with the C strtod function, but since it has problems with NaN, [+-]Infinity, [+-]Inf, those strings are identified (case-insensitively) separately. when outputting: NaN, [+-]Infinity is treated separately, otherwise the string is printed with snprintf %.*g and the extra_float_digits setting is honored. binary: the float abstime: A unix timestamp stored as a 32-bit signed integer with seconds-precision (seconds since 1970-01-01 00:00:00), in UTC Has three special values: Invalid (2^31-1), infinity (2^31-3), -infinity (-2^31) text: same format as timestamptz, or \"invalid\", \"infinity\", \"-infinity\" binary: Int32 reltime: A time interval with seconds-precision (stored as an 32-bit signed integer) text: same as interval binary: Int32 tinterval: Consists of a status (Int32) and two abstimes. Status is valid (1) iff both abstimes are valid, else 0. Note that the docs incorrectly states that ' is used as quote instead of \" text: [\"<abstime>\" \"<abstime>\"] binary: Int32 (status), Int32 (abstime 1), Int32 (abstime 2) unknown: text: text binary: text money: A 64-bit signed integer. For example, $123.45 is stored as the integer 12345. Number of fraction digits is locale-dependent. text: a locale-depedent string binary: the raw 64-bit integer macaddr: 6 bytes text: the 6 bytes in hex (always two characters per byte) separated by : binary: the 6 bytes appearing in the same order as when written in text inet/cidr: Struct of Family (byte: ipv4=2, ipv6=3), Netmask (byte with number of bits in the netmask), Ipaddr bytes (16) Text: The IP-address in text format and /netmask. /netmask is omitted in inet if the netmask is the whole address. Binary: family byte, netmask byte, byte (cidr=1, inet=0), number of bytes in address, bytes of the address aclitem: Access list item used in pg_class Text: Something like postgres=arwdDxt/postgres Binary: not available bpchar: Blank-padded char. The type modifier is used to blank-pad the input. text: text binary: text varchar: Variable-length char. The type modifier is used to check the input's length. text: text binary: text date: A signed 32-bit integer of a date. 0 = 2000-01-01. Infinity: INT_MAX, -Infinity: INT_MIN Text: Date only using the specified date style Binary: Int32 time: A signed 64-bit integer representing microseconds from 00:00:00.000000. (Legacy uses 64-bit float). Negative values are not allowed. Max value is 24:00:00.000000. text: hh:mm:ss or hh:mm:ss.ffffff where the fraction part is between 1 and 6 digits (trailing zeros are not written) binary: the 64-bit integer timetz: A struct of Time: A signed 64-bit integer representing microseconds from 00:00:00.000000. (Legacy uses 64-bit float). Negative values are not allowed. Max value is 24:00:00.000000. Zone: A signed 32-bit integer representing the zone (in seconds). Note that the sign is inverted. So GMT+1h is stored as -1h. text: hh:mm:ss or hh:mm:ss.ffffff where the fraction part is between 1 and 6 digits (trailing zeros are not written) binary: the 64-bit integer followed by the 32-bit integer timestamp: A signed 64-bit integer representing microseconds from 2000-01-01 00:00:00.000000 Infinity is LONG_MAX and -Infinity is LONG_MIN (Infinity would be 294277-01-09 04:00:54.775807) Earliest possible timestamp is 4714-11-24 00:00:00 BC. Even earlier would be possible, but due to internal calculations those are forbidden. text: dependent on date style binary: the 64-bit integer timestamptz: A signed 64-bit integer representing microseconds from 2000-01-01 00:00:00.000000 UTC. (Time zone is not stored). Infinity is LONG_MAX and -Infinity is LONG_MIN text: first converted to the time zone in the db settings, then printed according to the date style binary: the 64-bit integer interval: A struct of Time (Int64): all time units other than days, months and years (microseconds) Day (Int32): days, after time for alignment Month (Int32): months and years, after time for alignment text: Style dependent, but for example: \"-11 mons +15435 days -11111111:53:00\" binary: all fields in the struct bit/varbit: First a signed 32-bit integer containing the number of bits (negative length not allowed). Then all the bits in big end first. So a varbit of length 1 has the first (and only) byte set to either 0x80 or 0x00. Last byte is assumed (and is automatically zero-padded in recv) to be zero-padded. text: when sending from backend: all the bits, written with 1s and 0s. when receiving from client: (optionally b or B followed by) all the bits as 1s and 0s, or a x or X followed by hexadecimal digits (upper- or lowercase), big endian first. binary: the 32-bit length followed by the bytes containing the bits numeric: A variable-length numeric value, can be negative. text: NaN or first - if it is negative, then the digits with . as decimal separator binary: first a header of 4 16-bit signed integers: number of digits in the digits array that follows (can be 0, but not negative), weight of the first digit (10000^weight), can be both negative, positive or 0, sign: negative=0x4000, positive=0x0000, NaN=0xC000 dscale: number of digits (in base 10) to print after the decimal separator then the array of digits: The digits are stored in base 10000, where each digit is a 16-bit integer. Trailing zeros are not stored in this array, to save space. The digits are stored such that, if written as base 10000, the decimal separator can be inserted between two digits in base 10000, i.e. when this is to be printed in base 10, only the first digit in base 10000 can (possibly) be printed with less than 4 characters. Note that this does not apply for the digits after the decimal separator; the digits should be printed out in chunks of 4 characters and then truncated with the given dscale. refcursor: uses the same routines as text record: Describes a tuple. Is also the \"base class\" for composite types (i.e. it uses the same i/o functions). text: ( followed by a list of comma-separated text-encoded values followed by ). Empty element means null. Quoted with \" and \" if necessary. \" is escaped with \"\" and \\ is escaped with \\\\ (this differs from arrays where \" is escaped with \\\"). Must be quoted if it is an empty string or contains one of \"\\,() or a space. binary: First a 32-bit integer with the number of columns, then for each column: An OID indicating the type of the column The length of the column (32-bit integer), or -1 if null The column data encoded as binary cstring: text/binary: all characters are sent without the trailing null-character void: Used for example as return value in SELECT * FROM func_returning_void() text: an empty string binary: zero bytes uuid: A 16-byte uuid. text: group of 8, 4, 4, 4, 12 hexadecimal lower-case characters, separated by -. The first byte is written first. It is allowed to surround it with {}. binary: the 16 bytes txid_snapshot: (txid is a UInt64) A struct of UInt32 nxip (size of the xip array) txid xmin (no values in xip is smaller than this) txid xmax (no values in xip is larger than or equal this) txid[] xip (is ordered in ascending order) text: xmin:xmax:1,2,3,4 binary: all fields in the structure tsvector: Used for text searching. Example of tsvector: 'a':1,6,10 'on':5 'and':8 'ate':9A 'cat':3 'fat':2,11 'mat':7 'rat':12 'sat':4 Max length for each lexeme string is 2046 bytes (excluding the trailing null-char) The words are sorted when parsed, and only written once. Positions are also sorted and only written once. For some reason, the unique check does not seem to be made for binary input, only text input... text: As seen above. ' is escaped with '' and \\ is escaped with \\\\. binary: UInt32 number of lexemes for each lexeme: lexeme text in client encoding, null-terminated UInt16 number of positions for each position: UInt16 WordEntryPos, where the most significant 2 bits is weight, and the 14 least significant bits is pos (can't be 0). Weights 3,2,1,0 represent A,B,C,D tsquery: A tree with operands and operators (&, |, !). Operands are strings, with optional weight (bitmask of ABCD) and prefix search (yes/no, written with *). text: the tree written in infix notation. Example: ( 'abc':*B | 'def' ) & !'ghi' binary: the tree written in prefix notation: First the number of tokens (a token is an operand or an operator). For each token: UInt8 type (1 = val, 2 = oper) followed by For val: UInt8 weight + UInt8 prefix (1 = yes / 0 = no) + null-terminated string, For oper: UInt8 oper (1 = not, 2 = and, 3 = or, 4 = phrase). In case of phrase oper code, an additional UInt16 field is sent (distance value of operator). Default is 1 for <->, otherwise the n value in '<n>'. enum: Simple text gtsvector: GiST for tsvector. Probably internal type. int4range/numrange/tsrange/tstzrange/daterange/int8range and user-defined range types: /* A range's flags byte contains these bits: */ #define RANGE_EMPTY 0x01 /* range is empty */ #define RANGE_LB_INC 0x02 /* lower bound is inclusive */ #define RANGE_UB_INC 0x04 /* upper bound is inclusive */ #define RANGE_LB_INF 0x08 /* lower bound is -infinity */ #define RANGE_UB_INF 0x10 /* upper bound is +infinity */ #define RANGE_LB_NULL 0x20 /* lower bound is null (NOT USED) */ #define RANGE_UB_NULL 0x40 /* upper bound is null (NOT USED) */ #define RANGE_CONTAIN_EMPTY 0x80/* marks a GiST internal-page entry whose * subtree contains some empty ranges */ A range has no lower bound if any of RANGE_EMPTY, RANGE_LB_INF (or RANGE_LB_NULL, not used anymore) is set. The same applies for upper bounds. text: A range with RANGE_EMPTY is just written as the string \"empty\". Inclusive bounds are written with [ and ], else ( and ) is used. The two values are comma-separated. Missing bounds are written as an empty string (without quotes). Each value is quoted with \" if necessary. Quotes are necessary if the string is either the empty string or contains \"\\,()[] or spaces. \" is escaped with \"\" and \\ is escaped with \\\\. Example: [18,21] binary: First the flag byte. Then, if has lower bound: 32-bit length + binary-encoded data. Then, if has upper bound: 32-bit length + binary-encoded data. hstore: Key/value-store. Both keys and values are strings. text: Comma-space separated string, where each item is written as \"key\"=>\"value\" or \"key\"=>NULL. \" and \\ are escaped as \\\" and \\\\. Example: \"a\"=>\"b\", \"c\"=>NULL, \"d\"=>\"q\" binary: Int32 count for each item: Int32 keylen string of the key (not null-terminated) Int32 length of item (or -1 if null) the item as a string ghstore: internal type for indexing hstore domain types: mapped types used in information_schema: cardinal_number: int4 (must be nonnegative or null) character_data: varchar sql_identifier: varchar time_stamp: timestamptz yes_or_no: varchar(3) (must be \"YES\" or \"NO\" or null) intnotnull: when an int4 is cast to this type, it is checked that the int4 is not null, but it still returns an int4 and not intnotnull..."
  },
  "doc/logging.html": {
    "href": "doc/logging.html",
    "title": "Logging | Npgsql Documentation",
    "keywords": "Logging Note: Npgsql 3.2.0 and 3.2.1 significantly changed logging to use Microsoft.Extensions.Logging. After several complaints and issues (see #1504 ), this feature was rolled back. Starting with Npgsql 3.2.2, logging support is identical to Npgsql 3.1. Npgsql includes a built-in feature for outputting logging events which can help debug issues. Npgsql logging is disabled by default and must be turned on. Logging can be turned on by setting NpgsqlLogManager.Provider to a class implementing the INpgsqlLoggingProvider interface. Npgsql comes with a console implementation which can be set up as follows: NpgsqlLogManager.Provider = new ??? Note: you must set the logging provider before invoking any other Npgsql method, at the very start of your program. It's trivial to create a logging provider that passes log messages to whatever logging framework you use, you can find such an adapter for NLog below. Note: the logging API is a first implementation and will probably improve/change - don't treat it as a stable part of the Npgsql API. Let us know if you think there are any missing messages or features! ConsoleLoggingProvider Npgsql comes with one built-in logging provider: ConsoleLoggingProvider. It will simply dump all log messages with a given level or above to stdanrd output. You can set it up by including the following line at the beginning of your application: NpgsqlLogManager.Provider = new ConsoleLoggingProvider(<min level>, <print level?>, <print connector id?>); Level defaults to NpgsqlLogLevel.Info (which will only print warnings and errors). You can also have log levels and connector IDs logged. Statement and Parameter Logging Npgsql will log all SQL statements at level Debug, this can help you debug exactly what's being sent to PostgreSQL. By default, Npgsql will not log parameter values as these may contain sensitive information. You can turn on parameter logging by setting NpgsqlLogManager.IsParameterLoggingEnabled to true. NLogLoggingProvider (or implementing your own) The following provider is used in the Npgsql unit tests to pass log messages to NLog . You're welcome to copy-paste it into your project, or to use it as a starting point for implementing your own custom provider. class NLogLoggingProvider : INpgsqlLoggingProvider { public NpgsqlLogger CreateLogger(string name) { return new NLogLogger(name); } } class NLogLogger : NpgsqlLogger { readonly Logger _log; internal NLogLogger(string name) { _log = LogManager.GetLogger(name); } public override bool IsEnabled(NpgsqlLogLevel level) { return _log.IsEnabled(ToNLogLogLevel(level)); } public override void Log(NpgsqlLogLevel level, int connectorId, string msg, Exception exception = null) { var ev = new LogEventInfo(ToNLogLogLevel(level), \"\", msg); if (exception != null) ev.Exception = exception; if (connectorId != 0) ev.Properties[\"ConnectorId\"] = connectorId; _log.Log(ev); } static LogLevel ToNLogLogLevel(NpgsqlLogLevel level) { switch (level) { case NpgsqlLogLevel.Trace: return LogLevel.Trace; case NpgsqlLogLevel.Debug: return LogLevel.Debug; case NpgsqlLogLevel.Info: return LogLevel.Info; case NpgsqlLogLevel.Warn: return LogLevel.Warn; case NpgsqlLogLevel.Error: return LogLevel.Error; case NpgsqlLogLevel.Fatal: return LogLevel.Fatal; default: throw new ArgumentOutOfRangeException(\"level\"); } } }"
  },
  "doc/copy.html": {
    "href": "doc/copy.html",
    "title": "COPY | Npgsql Documentation",
    "keywords": "COPY PostgreSQL has a feature allowing efficient bulk import or export of data to and from a table. This is usually a much faster way of getting data in and out of a table than using INSERT and SELECT. See documentation for the COPY command for more details. Npgsql supports three COPY operation modes: binary, text and raw binary. Binary COPY This mode uses the efficient PostgreSQL binary format to transfer data in and out of the database. The user uses an API to read and write rows and fields, which Npgsql decodes and encodes. When you've finished, you must call Complete() to save the data; not doing so will cause the COPY operation to be rolled back when the writer is disposed (this behavior is important in case an exception is thrown). IMPORTANT : Note that it is the your responsibility to read and write the correct type! If you use COPY to write an int32 into a string field you may get an exception, or worse, silent data corruption. It is also highly recommended to use the overload of Write() which accepts an NpgsqlDbType , allowing you to unambiguously specify exactly what type you want to write. Test your code throroughly. // Import two columns to table data using (var writer = conn.BeginBinaryImport(\"COPY data (field_text, field_int2) FROM STDIN (FORMAT BINARY)\")) { writer.StartRow(); writer.Write(\"Hello\"); writer.Write(8, NpgsqlDbType.Smallint); writer.StartRow(); writer.Write(\"Goodbye\"); writer.WriteNull(); writer.Complete(); } // Export two columns to table data using (var reader = Conn.BeginBinaryExport(\"COPY data (field_text, field_int2) TO STDOUT (FORMAT BINARY)\")) { reader.StartRow(); Console.WriteLine(reader.Read<string>()); Console.WriteLine(reader.Read<int>(NpgsqlDbType.Smallint)); reader.StartRow(); reader.Skip(); Console.WriteLine(reader.IsNull); // Null check doesn't consume the column Console.WriteLine(reader.Read<int>()); reader.StartRow(); // Last StartRow() returns -1 to indicate end of data } Text COPY This mode uses the PostgreSQL text or csv format to transfer data in and out of the database. It is the user's responsibility to format the text or CSV appropriately, Npgsql simply provides a TextReader or Writer. This mode is less efficient than binary copy, and is suitable mainly if you already have the data in a CSV or compatible text format and don't care about performance. using (var writer = conn.BeginTextImport(\"COPY data (field_text, field_int4) FROM STDIN\")) { writer.Write(\"HELLO\\t1\\n\"); writer.Write(\"GOODBYE\\t2\\n\"); } using (var reader = conn.BeginTextExport(\"COPY data (field_text, field_int4) TO STDOUT\")) { Console.WriteLine(reader.ReadLine()); Console.WriteLine(reader.ReadLine()); } Raw Binary COPY In this mode, data transfer is binary, but Npgsql does no encoding or decoding whatsoever - data is exposed as a raw .NET Stream. This mode makes sense only for bulk data and restore a table: the table is saved as a blob, which can later be restored. If you need to actually make sense of the data, you should be using regular binary mode instead (not raw). Example: int len; var data = new byte[10000]; // Export table1 to data array using (var inStream = conn.BeginRawBinaryCopy(\"COPY table1 TO STDOUT (FORMAT BINARY)\")) { // We assume the data will fit in 10000 bytes, in real usage you would read repeatedly, writine to a file. len = inStream.Read(data, 0, data.Length); } // Import data array into table2 using (var outStream = conn.BeginRawBinaryCopy(\"COPY table2 FROM STDIN (FORMAT BINARY)\")) { outStream.Write(data, 0, len); } Cancel Import operations can be cancelled at any time by disposing NpgsqlBinaryImporter without calling Complete() on it. Export operations can be cancelled as well, by calling Cancel() . Other See the CopyTests.cs test fixture for more usage samples."
  },
  "index.html": {
    "href": "index.html",
    "title": "Npgsql - .NET Access to PostgreSQL | Npgsql Documentation",
    "keywords": "Npgsql - .NET Access to PostgreSQL About Npgsql is an open source ADO.NET Data Provider for PostgreSQL, it allows programs written in C#, Visual Basic, F# to access the PostgreSQL database server. It is implemented in 100% C# code, is free and is open source. In addition, providers have been written for Entity Framework Core and for Entity Framework 6.x. Getting Help The best way to get help for Npgsql is to post a question to Stack Overflow and tag it with the npgsql tag. If you think you've encountered a bug or want to request a feature, open an issue in the appropriate project's github repository. License Npgsql is licensed under the PostgreSQL License , a liberal OSI-approved open source license. Contributors Current active contributors to Npgsql are: Shay Rojansky Yoh Deadfall Raif Atef Austin Drenski Emil Lenngren Past contributors to Npgsql: Jon Asher Josh Cooley Francisco Figueiredo Jr. Federico Di Gregorio Jon Hanna Chris Morgan Dave Page Glen Parker Brar Piening Hiroshi Saito Kenji Uno Thanks A special thanks to Jetbrains for donating licenses to the project - Npgsql is developed with love on Rider on Linux. Thanks also to Appveyor and Travis for their build platforms."
  },
  "efcore/release-notes/2.0.html": {
    "href": "efcore/release-notes/2.0.html",
    "title": "2.0 Release Notes | Npgsql Documentation",
    "keywords": "2.0 Release Notes Version 2.0.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget.org . This version works with version 2.0.0 of Entity Framework Core , and contains some new Npgsql features as well. New Features Aside from general EF Core features new in 2.0.0, the Npgsql provider contains the following major new features: PostgreSQL array operation translation ( #120 ). While array properties have been supported since 1.1, operations on those arrays where client-evaluated. Version 2.0 will now translate array indexing, .Contains() , .SequenceEquals() and .Length . See the array mapping docs for more details. A retrying execution strategy ( #155 ), which will automatically retry operations on exceptions which are considered transient. PostgreSQL extensions are now included in scaffolded models ( #102 ). More LINQ operations are translated to SQL, and more database scenarios are scaffolded correctly (see the docs ). Here's the full list of issues . Please report any problems to https://github.com/npgsql/Npgsql.EntityFrameworkCore.PostgreSQL . Upgrading from 1.x Previously an Npgsql.EntityFrameworkCore.PostgreSQL.Design nuget package existed alongside the main package. Its contents have been merged into the main Npgsql.EntityFrameworkCore.PostgreSQL and no new version has been released. Specifying versions when specifying PostgreSQL extensions on your model is no longer supported - this was a very rarely-used feature which interfered with extension scaffolding."
  },
  "efcore/mapping/nodatime.html": {
    "href": "efcore/mapping/nodatime.html",
    "title": "Date/Time Mapping with NodaTime | Npgsql Documentation",
    "keywords": "Date/Time Mapping with NodaTime What is NodaTime? By default, the PostgreSQL date/time types are mapped to the built-in .NET types ( DateTime , TimeSpan ). Unfortunately, these built-in types are flawed in many ways. The NodaTime library was created to solve many of these problems, and if your application handles dates and times in anything but the most basic way, you should consider using it. To learn more read this blog post by Jon Skeet . Beyond NodaTime's general advantages, some specific advantages NodaTime for PostgreSQL date/time mapping include: NodaTime defines some types which are missing from the BCL, such as LocalDate , LocalTime , and OffsetTime . These cleanly correspond to PostgreSQL date , time and timetz . Period is much more suitable for mapping PostgreSQL interval than TimeSpan . NodaTime types can fully represent PostgreSQL's microsecond precision, and can represent dates outside the BCL's date limit (1AD-9999AD). Setup To set up the NodaTime plugin, add the Npgsql.EntityFrameworkCore.PostgreSQL.NodaTime nuget to your project. Then, make the following modification to your UseNpgsql() line: protected override void OnConfiguring(DbContextOptionsBuilder builder) { builder.UseNpgsql(\"Host=localhost;Database=test;Username=npgsql_tests;Password=npgsql_tests\", o => o.UseNodaTime()); } This will set up all the necessary mappings and operation translators. You can now use NodaTime types as regular properties in your entities, and even perform some operations: public class Post { public int Id { get; set; } public string Name { get; set; } public Instant CreationTime { get; set; } } var recentPosts = context.Posts.Where(p => p.CreationTime > someInstant); Member translation Currently, the EF Core provider knows how to translate the most date/time component members of NodaTime's LocalDateTime , LocalDate , LocalTime and Period . In other words, the following query will be translated to SQL and evaluated server-side: // Get all events which occurred on a Monday var mondayEvents = context.Events.Where(p => p.SomeDate.DayOfWeek == DayOfWeek.Monday); // Get all events which occurred before the year 2000 var oldEvents = context.Events.Where(p => p.SomeDate.Year < 2000); Note that the plugin is far from covering all translations. If a translation you need is missing, please open an issue to request for it."
  },
  "efcore/mapping/array.html": {
    "href": "efcore/mapping/array.html",
    "title": "Array Type Mapping | Npgsql Documentation",
    "keywords": "Array Type Mapping PostgreSQL has the unique feature of supporting array data types . This allow you to conveniently and efficiently store several values in a single column, where in other database you'd typically resort to concatenating the values in a string or defining another table with a one-to-many relationship. Note Although PostgreSQL supports multidimensional arrays, these aren't yet supported by the EF Core provider. Mapping arrays Simply define a regular .NET array or List<> property, and the provider public class Post { public int Id { get; set; } public string Name { get; set; } public string[] Tags { get; set; } public List<string> AlternativeTags { get; set; } } The provider will create text[] columns for the above two properties, and will properly detect changes in them - if you load an array and change one of its elements, calling SaveChanges() will automatically update the row in the database accordingly. Operation translation The provider can also translate CLR array operations to the corresponding SQL operation; this allows you to efficiently work with arrays by evaluating operations in the database and avoids pulling all the data. The following table lists the range operations that currently get translated. If you run into a missing operation, please open an issue. Note that operation translation on List<> is limited at this time, but will be improved in the future. It's recommended to use an array for now. C# expression SQL generated by Npgsql .Where(c => c.SomeArray[1] = \"foo\") WHERE \"c\".\"SomeArray\"[1] = 'foo' .Where(c => c.SomeArray.SequenceEqual(new[] { 1, 2, 3 }) WHERE \"c\".\"SomeArray\" = ARRAY[1, 2, 3]) .Where(c => c.SomeArray.Contains(3)) WHERE 3 = ANY(\"c\".\"SomeArray\") .Where(c => c.SomeArray.Length == 3) WHERE array_length(\"c\".\"SomeArray, 1) = 3"
  },
  "doc/types/nodatime.html": {
    "href": "doc/types/nodatime.html",
    "title": "NodaTime Type Plugin | Npgsql Documentation",
    "keywords": "NodaTime Type Plugin Since 4.0, Npgsql supports type plugins , which are external nuget packages that modify how Npgsql maps PostgreSQL values to CLR types. One of these is the NodaTime plugin, which makes Npgsql read and write NodaTime types. The NodaTime plugin is now the recommended way to interact with PostgreSQL date/time types, and isn't the default only because of the added dependency on the NodaTime library. What is NodaTime? By default, the PostgreSQL date/time types are mapped to the built-in .NET types ( DateTime , TimeSpan ). Unfortunately, these built-in types are flawed in many ways. The NodaTime library was created to solve many of these problems, and if your application handles dates and times in anything but the most basic way, you should consider using it. To learn more read this blog post by Jon Skeet . Beyond NodaTime's general advantages, some specific advantages NodaTime for PostgreSQL date/time mapping include: NodaTime defines some types which are missing from the BCL, such as LocalDate , LocalTime , and OffsetTime . These cleanly correspond to PostgreSQL date , time and timetz . Period is much more suitable for mapping PostgreSQL interval than TimeSpan . NodaTime types can fully represent PostgreSQL's microsecond precision, and can represent dates outside the BCL's date limit (1AD-9999AD). Setup To use the NodaTime plugin, simply add a dependency on Npgsql.NodaTime and set it up: using Npgsql; // Place this at the beginning of your program to use NodaTime everywhere (recommended) NpgsqlConnection.GlobalTypeMapper.UseNodaTime(); // Or to temporarily use NodaTime on a single connection only: conn.TypeMapper.UseNodaTime(); Reading and Writing Values Once the plugin is set up, you can transparently read and write NodaTime objects: // Write NodaTime Instant to PostgreSQL \"timestamp without time zone\" using (var cmd = new NpgsqlCommand(@\"INSERT INTO mytable (my_timestamp) VALUES (@p)\", conn)) { cmd.Parameters.Add(new NpgsqlParameter(\"p\", Instant.FromUtc(2011, 1, 1, 10, 30))); cmd.ExecuteNonQuery(); } // Read timestamp back from the database as an Instant using (var cmd = new NpgsqlCommand(@\"SELECT my_timestamp FROM mytable\", conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); var instant = reader.GetFieldValue<Instant>(0); } Mapping Table Warning A common mistake is for users to think that the PostgreSQL timestamp with timezone type stores the timezone in the database. This is not the case: only the timestamp is stored. There is no single PostgreSQL type that stores both a date/time and a timezone, similar to .NET DateTimeOffset . PostgreSQL Type Default NodaTime Type Additional NodaTime Type Notes timestamp Instant LocalDateTime It's common to store UTC timestamps in databases - you can simply do so and read/write Instant values. You also have the option of readin/writing LocalDateTime, which is a date/time with no information about timezones; this makes sense if you're storing the timezone in a different column and want to read both into a NodaTime ZonedDateTime. timestamp with time zone Instant ZonedDateTime, OffsetDateTime This PostgreSQL type stores only a timestamp, assumed to be in UTC. If you read/write this as an Instant, it will be provided as stored with no timezone conversions whatsoever. If, however, you read/write as a ZonedDateTime or OffsetDateTime, the plugin will automatically convert to and from UTC according to your PostgreSQL session's timezone. date LocalDate A simple date with no timezone or offset information. time LocalTime A simple time-of-day, with no timezone or offset information. time with time zone OffsetTime This is a PostgreSQL type that stores a time and an offset. interval Period This is a human interval which does not have a fixed absolute length (\"two months\" can vary depending on the months in question), and so it is mapped to NodaTime's Period (and not Duration or TimeSpan). Additional Notes The plugin automatically converts timestamp with time zone to and from your PostgreSQL session's configured timezone; this is unlike Npgsql's default mapping which uses your machine's local timezone instead. The NodaTime plugin behavior matches the regular PostgreSQL behavior when interacting with timestamptz values. To read and write timestamp or date infinity values, set the Convert Infinity DateTime connection string parameter to true and read/write MaxValue/MinValue."
  },
  "doc/prepare.html": {
    "href": "doc/prepare.html",
    "title": "Prepared Statements | Npgsql Documentation",
    "keywords": "Prepared Statements Introduction It's recommended that you start by reading this blog post . Most applications repeat the same SQL statements many times, passing different parameters. In such cases, it's very beneficial to prepare commands - this will send the command's statement(s) to PostgreSQL, which will parse and plan for them. The prepared statements can then be used on execution, saving valuable planning time. The more complex your queries, the more you'll notice the performance gain; but even very simple queries tend to benefit from preparation. Following is a benchmark Npgsql.Benchmarks.Prepare, which measures the execution time of the same query, executed prepared and unprepared. TablesToJoin is a parameter which increases the query complexity - it determines how many tables the query joins from. Method TablesToJoin Mean StdErr StdDev Op/s Scaled Scaled-StdDev Allocated Unprepared 0 67.1964 us 0.1586 us 0.6142 us 14881.75 1.00 0.00 1.9 kB Prepared 0 43.5007 us 0.2466 us 0.9227 us 22988.13 0.65 0.01 305 B Unprepared 1 98.8502 us 0.1278 us 0.4949 us 10116.32 1.00 0.00 1.93 kB Prepared 1 53.7518 us 0.0486 us 0.1818 us 18604.04 0.54 0.00 306 B Unprepared 2 180.0599 us 0.2990 us 1.1579 us 5553.71 1.00 0.00 2.06 kB Prepared 2 70.3609 us 0.1715 us 0.6417 us 14212.44 0.39 0.00 306 B Unprepared 5 1,084.6065 us 1.1822 us 4.2626 us 921.99 1.00 0.00 2.37 kB Prepared 5 110.0652 us 0.1098 us 0.3805 us 9085.52 0.10 0.00 308 B Unprepared 10 23,086.5956 us 37.2072 us 139.2167 us 43.32 1.00 0.00 3.11 kB Prepared 10 197.1392 us 0.3044 us 1.1790 us 5072.56 0.01 0.00 308 B As is immediately apparent, even an extremely simple scenario (TablesToJoin=0, SQL=SELECT 1), preparing the query with PostgreSQL provides a 36% speedup. As query complexity increases by adding join tables, the gap widens dramatically. The only potential disadvantage of prepared statements is that they hold server-side resources (e.g. cached plans). If you're dynamically generating SQL queries, make sure you don't overwhelm the server by preparing too much. Most reasonable applications shouldn't have to worry about this. Simple Preparation To prepare your commands, simply use the following standard ADO.NET code: var cmd = new NpgsqlCommand(...); cmd.Parameters.Add(\"param\", NpgsqlDbType.Integer); cmd.Prepare(); // Set parameters cmd.ExecuteNonQuery(); // And so on Note that all parameters must be set before calling Prepare() - they are part of the information transmitted to PostgreSQL and used to effectively plan the statement. You must also set the DbType or NpgsqlDbType on your parameters to unambiguously specify the data type (setting the value isn't supported). Note that preparation happens on individual statements, and not on commands, which can contain multiple statements, batching them together. This can be important in cases such as the following: var cmd = new NpgsqlCommand(\"UPDATE foo SET bar=@bar WHERE baz=@baz; UPDATE foo SET bar=@bar WHERE baz=@baz\"); // set parameters. cmd.Prepare(); Although there are two statements in this command, the same prepared statement is used to execute since the SQL is identical. Persistency Prior to 3.2, prepared statements were closed when their owning command was disposed. This significantly reduced their usefulness, especially since closing a pooled connection automatically closed all prepared statements. For applications where connections are short-lived, such as most web applications, this effectively made prepared statements useless. Starting from 3.2, all prepared statements are persistent - they no longer get closed when a command or connection is closed. Npgsql keeps track of statements prepared on each physical connection; if you prepare the same SQL a second time on the same connection, Npgsql will simply reuse the prepared statement from the first preparation. This means that in an application with short-lived, pooled connections, prepared statements will gradually be created as the application warms up and the connections are first used. Then, opening a new pooled connection will return a physical connection that already has a prepared statement for your SQL, providing a very substantial performance boost. For example: using (var conn = new NpgsqlConnection(...) using (var cmd = new NpgsqlCommand(\"<some_sql>\", conn) { conn.Open(); cmd.Prepare(); // First time on this physical connection, Npgsql prepares with PostgreSQL cmd.ExecuteNonQuery(); } using (var conn = new NpgsqlConnection(...) using (var cmd = new NpgsqlCommand(\"<some_sql>\", conn) { conn.Open(); // We assume the pool returned the same physical connection used above cmd.Prepare(); // The connection already has a prepared statement for <some_sql>, this doesn't need to do anything cmd.ExecuteNonQuery(); } You can still choose to close a prepared statement by calling NpgsqlCommand.Unprepare() . You can also unprepare all statements on a given connection by calling NpgsqlConnection.UnprepareAll() . Automatic Preparation While the preparation examples shown above provide a very significant performance boost, they depend on you calling the Prepare() command. Unfortunately, if you're using some data layer above ADO.NET, such as Dapper or Entity Framework , chances are these layers don't prepare for you. While issues exist for both Dapper and Entity Framework Core , they don't take advantage of prepared statement at the moment. Npgsql 3.2 introduces automatic preparation. When turned on, this will make Npgsql track the statements you execute and automatically prepare them when you reach a certain threshold. When you reach that threshold, the statement is automatically prepared, and from that point on will be executed as prepared, yielding all the performance benefits discussed above. To turn on this feature, you simply need to set the Max Auto Prepare connection string parameter, which determines how many statements can be automatically prepared on the connection at any given time (this parameter defaults to 0, disabling the feature). A second parameter, Auto Prepare Min Usages , determines how many times a statement needs to be executed before it is auto-prepared (defaults to 5). Since no code changes are required, you can simply try setting Max Auto Prepare and running your application to see an immediate speed increase. Note also that, like explicitly-prepared statements, auto-prepared statements are persistent, allowing you to reap the performance benefits in short-lived connection applications. Note that if you're coding directly against Npgsql or ADO.NET, explicitly preparing your commands with Prepare() is still recommended over letting Npgsql prepare automatically. Automatic preparation does incur a slight performance cost compared to explicit preparation, because of the internal LRU cache and various book-keeping data structures. Explicitly preparing also allows you to better control exactly which statements are prepared and which aren't, and ensures your statements will always stay prepared, and never get ejected because of the LRU mechanism. Note that automatic preparation is a complex new feature which should be considered somewhat experimental; test carefully, and if you see any strange behavior or problem try turning it off."
  },
  "dev/tests.html": {
    "href": "dev/tests.html",
    "title": "Tests | Npgsql Documentation",
    "keywords": "Overview Npgsql comes with an extensive test suite to make sure no regressions occur. All tests are run on our build server on all supported .NET versions (including a recent version of mono) and all supported PostgreSQL backends. There is also a growing suite of speed tests to be able to measure performance. These tests are currently marked [Explicit] and aren't executed automatically. Simple setup The Npgsql test suite requires a PostgreSQL backend to test against. Simply use the latest version of PostgreSQL on your dev machine on the default port (5432). By default, all tests will be run using user npgsql_tests , and password npgsql_tests . Npgsql will automatically create a database called npgsql_tests and run its tests against this. To set this up, connect to PostgreSQL as the admin user as follows: psql -h localhost -U postgres <enter the admin password> create user npgsql_tests password 'npgsql_tests' superuser; And you're done. Superuser access is needed for some tests, e.g. loading the hstore extension, creating and dropping test databases in the Entity Framework tests..."
  },
  "efcore/mapping/enum.html": {
    "href": "efcore/mapping/enum.html",
    "title": "Enum Type Mapping | Npgsql Documentation",
    "keywords": "Enum Type Mapping By default, any enum properties in your model will be mapped to database integers. EF Core 2.1 also allows you to map these to strings in the database with value converters. However, the Npgsql provider also allows you to map your CLR enums to database enum types . This option, unique to PostgreSQL, provides the best of both worlds: the enum is internally stored in the database as a number (minimal storage), but is handled like a string (more usable, no need to remember numeric values) and has type safety. Creating your database enum First, you must specify the PostgreSQL enum type on your model, just like you would with tables, sequences or other databases objects: Version 2.2 Version 2.1 protected override void OnModelCreating(ModelBuilder builder) => builder.ForNpgsqlHasEnum<Mood>(); protected override void OnModelCreating(ModelBuilder builder) => builder.ForNpgsqlHasEnum(\"Mood\", new[] { \"happy\", \"sad\" }); This causes the EF Core provider to create your data enum type, Mood , with two labels: happy and sad . This will cause the appropriate migration to be created. Mapping your enum Even if your database enum is created, Npgsql has to know about it, and especially about your CLR enum type that should be mapped to it. This is done by adding the following code, before any EF Core operations take place. An appropriate place for this is in the static constructor on your DbContext class: static MyDbContext() => NpgsqlConnection.GlobalTypeMapper.MapEnum<Mood>(); This code lets Npgsql know that your CLR enum type, Mood , should be mapped to a database enum called Mood . If you're curious as to inner workings, this code maps the enum with the ADO.NET provider - see here for the full docs . When the Npgsql EF Core first initializes, it calls into the ADO.NET provider to get all mapped enums, and sets everything up internally at the EF Core layer as well. Using enum properties Once your enum is mapped and created in the database, you can use your CLR enum type just like any other property: public class Blog { public int Id { get; set; } public Mood Mood { get; set; } } using (var ctx = new MyDbContext()) { // Insert ctx.Blogs.Add(new Blog { Mood = Mood.Happy }); ctx.Blogs.SaveChanges(); // Query var blog = ctx.Blogs.Single(b => b.Mood == Mood.Happy); } Altering enum definitions Although PostgreSQL allows altering enum types , the Npgsql provider currently does not generate SQL for those operations (beyond creating and dropping the entire type). If you to add, remove or rename enum values, you'll have to include raw SQL in your migrations (this is quite easy to do). As always, test your migrations carefully before running them on production databases. Scaffolding from an existing database If you're creating your model from an existing database, the provider will recognize enums in your database, and scaffold the appropriate ForNpgsqlHasEnum() lines in your model. However, since the scaffolding process has no knowledge of your CLR type, and will therefore skip your enum columns (warnings will be logged). You will have to create the CLR type, add the global mapping and add the properties to your entities. In the future it may be possible to scaffold the actual enum type (and with it the properties), but this doesn't happen at the moment."
  }
}